{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Python\u306b\u3088\u308b\u6642\u7cfb\u5217\u4e88\u6e2c","text":""},{"location":"index.html#_1","title":"\u6982\u8981","text":"<ul> <li>\u300ePython\u306b\u3088\u308b\u6642\u7cfb\u5217\u4e88\u6e2c\u300f\u306ePython\u5b9f\u88c5\u3092\u307e\u3068\u3081\u3066\u3044\u304f\u3002 </li> </ul> \u5404\u7ae0 \u30ea\u30f3\u30af \u6700\u7d42\u66f4\u65b0\u65e5 \u7b2c1\u7ae0 \u6642\u7cfb\u5217\u4e88\u6e2c chapter01 \u7b2c2\u7ae0 \u5358\u7d14\u306a\u672a\u6765\u4e88\u6e2c chapter02 2023-11-12 \u7b2c3\u7ae0 \u30e9\u30f3\u30c0\u30e0\u30a6\u30a9\u30fc\u30af chapter03 2023-11-12 \u7b2c4\u7ae0 \u79fb\u52d5\u5e73\u5747\u30d7\u30ed\u30bb\u30b9\u306e\u30e2\u30c7\u30eb\u5316 chapter04 2023-11-13 \u7b2c5\u7ae0 \u81ea\u5df1\u56de\u5e30\u30d7\u30ed\u30bb\u30b9\u306e\u30e2\u30c7\u30eb\u5316 chapter05 2023-11-13 \u7b2c6\u7ae0 \u8907\u96d1\u306a\u6642\u7cfb\u5217\u306e\u30e2\u30c7\u30eb\u5316 chapter06 2023-11-14 \u7b2c7\u7ae0 \u975e\u5b9a\u5e38\u6642\u7cfb\u5217\u306e\u4e88\u6e2c chapter07 2023-11-15 \u7b2c8\u7ae0 \u5b63\u7bc0\u6027\u306e\u8003\u616e chapter08 2023-11-16 \u7b2c9\u7ae0 \u30e2\u30c7\u30eb\u3078\u306e\u5916\u90e8\u5909\u6570\u306e\u8ffd\u52a0 chapter09 2023-11-17 \u7b2c10\u7ae0 \u8907\u6570\u306e\u6642\u7cfb\u5217\u306e\u4e88\u6e2c chapter10 2023-11-18 \u7b2c11\u7ae0 \u30ad\u30e3\u30c3\u30d7\u30b9\u30c8\u30fc\u30f3\uff1a\u30aa\u30fc\u30b9\u30c8\u30e9\u30ea\u30a2\u306e\u6297\u7cd6\u5c3f\u75c5\u85ac\u51e6\u65b9\u6570\u306e\u4e88\u6e2c chapter11 2023-11-19 \u7b2c12\u7ae0 \u6642\u7cfb\u5217\u4e88\u6e2c\u306e\u305f\u3081\u306e\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0 chapter12 2023-11-19 \u7b2c13\u7ae0 \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u305f\u3081\u306e\u30c7\u30fc\u30bf\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u306e\u4f5c\u6210 chapter13 2023-11-20 \u7b2c14\u7ae0 \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u624b\u307b\u3069\u304d chapter14 2023-11-20 \u7b2c15\u7ae0 LSTM\u3067\u904e\u53bb\u3092\u8a18\u61b6\u3059\u308b chapter15 2023-11-23 \u7b2c16\u7ae0 CNN\u3092\u4f7f\u3063\u305f\u6642\u7cfb\u5217\u306e\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0 chapter16 2023-11-23 \u7b2c17\u7ae0 \u4e88\u6e2c\u3092\u4f7f\u3063\u3066\u3055\u3089\u306b\u4e88\u6e2c\u3092\u884c\u3046 chapter17 2023-11-23 \u7b2c18\u7ae0 \u30ad\u30e3\u30c3\u30d7\u30b9\u30c8\u30fc\u30f3\uff1a\u5bb6\u5ead\u306e\u96fb\u529b\u6d88\u8cbb\u91cf\u306e\u4e88\u6e2c chapter18 \u7b2c19\u7ae0 Prophet\u3092\u4f7f\u3063\u305f\u6642\u7cfb\u5217\u4e88\u6e2c\u306e\u81ea\u52d5\u5316 chapter19 \u7b2c20\u7ae0 \u30ad\u30e3\u30c3\u30d7\u30b9\u30c8\u30fc\u30f3\uff1a\u30ab\u30ca\u30c0\u3067\u306e\u30b9\u30c6\u30fc\u30ad\u8089\u306e\u6708\u9593\u5e73\u5747\u5c0f\u58f2\u4fa1\u683c\u306e\u4e88\u6e2c chapter20 \u7b2c21\u7ae0 \u3055\u3089\u306a\u308b\u9ad8\u307f\u3092\u76ee\u6307\u3057\u3066 chapter21"},{"location":"chapter02.html","title":"\u7b2c2\u7ae0 \u5358\u7d14\u306a\u672a\u6765\u4e88\u6e2c","text":"In\u00a0[91]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndf = pd.read_csv('https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/jj.csv', parse_dates=['date'])\ntrain = df[:-4]\ntest = df[-4:]\n</pre> import pandas as pd import matplotlib.pyplot as plt import numpy as np  df = pd.read_csv('https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/jj.csv', parse_dates=['date']) train = df[:-4] test = df[-4:] In\u00a0[92]: Copied! <pre>test.loc[:, 'pred_mean'] = train['data'].mean()\ntest.loc[:, 'pred_last_year_mean'] = train['data'].iloc[-4:].mean()\ntest.loc[:, 'pred_last'] = train['data'].iloc[-1]\ntest.loc[:, 'pred_last_season'] = train['data'].iloc[-4:].values\n\nplt.xlabel('Date')\nplt.ylabel('EPS')\nplt.plot(train['date'], train['data'], label='Train')\nplt.plot(test['date'], test['data'], label='Test')\nplt.plot(test['date'], test['pred_mean'], ls='dashed', label='Mean')\nplt.plot(test['date'], test['pred_last_year_mean'], ls='dashed', label='Last Year Mean')\nplt.plot(test['date'], test['pred_last'], ls='dashed', label='Last Value')\nplt.plot(test['date'], test['pred_last_season'], ls='dashed', label='Last Season Value')\nplt.axvspan(test['date'].iloc[0], test['date'].iloc[-1], alpha=0.2, color='gray')\n\nplt.legend()\n</pre> test.loc[:, 'pred_mean'] = train['data'].mean() test.loc[:, 'pred_last_year_mean'] = train['data'].iloc[-4:].mean() test.loc[:, 'pred_last'] = train['data'].iloc[-1] test.loc[:, 'pred_last_season'] = train['data'].iloc[-4:].values  plt.xlabel('Date') plt.ylabel('EPS') plt.plot(train['date'], train['data'], label='Train') plt.plot(test['date'], test['data'], label='Test') plt.plot(test['date'], test['pred_mean'], ls='dashed', label='Mean') plt.plot(test['date'], test['pred_last_year_mean'], ls='dashed', label='Last Year Mean') plt.plot(test['date'], test['pred_last'], ls='dashed', label='Last Value') plt.plot(test['date'], test['pred_last_season'], ls='dashed', label='Last Season Value') plt.axvspan(test['date'].iloc[0], test['date'].iloc[-1], alpha=0.2, color='gray')  plt.legend() <pre>/tmp/ipykernel_759/3449879940.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test.loc[:, 'pred_mean'] = train['data'].mean()\n/tmp/ipykernel_759/3449879940.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test.loc[:, 'pred_last_year_mean'] = train['data'].iloc[-4:].mean()\n/tmp/ipykernel_759/3449879940.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test.loc[:, 'pred_last'] = train['data'].iloc[-1]\n/tmp/ipykernel_759/3449879940.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test.loc[:, 'pred_last_season'] = train['data'].iloc[-4:].values\n</pre> Out[92]: <pre>&lt;matplotlib.legend.Legend at 0x7f05e0135a50&gt;</pre> In\u00a0[93]: Copied! <pre>def mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred - y_true) / y_true))\n\nxs = ['pred_mean', 'pred_last_year_mean', 'pred_last', 'pred_last_season']\nys = [mape(test['data'], test[x]) for x in xs]\n\nplt.bar(xs, ys)\nplt.xlabel('Baseline')\nplt.ylabel('MAPE')\n# set y value to percentage\nplt.gca().yaxis.set_major_formatter(lambda y, _: '{:.0%}'.format(y))\n</pre> def mape(y_true, y_pred):     return np.mean(np.abs((y_pred - y_true) / y_true))  xs = ['pred_mean', 'pred_last_year_mean', 'pred_last', 'pred_last_season'] ys = [mape(test['data'], test[x]) for x in xs]  plt.bar(xs, ys) plt.xlabel('Baseline') plt.ylabel('MAPE') # set y value to percentage plt.gca().yaxis.set_major_formatter(lambda y, _: '{:.0%}'.format(y)) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter03.html","title":"\u7b2c3\u7ae0 \u30e9\u30f3\u30c0\u30e0\u30a6\u30a9\u30fc\u30af","text":"In\u00a0[14]: Copied! <pre>import yfinance as yf\nimport pandas_datareader.data as web\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nyf.pdr_override()\n</pre> import yfinance as yf import pandas_datareader.data as web import numpy as np from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import plot_acf from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt import pandas as pd  yf.pdr_override() In\u00a0[6]: Copied! <pre>ser_google = web.get_data_yahoo(tickers='GOOGL', start='2020-04-27', end='2021-04-27')['Close']\nser_google.plot()\n</pre> ser_google = web.get_data_yahoo(tickers='GOOGL', start='2020-04-27', end='2021-04-27')['Close'] ser_google.plot() <pre>[*********************100%***********************]  1 of 1 completed\n</pre> Out[6]: <pre>&lt;Axes: xlabel='Date'&gt;</pre> In\u00a0[7]: Copied! <pre>stationary = np.empty(400)\nstationary[0] = 0\nnon_stationary = np.empty(400)\nnon_stationary[0] = 10\nnp.random.seed(42)\nsteps = np.random.standard_normal(400)\n\nfor i in range(1, 400):\n    stationary[i] = 0.5 * stationary[i-1] + steps[i]\n    non_stationary[i] = non_stationary[i-1] + steps[i]\n\nmean_stationary = [stationary[:i].mean() for i in range(1, 400)]\nmean_non_stationary = [non_stationary[:i].mean() for i in range(1, 400)]\n\nvar_stationary = [stationary[:i].var() for i in range(1, 400)]\nvar_non_stationary = [non_stationary[:i].var() for i in range(1, 400)]\n\nfig, axes = plt.subplots(1, 3, figsize=[15, 4])\n\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('Value')\naxes[0].plot(stationary, label='stationary')\naxes[0].plot(non_stationary, label='non-stationary', ls='dashed')\naxes[0].legend()\n\naxes[1].set_xlabel('Timestep')\naxes[1].set_ylabel('Mean')\naxes[1].plot(mean_stationary, label='stationary')\naxes[1].plot(mean_non_stationary, label='non-stationary', ls='dashed')\naxes[1].legend()\n\naxes[2].set_xlabel('Timestep')\naxes[2].set_ylabel('Variance')\naxes[2].plot(var_stationary, label='stationary')\naxes[2].plot(var_non_stationary, label='non-stationary', ls='dashed')\naxes[2].legend()\n</pre> stationary = np.empty(400) stationary[0] = 0 non_stationary = np.empty(400) non_stationary[0] = 10 np.random.seed(42) steps = np.random.standard_normal(400)  for i in range(1, 400):     stationary[i] = 0.5 * stationary[i-1] + steps[i]     non_stationary[i] = non_stationary[i-1] + steps[i]  mean_stationary = [stationary[:i].mean() for i in range(1, 400)] mean_non_stationary = [non_stationary[:i].mean() for i in range(1, 400)]  var_stationary = [stationary[:i].var() for i in range(1, 400)] var_non_stationary = [non_stationary[:i].var() for i in range(1, 400)]  fig, axes = plt.subplots(1, 3, figsize=[15, 4])  axes[0].set_xlabel('Timestep') axes[0].set_ylabel('Value') axes[0].plot(stationary, label='stationary') axes[0].plot(non_stationary, label='non-stationary', ls='dashed') axes[0].legend()  axes[1].set_xlabel('Timestep') axes[1].set_ylabel('Mean') axes[1].plot(mean_stationary, label='stationary') axes[1].plot(mean_non_stationary, label='non-stationary', ls='dashed') axes[1].legend()  axes[2].set_xlabel('Timestep') axes[2].set_ylabel('Variance') axes[2].plot(var_stationary, label='stationary') axes[2].plot(var_non_stationary, label='non-stationary', ls='dashed') axes[2].legend() Out[7]: <pre>&lt;matplotlib.legend.Legend at 0x7fcfcd0d67a0&gt;</pre> In\u00a0[8]: Copied! <pre>np.random.seed(42)\nsteps = np.random.standard_normal(1000)\nsteps[0] = 0\nrandom_walk = steps.cumsum()\n\nADF_result = adfuller(random_walk)\nprint(f'ADF Statics: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n\nfig, axes = plt.subplots(1, 2, figsize=[12, 4])\naxes[0].plot(random_walk)\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('Value')\nplot_acf(random_walk, ax=axes[1], lags=20, auto_ylims=True)\nplt.show()\n</pre> np.random.seed(42) steps = np.random.standard_normal(1000) steps[0] = 0 random_walk = steps.cumsum()  ADF_result = adfuller(random_walk) print(f'ADF Statics: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}')  fig, axes = plt.subplots(1, 2, figsize=[12, 4]) axes[0].plot(random_walk) axes[0].set_xlabel('Timestep') axes[0].set_ylabel('Value') plot_acf(random_walk, ax=axes[1], lags=20, auto_ylims=True) plt.show() <pre>ADF Statics: -0.966\np-value: 0.765\n</pre> In\u00a0[9]: Copied! <pre>diff_random_walk = np.diff(random_walk)\nADF_result = adfuller(diff_random_walk)\nprint(f'ADF Statics: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n\nfig, axes = plt.subplots(1, 2, figsize=[12, 4])\naxes[0].plot(diff_random_walk)\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('Value')\nplot_acf(diff_random_walk, ax=axes[1], lags=20, auto_ylims=True)\nplt.show()\n</pre> diff_random_walk = np.diff(random_walk) ADF_result = adfuller(diff_random_walk) print(f'ADF Statics: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}')  fig, axes = plt.subplots(1, 2, figsize=[12, 4]) axes[0].plot(diff_random_walk) axes[0].set_xlabel('Timestep') axes[0].set_ylabel('Value') plot_acf(diff_random_walk, ax=axes[1], lags=20, auto_ylims=True) plt.show() <pre>ADF Statics: -31.789\np-value: 0.000\n</pre> In\u00a0[10]: Copied! <pre>ADF_google_result = adfuller(ser_google)\nprint(f'ADF Statics: {ADF_google_result[0]:.3f}')\nprint(f'p-value: {ADF_google_result[1]:.3f}')\n</pre> ADF_google_result = adfuller(ser_google) print(f'ADF Statics: {ADF_google_result[0]:.3f}') print(f'p-value: {ADF_google_result[1]:.3f}') <pre>ADF Statics: 0.318\np-value: 0.978\n</pre> In\u00a0[11]: Copied! <pre>ser_google_diff = ser_google.diff().dropna()\nADF_google_result = adfuller(ser_google_diff)\nprint(f'ADF Statics: {ADF_google_result[0]:.3f}')\nprint(f'p-value: {ADF_google_result[1]:.3f}')\n\nplot_acf(ser_google_diff, lags=20, auto_ylims=True)\nplt.show()\n</pre> ser_google_diff = ser_google.diff().dropna() ADF_google_result = adfuller(ser_google_diff) print(f'ADF Statics: {ADF_google_result[0]:.3f}') print(f'p-value: {ADF_google_result[1]:.3f}')  plot_acf(ser_google_diff, lags=20, auto_ylims=True) plt.show() <pre>ADF Statics: -5.263\np-value: 0.000\n</pre> In\u00a0[15]: Copied! <pre>df = pd.DataFrame({'value': random_walk})\ntrain = df[:800]\ntest = df[800:]\n</pre> df = pd.DataFrame({'value': random_walk}) train = df[:800] test = df[800:] In\u00a0[16]: Copied! <pre>test.loc[:, 'pred_mean'] = train.mean().value\ntest.loc[:, 'pred_last'] = train.iloc[-1].value\ndrift = (train['value'].iloc[-1] - train['value'].iloc[0]) / (train.index[-1] - train.index[0])\ntest.loc[:, 'pred_drift'] = np.arange(801, 1001) * drift\ntest.loc[:, 'pred_shift'] = df['value'].shift()[800:]\n\nplt.xlabel('Timestep')\nplt.ylabel('Value')\nplt.plot(train.index, train['value'], label='Train')\nplt.plot(test.index, test['value'], label='Test')\nplt.plot(test.index, test['pred_mean'], ls='dashed', label='Mean')\nplt.plot(test.index, test['pred_last'], ls='dashed', label='Last Value')\nplt.plot(test.index, test['pred_drift'], ls='dashed', label='Drift')\nplt.plot(test.index, test['pred_shift'], ls='dashed', label='Shift')\nplt.axvspan(test.index[0], test.index[-1], alpha=0.2, color='gray')\n\nplt.legend()\n</pre> test.loc[:, 'pred_mean'] = train.mean().value test.loc[:, 'pred_last'] = train.iloc[-1].value drift = (train['value'].iloc[-1] - train['value'].iloc[0]) / (train.index[-1] - train.index[0]) test.loc[:, 'pred_drift'] = np.arange(801, 1001) * drift test.loc[:, 'pred_shift'] = df['value'].shift()[800:]  plt.xlabel('Timestep') plt.ylabel('Value') plt.plot(train.index, train['value'], label='Train') plt.plot(test.index, test['value'], label='Test') plt.plot(test.index, test['pred_mean'], ls='dashed', label='Mean') plt.plot(test.index, test['pred_last'], ls='dashed', label='Last Value') plt.plot(test.index, test['pred_drift'], ls='dashed', label='Drift') plt.plot(test.index, test['pred_shift'], ls='dashed', label='Shift') plt.axvspan(test.index[0], test.index[-1], alpha=0.2, color='gray')  plt.legend() <pre>/tmp/ipykernel_13890/1854262779.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test.loc[:, 'pred_mean'] = train.mean().value\n/tmp/ipykernel_13890/1854262779.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test.loc[:, 'pred_last'] = train.iloc[-1].value\n/tmp/ipykernel_13890/1854262779.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test.loc[:, 'pred_drift'] = np.arange(801, 1001) * drift\n/tmp/ipykernel_13890/1854262779.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test.loc[:, 'pred_shift'] = df['value'].shift()[800:]\n</pre> Out[16]: <pre>&lt;matplotlib.legend.Legend at 0x7fcfcbc89720&gt;</pre> In\u00a0[17]: Copied! <pre>xs = ['pred_mean', 'pred_last', 'pred_drift', 'pred_shift']\nys = [mean_squared_error(test['value'], test[x]) for x in xs]\n\nplt.bar(xs, ys)\nplt.xlabel('Methods')\nplt.ylabel('MSE')\n</pre> xs = ['pred_mean', 'pred_last', 'pred_drift', 'pred_shift'] ys = [mean_squared_error(test['value'], test[x]) for x in xs]  plt.bar(xs, ys) plt.xlabel('Methods') plt.ylabel('MSE') Out[17]: <pre>Text(0, 0.5, 'MSE')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter04.html","title":"\u7b2c4\u7ae0 \u79fb\u52d5\u5e73\u5747\u30d7\u30ed\u30bb\u30b9\u306e\u30e2\u30c7\u30eb\u5316","text":"In\u00a0[115]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import plot_acf from statsmodels.tsa.statespace.sarimax import SARIMAX from sklearn.metrics import mean_squared_error, mean_absolute_error In\u00a0[51]: Copied! <pre>url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/widget_sales.csv'\ndf = pd.read_csv(url)\ndf.index = pd.date_range('2019-01-01', periods=500, name='Time')\n</pre> url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/widget_sales.csv' df = pd.read_csv(url) df.index = pd.date_range('2019-01-01', periods=500, name='Time') In\u00a0[60]: Copied! <pre>ADF_result = adfuller(df['widget_sales'])\nprint('\u88fd\u54c1\u58f2\u4e0a\u9ad8ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n\ndf_diff = df.diff().dropna()\nADF_result = adfuller(df_diff['widget_sales'])\nprint('\u88fd\u54c1\u58f2\u4e0a\u9ad8(1\u6b21\u5dee\u5206) ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n\nfig, axes = plt.subplots(1, 2, figsize=[12, 4])\ndf['widget_sales'].plot(ax=axes[0])\ndf_diff['widget_sales'].plot(ax=axes[1])\naxes[0].set_ylabel('Widget sales (k$)')\naxes[1].set_ylabel('Widget sales - diff (k$)')\nplt.show()\n</pre> ADF_result = adfuller(df['widget_sales']) print('\u88fd\u54c1\u58f2\u4e0a\u9ad8ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}')  df_diff = df.diff().dropna() ADF_result = adfuller(df_diff['widget_sales']) print('\u88fd\u54c1\u58f2\u4e0a\u9ad8(1\u6b21\u5dee\u5206) ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}')  fig, axes = plt.subplots(1, 2, figsize=[12, 4]) df['widget_sales'].plot(ax=axes[0]) df_diff['widget_sales'].plot(ax=axes[1]) axes[0].set_ylabel('Widget sales (k$)') axes[1].set_ylabel('Widget sales - diff (k$)') plt.show() <pre>\u88fd\u54c1\u58f2\u4e0a\u9ad8ADF\u691c\u5b9a\nADF Statistic: -1.512\np-value: 0.527\n\u88fd\u54c1\u58f2\u4e0a\u9ad8(1\u6b21\u5dee\u5206) ADF\u691c\u5b9a\nADF Statistic: -10.577\np-value: 0.000\n</pre> In\u00a0[53]: Copied! <pre>plot_acf(df_diff, auto_ylims=True, lags=30)\nplt.show()\n</pre> plot_acf(df_diff, auto_ylims=True, lags=30) plt.show() In\u00a0[79]: Copied! <pre>train = df_diff[:int(0.9 * len(df_diff))]\ntest = df_diff[int(0.9 * len(df_diff)):]\n\nfig, axes = plt.subplots(2, 1, figsize=[8, 8], sharex=True)\ndf['widget_sales'].plot(ax=axes[0])\ndf_diff['widget_sales'].plot(ax=axes[1])\naxes[0].set_ylabel('Widget sales (k$)')\naxes[1].set_ylabel('Widget sales - diff (k$)')\nfor i in range(2):\n    axes[i].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\n</pre> train = df_diff[:int(0.9 * len(df_diff))] test = df_diff[int(0.9 * len(df_diff)):]  fig, axes = plt.subplots(2, 1, figsize=[8, 8], sharex=True) df['widget_sales'].plot(ax=axes[0]) df_diff['widget_sales'].plot(ax=axes[1]) axes[0].set_ylabel('Widget sales (k$)') axes[1].set_ylabel('Widget sales - diff (k$)') for i in range(2):     axes[i].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) In\u00a0[101]: Copied! <pre>def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window :int, method: str) -&gt; list:\n    total_len = train_len + horizon\n    preds = []\n    if method == 'mean':\n        for i in range(train_len, total_len, window):\n            mean = np.mean(df[:i].values)\n            preds.extend(mean for _ in range(window))\n    \n    elif method == 'last':\n        for i in range(train_len, total_len, window):\n            last_val = df.iloc[i-1, 0]\n            preds.extend(last_val for _ in range(window))\n    \n    elif method == 'MA':\n        for i in range(train_len, total_len, window):\n            model = SARIMAX(df[:i], order=(0, 0, 2))\n            res = model.fit(disp=False)\n            pred = res.get_prediction(0, i + window - 1).predicted_mean.iloc[-window:]\n            preds.extend(pred)\n    else:\n        raise ValueError(f\"Invalid method: {method}, choose from ['mean', 'last', 'MA']\")\n    return preds\n</pre> def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window :int, method: str) -&gt; list:     total_len = train_len + horizon     preds = []     if method == 'mean':         for i in range(train_len, total_len, window):             mean = np.mean(df[:i].values)             preds.extend(mean for _ in range(window))          elif method == 'last':         for i in range(train_len, total_len, window):             last_val = df.iloc[i-1, 0]             preds.extend(last_val for _ in range(window))          elif method == 'MA':         for i in range(train_len, total_len, window):             model = SARIMAX(df[:i], order=(0, 0, 2))             res = model.fit(disp=False)             pred = res.get_prediction(0, i + window - 1).predicted_mean.iloc[-window:]             preds.extend(pred)     else:         raise ValueError(f\"Invalid method: {method}, choose from ['mean', 'last', 'MA']\")     return preds In\u00a0[102]: Copied! <pre>TRAIN_LEN = len(train)\nHORIZON = len(test)\nWINDOW = 2\n\nmethods = ['mean', 'last', 'MA']\nfor method in methods:\n    test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n</pre> TRAIN_LEN = len(train) HORIZON = len(test) WINDOW = 2  methods = ['mean', 'last', 'MA'] for method in methods:     test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method) <pre>/tmp/ipykernel_61933/3131777319.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n/tmp/ipykernel_61933/3131777319.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n</pre> In\u00a0[108]: Copied! <pre>fig, ax = plt.subplots()\ndf_diff['widget_sales'].iloc[-70:].plot(ax=ax, label='actual')\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nax.set_ylabel('Widget sales - diff (k$)')\nfor method in methods:\n    test[method].plot(ax=ax, label=method, ls='dashed')\nax.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() df_diff['widget_sales'].iloc[-70:].plot(ax=ax, label='actual') ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) ax.set_ylabel('Widget sales - diff (k$)') for method in methods:     test[method].plot(ax=ax, label=method, ls='dashed') ax.legend() plt.show() In\u00a0[107]: Copied! <pre>plt.xlabel('Methods')\nplt.ylabel('MSE')\nplt.bar(\n    methods,\n    [mean_squared_error(test['widget_sales'], test[method]) for method in methods]\n)\nplt.show()\n</pre> plt.xlabel('Methods') plt.ylabel('MSE') plt.bar(     methods,     [mean_squared_error(test['widget_sales'], test[method]) for method in methods] ) plt.show() In\u00a0[119]: Copied! <pre>df['pred_MA'] = pd.Series()\ndf['pred_MA'].iloc[450:] = df['widget_sales'].iloc[450] + test['MA'].cumsum()\n\n# plot\nfig, ax = plt.subplots()\ndf['widget_sales'].iloc[-90:].plot(ax=ax, label='actual')\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nax.set_ylabel('Widget sales (k$)')\ndf['pred_MA'].iloc[-90:].plot(ax=ax, label=method, ls='dashed')\nax.legend()\nplt.show()\n</pre> df['pred_MA'] = pd.Series() df['pred_MA'].iloc[450:] = df['widget_sales'].iloc[450] + test['MA'].cumsum()  # plot fig, ax = plt.subplots() df['widget_sales'].iloc[-90:].plot(ax=ax, label='actual') ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) ax.set_ylabel('Widget sales (k$)') df['pred_MA'].iloc[-90:].plot(ax=ax, label=method, ls='dashed') ax.legend() plt.show() <pre>/tmp/ipykernel_61933/3235568680.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['pred_MA'].iloc[450:] = df['widget_sales'].iloc[450] + test['MA'].cumsum()\n</pre> In\u00a0[121]: Copied! <pre># calculate MAE\nprint(f\"MAE: {mean_absolute_error(df['widget_sales'].iloc[450:], df['pred_MA'].iloc[450:]):.3f}\")\n</pre> # calculate MAE print(f\"MAE: {mean_absolute_error(df['widget_sales'].iloc[450:], df['pred_MA'].iloc[450:]):.3f}\") <pre>MAE: 2.324\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter04.html#ma","title":"\u79fb\u52d5\u5e73\u5747(MA)\u30d7\u30ed\u30bb\u30b9\u3092\u5b9a\u7fa9\u3059\u308b\u00b6","text":""},{"location":"chapter04.html#ma","title":"MA\u30d7\u30ed\u30bb\u30b9\u3092\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter05.html","title":"\u7b2c5\u7ae0 \u81ea\u5df1\u56de\u5e30\u30d7\u30ed\u30bb\u30b9\u306e\u30e2\u30c7\u30eb\u5316","text":"In\u00a0[16]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import plot_acf, plot_pacf from statsmodels.tsa.statespace.sarimax import SARIMAX from sklearn.metrics import mean_squared_error, mean_absolute_error In\u00a0[12]: Copied! <pre>url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/foot_traffic.csv'\ndf = pd.read_csv(url)\ndf.index = pd.date_range('2000-01-01', freq='W', periods=1000, name='Time')\n\n# plot\nfig, ax = plt.subplots()\nax.set_ylabel('Average weekly foot traffic')\ndf['foot_traffic'].plot()\nplt.show()\n</pre> url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/foot_traffic.csv' df = pd.read_csv(url) df.index = pd.date_range('2000-01-01', freq='W', periods=1000, name='Time')  # plot fig, ax = plt.subplots() ax.set_ylabel('Average weekly foot traffic') df['foot_traffic'].plot() plt.show() In\u00a0[22]: Copied! <pre>ADF_result = adfuller(df['foot_traffic'])\nprint('\u5e73\u5747\u6765\u5e97\u8005\u6570ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n\ndf_diff = df.diff().dropna()\nADF_result = adfuller(df_diff['foot_traffic'])\nprint('\u5e73\u5747\u6765\u5e97\u8005\u6570(1\u6b21\u5dee\u5206) ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n\nfig, axes = plt.subplots(1, 2, figsize=[12, 4])\ndf['foot_traffic'].plot(ax=axes[0])\ndf_diff['foot_traffic'].plot(ax=axes[1])\naxes[0].set_ylabel('Average weekly foot traffic')\naxes[1].set_ylabel('Average weekly foot traffic (difference)')\nplt.show()\n</pre> ADF_result = adfuller(df['foot_traffic']) print('\u5e73\u5747\u6765\u5e97\u8005\u6570ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}')  df_diff = df.diff().dropna() ADF_result = adfuller(df_diff['foot_traffic']) print('\u5e73\u5747\u6765\u5e97\u8005\u6570(1\u6b21\u5dee\u5206) ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}')  fig, axes = plt.subplots(1, 2, figsize=[12, 4]) df['foot_traffic'].plot(ax=axes[0]) df_diff['foot_traffic'].plot(ax=axes[1]) axes[0].set_ylabel('Average weekly foot traffic') axes[1].set_ylabel('Average weekly foot traffic (difference)') plt.show() <pre>\u5e73\u5747\u6765\u5e97\u8005\u6570ADF\u691c\u5b9a\nADF Statistic: -1.176\np-value: 0.684\n\u5e73\u5747\u6765\u5e97\u8005\u6570(1\u6b21\u5dee\u5206) ADF\u691c\u5b9a\nADF Statistic: -5.268\np-value: 0.000\n</pre> In\u00a0[23]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=[12, 4])\nplot_acf(df_diff, auto_ylims=True, lags=20, ax=axes[0])\nplot_pacf(df_diff, auto_ylims=True, lags=20, ax=axes[1])\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=[12, 4]) plot_acf(df_diff, auto_ylims=True, lags=20, ax=axes[0]) plot_pacf(df_diff, auto_ylims=True, lags=20, ax=axes[1]) plt.show() In\u00a0[24]: Copied! <pre>train = df_diff[:-52]\ntest = df_diff[-52:]\n\nfig, axes = plt.subplots(2, 1, figsize=[8, 8], sharex=True)\ndf['foot_traffic'].plot(ax=axes[0])\ndf_diff['foot_traffic'].plot(ax=axes[1])\naxes[0].set_ylabel('Avg. weekly foot traffic')\naxes[1].set_ylabel('Diff. avg. weekly traffic')\nfor i in range(2):\n    axes[i].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\n</pre> train = df_diff[:-52] test = df_diff[-52:]  fig, axes = plt.subplots(2, 1, figsize=[8, 8], sharex=True) df['foot_traffic'].plot(ax=axes[0]) df_diff['foot_traffic'].plot(ax=axes[1]) axes[0].set_ylabel('Avg. weekly foot traffic') axes[1].set_ylabel('Diff. avg. weekly traffic') for i in range(2):     axes[i].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) In\u00a0[28]: Copied! <pre>def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window :int, method: str) -&gt; list:\n    total_len = train_len + horizon\n    preds = []\n    if method == 'mean':\n        for i in range(train_len, total_len, window):\n            mean = np.mean(df[:i].values)\n            preds.extend(mean for _ in range(window))\n    \n    elif method == 'last':\n        for i in range(train_len, total_len, window):\n            last_val = df.iloc[i-1, 0]\n            preds.extend(last_val for _ in range(window))\n    \n    elif method == 'AR':\n        for i in range(train_len, total_len, window):\n            model = SARIMAX(df[:i], order=(3, 0, 0))\n            res = model.fit(disp=False)\n            pred = res.get_prediction(0, i + window - 1).predicted_mean.iloc[-window:]\n            preds.extend(pred)\n    else:\n        raise ValueError(f\"Invalid method: {method}, choose from ['mean', 'last', 'AR']\")\n    return preds\n</pre> def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window :int, method: str) -&gt; list:     total_len = train_len + horizon     preds = []     if method == 'mean':         for i in range(train_len, total_len, window):             mean = np.mean(df[:i].values)             preds.extend(mean for _ in range(window))          elif method == 'last':         for i in range(train_len, total_len, window):             last_val = df.iloc[i-1, 0]             preds.extend(last_val for _ in range(window))          elif method == 'AR':         for i in range(train_len, total_len, window):             model = SARIMAX(df[:i], order=(3, 0, 0))             res = model.fit(disp=False)             pred = res.get_prediction(0, i + window - 1).predicted_mean.iloc[-window:]             preds.extend(pred)     else:         raise ValueError(f\"Invalid method: {method}, choose from ['mean', 'last', 'AR']\")     return preds In\u00a0[29]: Copied! <pre>TRAIN_LEN = len(train)\nHORIZON = len(test)\nWINDOW = 1\n\nmethods = ['mean', 'last', 'AR']\nfor method in methods:\n    test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n</pre> TRAIN_LEN = len(train) HORIZON = len(test) WINDOW = 1  methods = ['mean', 'last', 'AR'] for method in methods:     test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method) <pre>/tmp/ipykernel_5331/2228618777.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n/tmp/ipykernel_5331/2228618777.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n</pre> In\u00a0[31]: Copied! <pre>fig, ax = plt.subplots()\ndf_diff['foot_traffic'].iloc[-80:].plot(ax=ax, label='actual')\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nax.set_ylabel('Diff. avg. weekly traffic')\nfor method in methods:\n    test[method].plot(ax=ax, label=method, ls='dashed')\nax.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() df_diff['foot_traffic'].iloc[-80:].plot(ax=ax, label='actual') ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) ax.set_ylabel('Diff. avg. weekly traffic') for method in methods:     test[method].plot(ax=ax, label=method, ls='dashed') ax.legend() plt.show() In\u00a0[33]: Copied! <pre>plt.xlabel('Methods')\nplt.ylabel('MSE')\nplt.bar(\n    methods,\n    [mean_squared_error(test['foot_traffic'], test[method]) for method in methods]\n)\nplt.show()\n</pre> plt.xlabel('Methods') plt.ylabel('MSE') plt.bar(     methods,     [mean_squared_error(test['foot_traffic'], test[method]) for method in methods] ) plt.show() In\u00a0[34]: Copied! <pre>df['pred_AR'] = pd.Series()\ndf['pred_AR'].iloc[-52:] = df['foot_traffic'].iloc[-52] + test['AR'].cumsum()\n\n# plot\nfig, ax = plt.subplots()\ndf['foot_traffic'].iloc[-90:].plot(ax=ax, label='actual')\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nax.set_ylabel('Avg. weekly foot traffic')\ndf['pred_AR'].iloc[-90:].plot(ax=ax, label=method, ls='dashed')\nax.legend()\nplt.show()\n</pre> df['pred_AR'] = pd.Series() df['pred_AR'].iloc[-52:] = df['foot_traffic'].iloc[-52] + test['AR'].cumsum()  # plot fig, ax = plt.subplots() df['foot_traffic'].iloc[-90:].plot(ax=ax, label='actual') ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) ax.set_ylabel('Avg. weekly foot traffic') df['pred_AR'].iloc[-90:].plot(ax=ax, label=method, ls='dashed') ax.legend() plt.show() <pre>/tmp/ipykernel_5331/3534851149.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['pred_AR'].iloc[-52:] = df['foot_traffic'].iloc[-52] + test['AR'].cumsum()\n</pre> In\u00a0[35]: Copied! <pre># calculate MAE\nprint(f\"MAE: {mean_absolute_error(df['foot_traffic'].iloc[-52:], df['pred_AR'].iloc[-52:]):.3f}\")\n</pre> # calculate MAE print(f\"MAE: {mean_absolute_error(df['foot_traffic'].iloc[-52:], df['pred_AR'].iloc[-52:]):.3f}\") <pre>MAE: 3.478\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter05.html","title":"\u5c0f\u58f2\u5e97\u306e\u9031\u5e73\u5747\u6765\u5e97\u8005\u6570\u3092\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter05.html#ar","title":"\u81ea\u5df1\u56de\u5e30\u30d7\u30ed\u30bb\u30b9(AR)\u3092\u5b9a\u7fa9\u3059\u308b\u00b6","text":""},{"location":"chapter05.html#ar","title":"\u5b9a\u5e38\u7684\u306aAR\u30d7\u30ed\u30bb\u30b9\u306e\u6b21\u6570\u3092\u7279\u5b9a\u3059\u308b\u00b6","text":""},{"location":"chapter05.html#ar","title":"AR\u30d7\u30ed\u30bb\u30b9\u3092\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter06.html","title":"\u7b2c6\u7ae0 \u8907\u96d1\u306a\u6642\u7cfb\u5217\u306e\u30e2\u30c7\u30eb\u5316","text":"In\u00a0[1]: Copied! <pre>from typing import Union\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import product\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom tqdm.auto import tqdm\n</pre> from typing import Union import pandas as pd import numpy as np import matplotlib.pyplot as plt from itertools import product from statsmodels.tsa.arima_process import ArmaProcess from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import plot_acf, plot_pacf from statsmodels.stats.diagnostic import acorr_ljungbox from statsmodels.tsa.statespace.sarimax import SARIMAX from sklearn.metrics import mean_absolute_error, mean_squared_error from tqdm.auto import tqdm In\u00a0[2]: Copied! <pre>url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/bandwidth.csv'\ndf = pd.read_csv(url)\ndf.index = pd.date_range('2019-01-01', freq='H', periods=10000, name='Time')\n\n# plot\nfig, ax = plt.subplots()\nax.set_ylabel('Hourly bandwith usage (MBps)')\ndf['hourly_bandwidth'].plot(ax=ax)\nplt.show()\n</pre> url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/bandwidth.csv' df = pd.read_csv(url) df.index = pd.date_range('2019-01-01', freq='H', periods=10000, name='Time')  # plot fig, ax = plt.subplots() ax.set_ylabel('Hourly bandwith usage (MBps)') df['hourly_bandwidth'].plot(ax=ax) plt.show() In\u00a0[20]: Copied! <pre>ar1 = np.array([1, -0.33])\nma1 = np.array([1, 0.9])\n\nnp.random.seed(42)\n\nARMA_1_1 = ArmaProcess(ar1, ma1).generate_sample(nsample=1000)\n</pre> ar1 = np.array([1, -0.33]) ma1 = np.array([1, 0.9])  np.random.seed(42)  ARMA_1_1 = ArmaProcess(ar1, ma1).generate_sample(nsample=1000) In\u00a0[21]: Copied! <pre>ADF_result = adfuller(ARMA_1_1)\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n\n# plot correlation\nfig, axes = plt.subplots(1, 2, figsize=[12, 4])\nplot_acf(ARMA_1_1, auto_ylims=True, lags=20, ax=axes[0])\nplot_pacf(ARMA_1_1, auto_ylims=True, lags=20, ax=axes[1])\nplt.show()\n</pre> ADF_result = adfuller(ARMA_1_1) print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}')  # plot correlation fig, axes = plt.subplots(1, 2, figsize=[12, 4]) plot_acf(ARMA_1_1, auto_ylims=True, lags=20, ax=axes[0]) plot_pacf(ARMA_1_1, auto_ylims=True, lags=20, ax=axes[1]) plt.show() <pre>ADF Statistic: -6.430\np-value: 0.000\n</pre> In\u00a0[22]: Copied! <pre># \u4e00\u610f\u306aARMA(p,q)\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570\ndef optimize_ARMA(endog: Union[pd.Series, list], orders: list) -&gt; pd.DataFrame:\n    \"\"\"\n    endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\n    orders: ARMA(p,q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b\n    \"\"\"\n    res = []\n    for order in tqdm(orders):\n        try:\n            model = SARIMAX(\n                endog, \n                order=(order[0], 0, order[1]),\n                simple_differencing=False\n            ).fit(disp=False)\n            res.append([order, model.aic])\n        except:\n            continue\n    df_res = (\n        pd.DataFrame(res, columns=['(p,q)', 'AIC'])\n        .sort_values(by='AIC', ascending=True)\n        .reset_index(drop=True)\n    )\n    return df_res\n</pre> # \u4e00\u610f\u306aARMA(p,q)\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570 def optimize_ARMA(endog: Union[pd.Series, list], orders: list) -&gt; pd.DataFrame:     \"\"\"     endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf     orders: ARMA(p,q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b     \"\"\"     res = []     for order in tqdm(orders):         try:             model = SARIMAX(                 endog,                  order=(order[0], 0, order[1]),                 simple_differencing=False             ).fit(disp=False)             res.append([order, model.aic])         except:             continue     df_res = (         pd.DataFrame(res, columns=['(p,q)', 'AIC'])         .sort_values(by='AIC', ascending=True)         .reset_index(drop=True)     )     return df_res In\u00a0[23]: Copied! <pre>orders = list(product(range(4), range(4))) # p,q\u3092\u305d\u308c\u305e\u308c0~3\u307e\u3067\u52d5\u304b\u3059\ndf_res = optimize_ARMA(ARMA_1_1, orders)\ndf_res # \u304d\u3061\u3093\u3068(1,1)\u304c\u6700\u5c0f\u306b\u306a\u308b\u3088\u3046\u306b\u3059\u308b\u306b\u306f\u30b5\u30f3\u30d7\u30eb\u3092\u4f55\u5ea6\u304b\u53d6\u308a\u76f4\u3059\u5fc5\u8981\u304c\u3042\u3063\u305f\n</pre> orders = list(product(range(4), range(4))) # p,q\u3092\u305d\u308c\u305e\u308c0~3\u307e\u3067\u52d5\u304b\u3059 df_res = optimize_ARMA(ARMA_1_1, orders) df_res # \u304d\u3061\u3093\u3068(1,1)\u304c\u6700\u5c0f\u306b\u306a\u308b\u3088\u3046\u306b\u3059\u308b\u306b\u306f\u30b5\u30f3\u30d7\u30eb\u3092\u4f55\u5ea6\u304b\u53d6\u308a\u76f4\u3059\u5fc5\u8981\u304c\u3042\u3063\u305f <pre>  0%|          | 0/16 [00:00&lt;?, ?it/s]</pre> <pre>/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n  warn('Non-stationary starting autoregressive parameters'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n</pre> Out[23]: (p,q) AIC 0 (1, 1) 2801.407785 1 (2, 1) 2802.906070 2 (1, 2) 2802.967762 3 (0, 3) 2803.666793 4 (1, 3) 2804.524027 5 (3, 1) 2804.588567 6 (2, 2) 2804.822282 7 (3, 3) 2806.162939 8 (2, 3) 2806.175380 9 (3, 2) 2806.894930 10 (0, 2) 2812.840730 11 (0, 1) 2891.869245 12 (3, 0) 2981.643911 13 (2, 0) 3042.627787 14 (1, 0) 3207.291261 15 (0, 0) 3780.418416 In\u00a0[24]: Copied! <pre># \u6b8b\u5dee\u5206\u6790\nmodel = SARIMAX(ARMA_1_1, order=(1, 0, 1), simple_differencing=False).fit(disp=False)\nmodel.plot_diagnostics(figsize=(10, 8))\nplt.show()\n</pre> # \u6b8b\u5dee\u5206\u6790 model = SARIMAX(ARMA_1_1, order=(1, 0, 1), simple_differencing=False).fit(disp=False) model.plot_diagnostics(figsize=(10, 8)) plt.show() In\u00a0[25]: Copied! <pre># \u6b8b\u5dee\u304c\u7121\u76f8\u95a2\u304b\u306e\u8a55\u4fa1\nacorr_ljungbox(model.resid, np.arange(1, 11))\n</pre> # \u6b8b\u5dee\u304c\u7121\u76f8\u95a2\u304b\u306e\u8a55\u4fa1 acorr_ljungbox(model.resid, np.arange(1, 11)) Out[25]: lb_stat lb_pvalue 1 0.030706 0.860897 2 0.528021 0.767966 3 0.540904 0.909817 4 2.079774 0.721089 5 2.271897 0.810385 6 2.896262 0.821750 7 2.900378 0.894063 8 4.418799 0.817501 9 4.787567 0.852419 10 5.246805 0.874093 In\u00a0[9]: Copied! <pre>ADF_result = adfuller(df['hourly_bandwidth'])\nprint('\u5e2f\u57df\u4f7f\u7528\u91cfADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n\ndf_diff = df.diff().dropna()\nADF_result = adfuller(df_diff['hourly_bandwidth'])\nprint('\u5e2f\u57df\u4f7f\u7528\u91cf(1\u6b21\u5dee\u5206) ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n\nfig, axes = plt.subplots(1, 2, figsize=[12, 4])\ndf['hourly_bandwidth'].plot(ax=axes[0])\ndf_diff['hourly_bandwidth'].plot(ax=axes[1])\naxes[0].set_ylabel('Hourly bandwith usage (MBps')\naxes[1].set_ylabel('Hourly bandwith usage (MBps) (difference)')\nplt.show()\n</pre> ADF_result = adfuller(df['hourly_bandwidth']) print('\u5e2f\u57df\u4f7f\u7528\u91cfADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}')  df_diff = df.diff().dropna() ADF_result = adfuller(df_diff['hourly_bandwidth']) print('\u5e2f\u57df\u4f7f\u7528\u91cf(1\u6b21\u5dee\u5206) ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}')  fig, axes = plt.subplots(1, 2, figsize=[12, 4]) df['hourly_bandwidth'].plot(ax=axes[0]) df_diff['hourly_bandwidth'].plot(ax=axes[1]) axes[0].set_ylabel('Hourly bandwith usage (MBps') axes[1].set_ylabel('Hourly bandwith usage (MBps) (difference)') plt.show() <pre>\u5e2f\u57df\u4f7f\u7528\u91cfADF\u691c\u5b9a\nADF Statistic: -0.871\np-value: 0.797\n\u5e2f\u57df\u4f7f\u7528\u91cf(1\u6b21\u5dee\u5206) ADF\u691c\u5b9a\nADF Statistic: -20.695\np-value: 0.000\n</pre> In\u00a0[10]: Copied! <pre>train = df_diff[:-168]\ntest = df_diff[-168:]\n\nfig, axes = plt.subplots(2, 1, figsize=[8, 8], sharex=True)\ndf['hourly_bandwidth'].plot(ax=axes[0])\ndf_diff['hourly_bandwidth'].plot(ax=axes[1])\naxes[0].set_ylabel('Hourly bandwith usage (MBps)')\naxes[1].set_ylabel('Diff. Hourly bandwith usage (MBps)')\nfor i in range(2):\n    axes[i].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\n</pre> train = df_diff[:-168] test = df_diff[-168:]  fig, axes = plt.subplots(2, 1, figsize=[8, 8], sharex=True) df['hourly_bandwidth'].plot(ax=axes[0]) df_diff['hourly_bandwidth'].plot(ax=axes[1]) axes[0].set_ylabel('Hourly bandwith usage (MBps)') axes[1].set_ylabel('Diff. Hourly bandwith usage (MBps)') for i in range(2):     axes[i].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) In\u00a0[11]: Copied! <pre>optimize_ARMA(train['hourly_bandwidth'], orders)\n</pre> optimize_ARMA(train['hourly_bandwidth'], orders) <pre>  0%|          | 0/16 [00:00&lt;?, ?it/s]</pre> <pre>/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n</pre> Out[11]: (p,q) AIC 0 (3, 2) 27991.063879 1 (2, 3) 27991.287509 2 (2, 2) 27991.603598 3 (3, 3) 27993.416924 4 (1, 3) 28003.349550 5 (1, 2) 28051.351401 6 (3, 1) 28071.155496 7 (3, 0) 28095.618186 8 (2, 1) 28097.250766 9 (2, 0) 28098.407664 10 (1, 1) 28172.510044 11 (1, 0) 28941.056983 12 (0, 3) 31355.802141 13 (0, 2) 33531.179284 14 (0, 1) 39402.269523 15 (0, 0) 49035.184224 In\u00a0[12]: Copied! <pre># \u4e0a\u306e\u7d50\u679c\u304b\u3089\u4e0a\u4f4d3\u3064\u306e\u30da\u30a2\u306f\u6709\u671b\u305d\u3046\u3067\u3042\u308b\u3002\n# \u7279\u306b(2, 2)\u306f\u5909\u6570\u304c\u5c11\u306a\u3044\u306e\u3067\u8907\u96d1\u3067\u306a\u3044\u3002\n# (2, 2)\u30e2\u30c7\u30eb\u306e\u6b8b\u5dee\u5206\u6790\u3092\u884c\u3046\n# \u6b8b\u5dee\u5206\u6790\nmodel = SARIMAX(train['hourly_bandwidth'], order=(2, 0, 2), simple_differencing=False).fit(disp=False)\nmodel.plot_diagnostics(figsize=(10, 8))\nplt.show()\n</pre> # \u4e0a\u306e\u7d50\u679c\u304b\u3089\u4e0a\u4f4d3\u3064\u306e\u30da\u30a2\u306f\u6709\u671b\u305d\u3046\u3067\u3042\u308b\u3002 # \u7279\u306b(2, 2)\u306f\u5909\u6570\u304c\u5c11\u306a\u3044\u306e\u3067\u8907\u96d1\u3067\u306a\u3044\u3002 # (2, 2)\u30e2\u30c7\u30eb\u306e\u6b8b\u5dee\u5206\u6790\u3092\u884c\u3046 # \u6b8b\u5dee\u5206\u6790 model = SARIMAX(train['hourly_bandwidth'], order=(2, 0, 2), simple_differencing=False).fit(disp=False) model.plot_diagnostics(figsize=(10, 8)) plt.show() In\u00a0[13]: Copied! <pre># \u6b8b\u5dee\u304c\u7121\u76f8\u95a2\u304b\u306e\u8a55\u4fa1\nacorr_ljungbox(model.resid, np.arange(1, 11))\n</pre> # \u6b8b\u5dee\u304c\u7121\u76f8\u95a2\u304b\u306e\u8a55\u4fa1 acorr_ljungbox(model.resid, np.arange(1, 11)) Out[13]: lb_stat lb_pvalue 1 0.042190 0.837257 2 0.418364 0.811247 3 0.520271 0.914416 4 0.850554 0.931545 5 0.850841 0.973678 6 1.111754 0.981019 7 2.124864 0.952607 8 3.230558 0.919067 9 3.248662 0.953615 10 3.588289 0.964015 In\u00a0[14]: Copied! <pre>def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window :int, method: str) -&gt; list:\n    total_len = train_len + horizon\n    preds = []\n    if method == 'mean':\n        for i in range(train_len, total_len, window):\n            mean = np.mean(df[:i].values)\n            preds.extend(mean for _ in range(window))\n    \n    elif method == 'last':\n        for i in range(train_len, total_len, window):\n            last_val = df.iloc[i-1, 0]\n            preds.extend(last_val for _ in range(window))\n    \n    elif method == 'ARMA':\n        for i in range(train_len, total_len, window):\n            model = SARIMAX(df[:i], order=(2, 0, 2))\n            res = model.fit(disp=False)\n            pred = res.get_prediction(0, i + window - 1).predicted_mean.iloc[-window:]\n            preds.extend(pred)\n    else:\n        raise ValueError(f\"Invalid method: {method}, choose from ['mean', 'last', 'ARMA']\")\n    return preds\n</pre> def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window :int, method: str) -&gt; list:     total_len = train_len + horizon     preds = []     if method == 'mean':         for i in range(train_len, total_len, window):             mean = np.mean(df[:i].values)             preds.extend(mean for _ in range(window))          elif method == 'last':         for i in range(train_len, total_len, window):             last_val = df.iloc[i-1, 0]             preds.extend(last_val for _ in range(window))          elif method == 'ARMA':         for i in range(train_len, total_len, window):             model = SARIMAX(df[:i], order=(2, 0, 2))             res = model.fit(disp=False)             pred = res.get_prediction(0, i + window - 1).predicted_mean.iloc[-window:]             preds.extend(pred)     else:         raise ValueError(f\"Invalid method: {method}, choose from ['mean', 'last', 'ARMA']\")     return preds In\u00a0[15]: Copied! <pre>TRAIN_LEN = len(train)\nHORIZON = len(test)\nWINDOW = 2\n\nmethods = ['mean', 'last', 'ARMA']\nfor method in methods:\n    test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n</pre> TRAIN_LEN = len(train) HORIZON = len(test) WINDOW = 2  methods = ['mean', 'last', 'ARMA'] for method in methods:     test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method) <pre>/tmp/ipykernel_824/1314583584.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n/tmp/ipykernel_824/1314583584.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n/tmp/ipykernel_824/1314583584.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[method] = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, method)\n</pre> In\u00a0[16]: Copied! <pre>fig, ax = plt.subplots()\ndf_diff['hourly_bandwidth'].iloc[-200:].plot(ax=ax, label='actual')\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nax.set_ylabel('Diff. Hourly bandwith usage (MBps)')\nfor method in methods:\n    test[method].plot(ax=ax, label=method, ls='dashed')\nax.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() df_diff['hourly_bandwidth'].iloc[-200:].plot(ax=ax, label='actual') ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) ax.set_ylabel('Diff. Hourly bandwith usage (MBps)') for method in methods:     test[method].plot(ax=ax, label=method, ls='dashed') ax.legend() plt.show() In\u00a0[17]: Copied! <pre>plt.xlabel('Methods')\nplt.ylabel('MSE')\nplt.bar(\n    methods,\n    [mean_squared_error(test['hourly_bandwidth'], test[method]) for method in methods]\n)\nplt.show()\n</pre> plt.xlabel('Methods') plt.ylabel('MSE') plt.bar(     methods,     [mean_squared_error(test['hourly_bandwidth'], test[method]) for method in methods] ) plt.show() In\u00a0[18]: Copied! <pre>df['pred_ARMA'] = pd.Series()\ndf['pred_ARMA'].iloc[-168:] = df['hourly_bandwidth'].iloc[-168] + test['ARMA'].cumsum()\n\n# plot\nfig, ax = plt.subplots()\ndf['hourly_bandwidth'].iloc[-200:].plot(ax=ax, label='actual')\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nax.set_ylabel('Hourly bandwith usage (MBps)')\ndf['pred_ARMA'].iloc[-200:].plot(ax=ax, label=method, ls='dashed')\nax.legend()\nplt.show()\n</pre> df['pred_ARMA'] = pd.Series() df['pred_ARMA'].iloc[-168:] = df['hourly_bandwidth'].iloc[-168] + test['ARMA'].cumsum()  # plot fig, ax = plt.subplots() df['hourly_bandwidth'].iloc[-200:].plot(ax=ax, label='actual') ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) ax.set_ylabel('Hourly bandwith usage (MBps)') df['pred_ARMA'].iloc[-200:].plot(ax=ax, label=method, ls='dashed') ax.legend() plt.show() <pre>/tmp/ipykernel_824/2455051698.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['pred_ARMA'].iloc[-168:] = df['hourly_bandwidth'].iloc[-168] + test['ARMA'].cumsum()\n</pre> In\u00a0[19]: Copied! <pre># calculate MAE\nprint(f\"MAE: {mean_absolute_error(df['hourly_bandwidth'].iloc[-168:], df['pred_ARMA'].iloc[-168:]):.3f}\")\n</pre> # calculate MAE print(f\"MAE: {mean_absolute_error(df['hourly_bandwidth'].iloc[-168:], df['pred_ARMA'].iloc[-168:]):.3f}\") <pre>MAE: 14.000\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter06.html","title":"\u30c7\u30fc\u30bf\u30bb\u30f3\u30bf\u30fc\u306e\u5e2f\u57df\u5e45\u4f7f\u7528\u91cf\u3092\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter06.html#arma","title":"\u81ea\u5df1\u56de\u5e30\u79fb\u52d5\u5e73\u5747(ARMA)\u30d7\u30ed\u30bb\u30b9\u3092\u8abf\u3079\u308b\u00b6","text":""},{"location":"chapter06.html#arma","title":"\u5b9a\u5e38\u7684\u306aARMA\u30d7\u30ed\u30bb\u30b9\u3092\u7279\u5b9a\u3059\u308b\u00b6","text":"<p>$$ y_t = 0.33 y_{t-1} + 0.9 \\varepsilon_{t-1} + \\varepsilon_{t} $$</p>"},{"location":"chapter06.html","title":"\u4e00\u822c\u7684\u306a\u30e2\u30c7\u30eb\u5316\u624b\u7d9a\u304d\u3092\u8003\u3048\u51fa\u3059\u00b6","text":""},{"location":"chapter06.html","title":"\u4e00\u822c\u7684\u306a\u30e2\u30c7\u30eb\u5316\u624b\u7d9a\u304d\u3092\u9069\u7528\u3059\u308b\u00b6","text":""},{"location":"chapter06.html","title":"\u5e2f\u57df\u5e45\u4f7f\u7528\u91cf\u3092\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter07.html","title":"\u7b2c7\u7ae0 \u975e\u5b9a\u5e38\u6642\u7cfb\u5217\u306e\u4e88\u6e2c","text":"In\u00a0[27]: Copied! <pre>from typing import Union\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import product\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom tqdm.auto import tqdm\n</pre> from typing import Union import pandas as pd import numpy as np import matplotlib.pyplot as plt from itertools import product from statsmodels.tsa.stattools import adfuller from statsmodels.stats.diagnostic import acorr_ljungbox from statsmodels.tsa.statespace.sarimax import SARIMAX from tqdm.auto import tqdm In\u00a0[40]: Copied! <pre>url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/jj.csv'\ndf = pd.read_csv(url, parse_dates=['date'], index_col='date')\n\ntrain = df.iloc[:-4]\ntest = df.iloc[-4:]\n\n# plot\nfig, ax = plt.subplots()\nax.set_ylabel('EPS')\ndf['data'].plot(ax=ax)\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nplt.show()\n</pre> url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/jj.csv' df = pd.read_csv(url, parse_dates=['date'], index_col='date')  train = df.iloc[:-4] test = df.iloc[-4:]  # plot fig, ax = plt.subplots() ax.set_ylabel('EPS') df['data'].plot(ax=ax) ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) plt.show() In\u00a0[41]: Copied! <pre>for d in [0, 1, 2]:\n    ADF_result = adfuller(np.diff(df['data'], n=d))\n    print(f'EPS({d}\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\n    print(f'ADF Statistic: {ADF_result[0]:.3f}')\n    print(f'p-value: {ADF_result[1]:.3f}')\n</pre> for d in [0, 1, 2]:     ADF_result = adfuller(np.diff(df['data'], n=d))     print(f'EPS({d}\u6b21\u5dee\u5206)ADF\u691c\u5b9a')     print(f'ADF Statistic: {ADF_result[0]:.3f}')     print(f'p-value: {ADF_result[1]:.3f}') <pre>EPS(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: 2.742\np-value: 1.000\nEPS(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -0.407\np-value: 0.909\nEPS(2\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -3.585\np-value: 0.006\n</pre> In\u00a0[42]: Copied! <pre># \u4e00\u610f\u306aARIMA(p,q)\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570\ndef optimize_ARIMA(endog: Union[pd.Series, list], orders: list, d: int) -&gt; pd.DataFrame:\n    \"\"\"\n    endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\n    orders: ARIMA(p, d, q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b\n    d: \u548c\u5206\u6b21\u6570\n    \"\"\"\n    res = []\n    for order in tqdm(orders):\n        try:\n            model = SARIMAX(\n                endog, \n                order=(order[0], d, order[1]),\n                simple_differencing=False\n            ).fit(disp=False)\n            res.append([order, model.aic])\n        except:\n            continue\n    df_res = (\n        pd.DataFrame(res, columns=['(p,q)', 'AIC'])\n        .sort_values(by='AIC', ascending=True)\n        .reset_index(drop=True)\n    )\n    return df_res\n</pre> # \u4e00\u610f\u306aARIMA(p,q)\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570 def optimize_ARIMA(endog: Union[pd.Series, list], orders: list, d: int) -&gt; pd.DataFrame:     \"\"\"     endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf     orders: ARIMA(p, d, q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b     d: \u548c\u5206\u6b21\u6570     \"\"\"     res = []     for order in tqdm(orders):         try:             model = SARIMAX(                 endog,                  order=(order[0], d, order[1]),                 simple_differencing=False             ).fit(disp=False)             res.append([order, model.aic])         except:             continue     df_res = (         pd.DataFrame(res, columns=['(p,q)', 'AIC'])         .sort_values(by='AIC', ascending=True)         .reset_index(drop=True)     )     return df_res In\u00a0[43]: Copied! <pre>d = 2 # \u4e0a\u306eADF\u691c\u5b9a\u304b\u3089\u548c\u5206\u6b21\u6570\u306f2\u306b\u6c7a\u5b9a\norders = list(product(range(4), range(4))) # p,q\u3092\u305d\u308c\u305e\u308c0~3\u307e\u3067\u52d5\u304b\u3059\noptimize_ARIMA(df['data'], orders, 2)\n</pre> d = 2 # \u4e0a\u306eADF\u691c\u5b9a\u304b\u3089\u548c\u5206\u6b21\u6570\u306f2\u306b\u6c7a\u5b9a orders = list(product(range(4), range(4))) # p,q\u3092\u305d\u308c\u305e\u308c0~3\u307e\u3067\u52d5\u304b\u3059 optimize_ARIMA(df['data'], orders, 2) <pre>  0%|          | 0/16 [00:00&lt;?, ?it/s]</pre> <pre>/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n  warn('Non-stationary starting autoregressive parameters'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n  warn('Non-stationary starting autoregressive parameters'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n  warn('Non-stationary starting autoregressive parameters'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n  warn('Non-stationary starting autoregressive parameters'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n  warn('Non-stationary starting autoregressive parameters'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n</pre> Out[43]: (p,q) AIC 0 (3, 3) 117.898033 1 (3, 1) 119.338719 2 (3, 2) 119.504218 3 (3, 0) 160.819665 4 (0, 3) 218.838417 5 (0, 2) 241.466413 6 (2, 3) 246.507250 7 (1, 3) 254.710421 8 (1, 2) 258.390204 9 (2, 2) 258.974897 10 (2, 1) 268.110244 11 (1, 1) 268.199857 12 (0, 1) 299.164245 13 (1, 0) 316.439752 14 (2, 0) 316.822281 15 (0, 0) 382.017235 In\u00a0[44]: Copied! <pre># (3, 3)\u30e2\u30c7\u30eb\u306e\u6b8b\u5dee\u5206\u6790\u3092\u884c\u3046\n# \u6b8b\u5dee\u5206\u6790\nmodel = SARIMAX(train['data'], order=(3, d, 3), simple_differencing=False).fit(disp=False)\nmodel.plot_diagnostics(figsize=(10, 8))\nplt.show()\n</pre> # (3, 3)\u30e2\u30c7\u30eb\u306e\u6b8b\u5dee\u5206\u6790\u3092\u884c\u3046 # \u6b8b\u5dee\u5206\u6790 model = SARIMAX(train['data'], order=(3, d, 3), simple_differencing=False).fit(disp=False) model.plot_diagnostics(figsize=(10, 8)) plt.show() <pre>/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n  warn('Non-stationary starting autoregressive parameters'\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n</pre> In\u00a0[45]: Copied! <pre># \u6b8b\u5dee\u304c\u7121\u76f8\u95a2\u304b\u306e\u8a55\u4fa1\nacorr_ljungbox(model.resid, np.arange(1, 11))\n</pre> # \u6b8b\u5dee\u304c\u7121\u76f8\u95a2\u304b\u306e\u8a55\u4fa1 acorr_ljungbox(model.resid, np.arange(1, 11)) Out[45]: lb_stat lb_pvalue 1 1.652020 0.198684 2 1.654544 0.437241 3 7.276173 0.063597 4 9.231226 0.055573 5 9.853425 0.079497 6 10.097125 0.120621 7 10.346926 0.169751 8 10.379005 0.239426 9 10.721222 0.295303 10 11.159917 0.345196 In\u00a0[46]: Copied! <pre>test['naive_seasonal'] = train.iloc[-4:].values\ntest['pred_ARIMA'] = model.get_prediction(train.shape[0], train.shape[0] + 3).predicted_mean.values\n</pre> test['naive_seasonal'] = train.iloc[-4:].values test['pred_ARIMA'] = model.get_prediction(train.shape[0], train.shape[0] + 3).predicted_mean.values <pre>/tmp/ipykernel_1794/416178706.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test['naive_seasonal'] = train.iloc[-4:].values\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  return get_prediction_index(\n/home/yoneda/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n  return get_prediction_index(\n/tmp/ipykernel_1794/416178706.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test['pred_ARIMA'] = model.get_prediction(train.shape[0], train.shape[0] + 3).predicted_mean.values\n</pre> In\u00a0[51]: Copied! <pre>fig, ax = plt.subplots()\ndf['data'].iloc[-20:].plot(ax=ax, label='actual')\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nmethods = ['naive_seasonal', 'pred_ARIMA']\nfor method in methods:\n    test[method].plot(ax=ax, label=method, ls='dashed')\nax.legend()\n</pre> fig, ax = plt.subplots() df['data'].iloc[-20:].plot(ax=ax, label='actual') ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) methods = ['naive_seasonal', 'pred_ARIMA'] for method in methods:     test[method].plot(ax=ax, label=method, ls='dashed') ax.legend() Out[51]: <pre>&lt;matplotlib.legend.Legend at 0x7f0544ea64d0&gt;</pre> In\u00a0[54]: Copied! <pre>def mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred - y_true) / y_true))\n\nplt.xlabel('Methods')\nplt.ylabel('MAPE')\nplt.bar(\n    methods,\n    [mape(test['data'], test[method]) for method in methods]\n)\nplt.gca().yaxis.set_major_formatter(lambda y, _: '{:.0%}'.format(y))\nplt.show()\n</pre> def mape(y_true, y_pred):     return np.mean(np.abs((y_pred - y_true) / y_true))  plt.xlabel('Methods') plt.ylabel('MAPE') plt.bar(     methods,     [mape(test['data'], test[method]) for method in methods] ) plt.gca().yaxis.set_major_formatter(lambda y, _: '{:.0%}'.format(y)) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter07.html#arima","title":"\u81ea\u5df1\u56de\u5e30\u548c\u5206\u79fb\u52d5\u5e73\u5747(ARIMA)\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3059\u308b\u00b6","text":""},{"location":"chapter07.html","title":"\u975e\u5b9a\u5e38\u6642\u7cfb\u5217\u3092\u8003\u616e\u306e\u5bfe\u51e6\u306b\u3059\u308b\u305f\u3081\u306b\u4e00\u822c\u7684\u306a\u30e2\u30c7\u30eb\u5316\u624b\u7d9a\u304d\u3092\u5909\u66f4\u3059\u308b\u00b6","text":""},{"location":"chapter07.html","title":"\u975e\u5b9a\u5e38\u6642\u7cfb\u5217\u3092\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter08.html","title":"\u7b2c8\u7ae0 \u5b63\u7bc0\u6027\u306e\u8003\u616e","text":"In\u00a0[1]: Copied! <pre>from typing import Union\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import product\nfrom statsmodels.tsa.seasonal import STL\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> from typing import Union import pandas as pd import numpy as np import matplotlib.pyplot as plt from itertools import product from statsmodels.tsa.seasonal import STL from statsmodels.tsa.stattools import adfuller from statsmodels.stats.diagnostic import acorr_ljungbox from statsmodels.tsa.statespace.sarimax import SARIMAX from tqdm.auto import tqdm import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre>url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/air-passengers.csv'\ndf = pd.read_csv(url, index_col='Month')\ndf.index = pd.to_datetime(df.index, format=('%Y-%m')).to_period('M')\n</pre> url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/air-passengers.csv' df = pd.read_csv(url, index_col='Month') df.index = pd.to_datetime(df.index, format=('%Y-%m')).to_period('M') In\u00a0[3]: Copied! <pre>fig, ax = plt.subplots()\nax.set_ylabel('Number of air passengers')\ndf['Passengers'].plot(ax=ax)\nser_month7 = df[df.index.month == 7]['Passengers']\nplt.scatter(ser_month7.index, ser_month7, color='tab:orange')\nfor year_start in df.index[df.index.month==1]:\n    ax.axvline(year_start, color='gray', ls='dashed')\nplt.show()\n</pre> fig, ax = plt.subplots() ax.set_ylabel('Number of air passengers') df['Passengers'].plot(ax=ax) ser_month7 = df[df.index.month == 7]['Passengers'] plt.scatter(ser_month7.index, ser_month7, color='tab:orange') for year_start in df.index[df.index.month==1]:     ax.axvline(year_start, color='gray', ls='dashed') plt.show() In\u00a0[4]: Copied! <pre>decomposition = STL(df['Passengers'], period=12).fit()\n\nfig, axes = plt.subplots(4, 1, sharex=True, figsize=(10, 8))\n\naxes[0].set_ylabel('Observed')\ndecomposition.observed.plot(ax=axes[0])\n\naxes[1].set_ylabel('Trend')\ndecomposition.trend.plot(ax=axes[1])\n\naxes[2].set_ylabel('Seasonal')\ndecomposition.seasonal.plot(ax=axes[2])\n\naxes[3].set_ylabel('Residuals')\ndecomposition.resid.plot(ax=axes[3])\n\nfig.autofmt_xdate()\nplt.tight_layout()\n</pre> decomposition = STL(df['Passengers'], period=12).fit()  fig, axes = plt.subplots(4, 1, sharex=True, figsize=(10, 8))  axes[0].set_ylabel('Observed') decomposition.observed.plot(ax=axes[0])  axes[1].set_ylabel('Trend') decomposition.trend.plot(ax=axes[1])  axes[2].set_ylabel('Seasonal') decomposition.seasonal.plot(ax=axes[2])  axes[3].set_ylabel('Residuals') decomposition.resid.plot(ax=axes[3])  fig.autofmt_xdate() plt.tight_layout() In\u00a0[5]: Copied! <pre># \u6bd4\u8f03\u306e\u305f\u3081\u306b\u7dda\u5f62\u306e\u5024\u3092\u5165\u308c\u305f\u3082\u306e\u306eSTL\u3092\u898b\u3066\u307f\u308b\n# \u6b8b\u5dee\u6210\u5206\u304c0\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304c\u78ba\u8a8d\u3067\u304d\u308b\ndf_linear = pd.DataFrame(\n    data={'linear': np.linspace(0, 150, df.index.shape[0])},\n    index=df.index\n)\ndecomposition = STL(df_linear['linear'], period=12).fit()\n\nfig, axes = plt.subplots(4, 1, sharex=True, figsize=(10, 8))\n\naxes[0].set_ylabel('Observed')\ndecomposition.observed.plot(ax=axes[0])\n\naxes[1].set_ylabel('Trend')\ndecomposition.trend.plot(ax=axes[1])\n\naxes[2].set_ylabel('Seasonal')\naxes[2].set_ylim(-1, 1)\ndecomposition.seasonal.plot(ax=axes[2])\n\naxes[3].set_ylabel('Residuals')\naxes[3].set_ylim(-1, 1)\ndecomposition.resid.plot(ax=axes[3])\n\nfig.autofmt_xdate()\nplt.tight_layout()\n</pre> # \u6bd4\u8f03\u306e\u305f\u3081\u306b\u7dda\u5f62\u306e\u5024\u3092\u5165\u308c\u305f\u3082\u306e\u306eSTL\u3092\u898b\u3066\u307f\u308b # \u6b8b\u5dee\u6210\u5206\u304c0\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u304c\u78ba\u8a8d\u3067\u304d\u308b df_linear = pd.DataFrame(     data={'linear': np.linspace(0, 150, df.index.shape[0])},     index=df.index ) decomposition = STL(df_linear['linear'], period=12).fit()  fig, axes = plt.subplots(4, 1, sharex=True, figsize=(10, 8))  axes[0].set_ylabel('Observed') decomposition.observed.plot(ax=axes[0])  axes[1].set_ylabel('Trend') decomposition.trend.plot(ax=axes[1])  axes[2].set_ylabel('Seasonal') axes[2].set_ylim(-1, 1) decomposition.seasonal.plot(ax=axes[2])  axes[3].set_ylabel('Residuals') axes[3].set_ylim(-1, 1) decomposition.resid.plot(ax=axes[3])  fig.autofmt_xdate() plt.tight_layout() In\u00a0[6]: Copied! <pre>train = df.iloc[:-12]\ntest = df.iloc[-12:]\n\n# plot\nfig, ax = plt.subplots()\nax.set_ylabel('Number of air passengers')\ndf['Passengers'].plot(ax=ax)\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nplt.show()\n</pre> train = df.iloc[:-12] test = df.iloc[-12:]  # plot fig, ax = plt.subplots() ax.set_ylabel('Number of air passengers') df['Passengers'].plot(ax=ax) ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) plt.show() In\u00a0[7]: Copied! <pre>for d in [0, 1, 2]:\n    ADF_result = adfuller(np.diff(df['Passengers'], n=d))\n    print(f'\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570({d}\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\n    print(f'ADF Statistic: {ADF_result[0]:.3f}')\n    print(f'p-value: {ADF_result[1]:.3f}')\n</pre> for d in [0, 1, 2]:     ADF_result = adfuller(np.diff(df['Passengers'], n=d))     print(f'\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570({d}\u6b21\u5dee\u5206)ADF\u691c\u5b9a')     print(f'ADF Statistic: {ADF_result[0]:.3f}')     print(f'p-value: {ADF_result[1]:.3f}') <pre>\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: 0.815\np-value: 0.992\n\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -2.829\np-value: 0.054\n\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(2\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -16.384\np-value: 0.000\n</pre> In\u00a0[8]: Copied! <pre># \u4e00\u610f\u306aSARIMA(p,q)\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570\ndef optimize_SARIMA(endog: Union[pd.Series, list], orders: list, d: int, D: int, s: int) -&gt; pd.DataFrame:\n    \"\"\"\n    endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\n    orders: ARIMA(p, d, q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b\n    d: \u548c\u5206\u6b21\u6570\n    D: \u5b63\u7bc0\u5dee\u5206\u306e\u6b21\u6570\n    s: \u983b\u5ea6\n    \"\"\"\n    res = []\n    for order in tqdm(orders):\n        try:\n            model = SARIMAX(\n                endog, \n                order=(order[0], d, order[1]),\n                seasonal_order=(order[2], D, order[3], s), \n                simple_differencing=False\n            ).fit(disp=False)\n            res.append([order, model.aic])\n        except:\n            continue\n    df_res = (\n        pd.DataFrame(res, columns=['(p,q,P,Q)', 'AIC'])\n        .sort_values(by='AIC', ascending=True)\n        .reset_index(drop=True)\n    )\n    return df_res\n</pre> # \u4e00\u610f\u306aSARIMA(p,q)\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570 def optimize_SARIMA(endog: Union[pd.Series, list], orders: list, d: int, D: int, s: int) -&gt; pd.DataFrame:     \"\"\"     endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf     orders: ARIMA(p, d, q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b     d: \u548c\u5206\u6b21\u6570     D: \u5b63\u7bc0\u5dee\u5206\u306e\u6b21\u6570     s: \u983b\u5ea6     \"\"\"     res = []     for order in tqdm(orders):         try:             model = SARIMAX(                 endog,                  order=(order[0], d, order[1]),                 seasonal_order=(order[2], D, order[3], s),                  simple_differencing=False             ).fit(disp=False)             res.append([order, model.aic])         except:             continue     df_res = (         pd.DataFrame(res, columns=['(p,q,P,Q)', 'AIC'])         .sort_values(by='AIC', ascending=True)         .reset_index(drop=True)     )     return df_res In\u00a0[13]: Copied! <pre>d = 2 # \u5dee\u5206\u30922\u56de\u53d6\u308b\nD = 0 # ARIMA(p, d, q)\u30e2\u30c7\u30eb\u3092\u4f7f\u3046\ns = 12 # statsmodels\u306b\u304a\u3044\u3066s=\u983b\u5ea6(m)\u3067\u3042\u308b\u3001\u4eca\u56de\u306f\u6708\u6b21\u306a\u306e\u306712\n\n# p, q\u3092\u305d\u308c\u305e\u308c0~12\u307e\u3067\u52d5\u304b\u3059\n# P, Q\u306f\u30bc\u30ed\u306b\u56fa\u5b9a\norders = list(product(range(13), range(13), [0], [0]))\noptimize_SARIMA(train['Passengers'], orders, d, D, s) # \u306a\u305c\u304b(10, 12, 0, 0)\u306eAIC\u304c\u5c0f\u3055\u3059\u304e\u308b\n</pre> d = 2 # \u5dee\u5206\u30922\u56de\u53d6\u308b D = 0 # ARIMA(p, d, q)\u30e2\u30c7\u30eb\u3092\u4f7f\u3046 s = 12 # statsmodels\u306b\u304a\u3044\u3066s=\u983b\u5ea6(m)\u3067\u3042\u308b\u3001\u4eca\u56de\u306f\u6708\u6b21\u306a\u306e\u306712  # p, q\u3092\u305d\u308c\u305e\u308c0~12\u307e\u3067\u52d5\u304b\u3059 # P, Q\u306f\u30bc\u30ed\u306b\u56fa\u5b9a orders = list(product(range(13), range(13), [0], [0])) optimize_SARIMA(train['Passengers'], orders, d, D, s) # \u306a\u305c\u304b(10, 12, 0, 0)\u306eAIC\u304c\u5c0f\u3055\u3059\u304e\u308b <pre>  0%|          | 0/169 [00:00&lt;?, ?it/s]</pre> Out[13]: (p,q, P, Q) AIC 0 (10, 12, 0, 0) 64.550735 1 (11, 3, 0, 0) 1016.842652 2 (11, 4, 0, 0) 1019.034751 3 (11, 5, 0, 0) 1020.379289 4 (11, 1, 0, 0) 1021.025922 ... ... ... 164 (5, 0, 0, 0) 1281.732157 165 (3, 0, 0, 0) 1300.282335 166 (2, 0, 0, 0) 1302.913196 167 (1, 0, 0, 0) 1308.152194 168 (0, 0, 0, 0) 1311.919269 <p>169 rows \u00d7 2 columns</p> In\u00a0[20]: Copied! <pre>model_ARIMA = SARIMAX(train, order=(11, d, 3), simple_differencing=False).fit(disp=False)\n\nmodel_ARIMA.plot_diagnostics(figsize=(10, 8))\nplt.tight_layout()\n</pre> model_ARIMA = SARIMAX(train, order=(11, d, 3), simple_differencing=False).fit(disp=False)  model_ARIMA.plot_diagnostics(figsize=(10, 8)) plt.tight_layout() In\u00a0[22]: Copied! <pre># \u6b8b\u5dee\u81ea\u4f53\u306f\u7121\u76f8\u95a2\u3067\u306f\u306a\u3044\n# 1, 2\u756a\u76ee\u304c0.05\u3088\u308a\u3082\u5c0f\u3055\u3044\n# ARIMA\u306e\u9650\u754c\u3092\u793a\u3057\u3066\u3044\u308b\nacorr_ljungbox(model_ARIMA.resid, np.arange(1, 11))\n</pre> # \u6b8b\u5dee\u81ea\u4f53\u306f\u7121\u76f8\u95a2\u3067\u306f\u306a\u3044 # 1, 2\u756a\u76ee\u304c0.05\u3088\u308a\u3082\u5c0f\u3055\u3044 # ARIMA\u306e\u9650\u754c\u3092\u793a\u3057\u3066\u3044\u308b acorr_ljungbox(model_ARIMA.resid, np.arange(1, 11)) Out[22]: lb_stat lb_pvalue 1 6.539124 0.010553 2 6.667840 0.035653 3 6.943387 0.073724 4 7.736593 0.101718 5 8.377928 0.136601 6 8.669212 0.193054 7 9.493241 0.219155 8 9.731307 0.284393 9 9.741755 0.371793 10 11.546080 0.316583 In\u00a0[28]: Copied! <pre># \u5b9a\u5e38\u6027\u306e\u78ba\u8a8d\nADF_result = adfuller(df['Passengers'])\nprint(f'\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\nADF_result = adfuller(np.diff(df['Passengers']))\nprint(f'\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n# \u5b63\u7bc0\u5dee\u5206\u3092\u4f7f\u3046\u3068m=12\nADF_result = adfuller(np.diff(np.diff(df['Passengers']), n=12))\nprint(f'\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(1\u6b21\u5dee\u5206\u3001\u5b63\u7bc01\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n</pre> # \u5b9a\u5e38\u6027\u306e\u78ba\u8a8d ADF_result = adfuller(df['Passengers']) print(f'\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}') ADF_result = adfuller(np.diff(df['Passengers'])) print(f'\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}') # \u5b63\u7bc0\u5dee\u5206\u3092\u4f7f\u3046\u3068m=12 ADF_result = adfuller(np.diff(np.diff(df['Passengers']), n=12)) print(f'\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(1\u6b21\u5dee\u5206\u3001\u5b63\u7bc01\u6b21\u5dee\u5206)ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}') <pre>\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: 0.815\np-value: 0.992\n\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -2.829\np-value: 0.054\n\u6708\u9593\u822a\u7a7a\u65c5\u5ba2\u6570(1\u6b21\u5dee\u5206\u3001\u5b63\u7bc01\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -17.625\np-value: 0.000\n</pre> In\u00a0[29]: Copied! <pre>d = 1 # \u5dee\u5206\u30922\u56de\u53d6\u308b\nD = 1 # ARIMA(p, d, q)\u30e2\u30c7\u30eb\u3092\u4f7f\u3046\ns = 12 # statsmodels\u306b\u304a\u3044\u3066s=\u983b\u5ea6(m)\u3067\u3042\u308b\u3001\u4eca\u56de\u306f\u6708\u6b21\u306a\u306e\u306712\n\n# p,q,P,Q\u30920~3\u307e\u3067\u52d5\u304b\u3059\norders = list(product(range(4), repeat=4))\noptimize_SARIMA(train['Passengers'], orders, d, D, s)\n</pre> d = 1 # \u5dee\u5206\u30922\u56de\u53d6\u308b D = 1 # ARIMA(p, d, q)\u30e2\u30c7\u30eb\u3092\u4f7f\u3046 s = 12 # statsmodels\u306b\u304a\u3044\u3066s=\u983b\u5ea6(m)\u3067\u3042\u308b\u3001\u4eca\u56de\u306f\u6708\u6b21\u306a\u306e\u306712  # p,q,P,Q\u30920~3\u307e\u3067\u52d5\u304b\u3059 orders = list(product(range(4), repeat=4)) optimize_SARIMA(train['Passengers'], orders, d, D, s) <pre>  0%|          | 0/256 [00:00&lt;?, ?it/s]</pre> Out[29]: (p,q, P, Q) AIC 0 (2, 1, 1, 2) 892.245047 1 (2, 1, 1, 3) 894.082348 2 (1, 0, 1, 2) 894.287272 3 (2, 1, 2, 3) 894.476601 4 (0, 1, 1, 2) 894.991248 ... ... ... 251 (0, 0, 2, 0) 906.940147 252 (3, 2, 0, 3) 907.181875 253 (0, 0, 3, 2) 907.543943 254 (0, 0, 3, 0) 908.742583 255 (0, 0, 0, 3) 908.781405 <p>256 rows \u00d7 2 columns</p> In\u00a0[30]: Copied! <pre># \u6700\u3082AIC\u304c\u5c0f\u3055\u3044\u306e\u306f(2, 1, 1, 2)\nmodel_SARIMA = SARIMAX(train, order=(2, d, 1), seasonal_order=(1, D, 2, s), simple_differencing=False).fit(disp=False)\n\nmodel_SARIMA.plot_diagnostics(figsize=(10, 8))\nplt.tight_layout()\n</pre> # \u6700\u3082AIC\u304c\u5c0f\u3055\u3044\u306e\u306f(2, 1, 1, 2) model_SARIMA = SARIMAX(train, order=(2, d, 1), seasonal_order=(1, D, 2, s), simple_differencing=False).fit(disp=False)  model_SARIMA.plot_diagnostics(figsize=(10, 8)) plt.tight_layout() In\u00a0[33]: Copied! <pre># \u6b8b\u5dee\u304c\u30e9\u30f3\u30c0\u30e0\u3067\u3042\u308b\u3053\u3068\u3082\u78ba\u8a8d\u3067\u304d\u308b\n# SARIMA\u306b\u3088\u308b\u6709\u52b9\u6027\u304c\u78ba\u8a8d\u3067\u304d\u308b\nacorr_ljungbox(model_SARIMA.resid, np.arange(1, 11))\n</pre> # \u6b8b\u5dee\u304c\u30e9\u30f3\u30c0\u30e0\u3067\u3042\u308b\u3053\u3068\u3082\u78ba\u8a8d\u3067\u304d\u308b # SARIMA\u306b\u3088\u308b\u6709\u52b9\u6027\u304c\u78ba\u8a8d\u3067\u304d\u308b acorr_ljungbox(model_SARIMA.resid, np.arange(1, 11)) Out[33]: lb_stat lb_pvalue 1 0.004324 0.947568 2 0.746951 0.688338 3 1.022840 0.795726 4 1.227835 0.873495 5 1.437620 0.920154 6 1.713606 0.944066 7 2.309311 0.940759 8 2.721393 0.950608 9 2.737798 0.973790 10 4.976175 0.892764 In\u00a0[36]: Copied! <pre># \u6700\u5f8c\u306b\u4e88\u6e2c\u3092\u884c\u3063\u3066\u7d50\u679c\u306e\u6bd4\u8f03\u3092\u3059\u308b\ntest['naive_seasonal'] = train['Passengers'].iloc[-12:].values\ntest['ARIMA'] = model_ARIMA.forecast(12)\ntest['SARIMA'] = model_SARIMA.forecast(12)\n</pre> # \u6700\u5f8c\u306b\u4e88\u6e2c\u3092\u884c\u3063\u3066\u7d50\u679c\u306e\u6bd4\u8f03\u3092\u3059\u308b test['naive_seasonal'] = train['Passengers'].iloc[-12:].values test['ARIMA'] = model_ARIMA.forecast(12) test['SARIMA'] = model_SARIMA.forecast(12) In\u00a0[38]: Copied! <pre>fig, ax = plt.subplots()\ndf['Passengers'].iloc[-30:].plot(ax=ax, label='actual')\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nmethods = ['naive_seasonal', 'ARIMA', 'SARIMA']\nfor method in methods:\n    test[method].plot(ax=ax, label=method, ls='dashed')\nax.legend()\n</pre> fig, ax = plt.subplots() df['Passengers'].iloc[-30:].plot(ax=ax, label='actual') ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) methods = ['naive_seasonal', 'ARIMA', 'SARIMA'] for method in methods:     test[method].plot(ax=ax, label=method, ls='dashed') ax.legend() Out[38]: <pre>&lt;matplotlib.legend.Legend at 0x7fd3303f7220&gt;</pre> In\u00a0[39]: Copied! <pre>def mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred - y_true) / y_true))\n\nplt.xlabel('Methods')\nplt.ylabel('MAPE')\nplt.bar(\n    methods,\n    [mape(test['Passengers'], test[method]) for method in methods]\n)\nplt.gca().yaxis.set_major_formatter(lambda y, _: '{:.0%}'.format(y))\nplt.show()\n</pre> def mape(y_true, y_pred):     return np.mean(np.abs((y_pred - y_true) / y_true))  plt.xlabel('Methods') plt.ylabel('MAPE') plt.bar(     methods,     [mape(test['Passengers'], test[method]) for method in methods] ) plt.gca().yaxis.set_major_formatter(lambda y, _: '{:.0%}'.format(y)) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter08.html#sarima","title":"\u5b63\u7bc0\u81ea\u5df1\u56de\u5e30\u548c\u5206\u79fb\u52d5\u5e73\u5747(SARIMA)\u30e2\u30c7\u30eb\u3092\u8abf\u3079\u308b\u00b6","text":""},{"location":"chapter08.html","title":"\u30c7\u30fc\u30bf\u3068\u305d\u306e\u9069\u5207\u306a\u983b\u5ea6\u00b6","text":"\u30c7\u30fc\u30bf\u53ce\u96c6 \u983b\u5ea6$m$ \u5e74\u6b21 1 \u56db\u534a\u671f\u6b21 4 \u6708\u6b21 12 \u9031\u6b21 52"},{"location":"chapter08.html","title":"\u65e5\u6b21\u307e\u305f\u306f\u305d\u308c\u3088\u308a\u3082\u77ed\u3044\u30c7\u30fc\u30bf\u306e\u5468\u671f\u306b\u5fdc\u3058\u305f\u983b\u5ea6\u00b6","text":"\u30c7\u30fc\u30bf\u53ce\u96c6 \u5206 \u6642 \u65e5 \u9031 \u5e74 \u6bce\u65e5 7 365 \u6bce\u6642 24 168 8766 \u6bce\u5206 60 1440 10080 525960 \u6bce\u79d2 60 3600 86400 604800 31557600"},{"location":"chapter08.html","title":"\u6642\u7cfb\u5217\u3067\u5b63\u7bc0\u30d1\u30bf\u30fc\u30f3\u3092\u7279\u5b9a\u3059\u308b\u00b6","text":""},{"location":"chapter08.html","title":"\u6708\u3054\u3068\u306e\u822a\u7a7a\u65c5\u5ba2\u6570\u3092\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter08.html#arimapdq","title":"ARIMA(p,d,q)\u30e2\u30c7\u30eb\u3067\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter08.html#sarimapdqpdq_m","title":"SARIMA(p,d,q)(P,D,Q)$_{m}$\u30e2\u30c7\u30eb\u3067\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter09.html","title":"\u7b2c9\u7ae0 \u30e2\u30c7\u30eb\u3078\u306e\u5916\u90e8\u5909\u6570\u306e\u8ffd\u52a0","text":"In\u00a0[12]: Copied! <pre>from typing import Union\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import product\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> from typing import Union import numpy as np import pandas as pd import matplotlib.pyplot as plt from itertools import product import statsmodels.api as sm from statsmodels.tsa.stattools import adfuller from statsmodels.tsa.statespace.sarimax import SARIMAX from statsmodels.stats.diagnostic import acorr_ljungbox from tqdm.auto import tqdm import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre>df_macro = sm.datasets.macrodata.load_pandas().data\ndf_macro.index = pd.PeriodIndex(\n    df_macro.apply(lambda ser: f\"{int(ser['year'])}Q{int(ser['quarter'])}\", axis=1),\n    freq='Q', name='date'\n)\ndf_macro.drop(columns=['year', 'quarter'], inplace=True)\ndf_macro.head(10)\n</pre> df_macro = sm.datasets.macrodata.load_pandas().data df_macro.index = pd.PeriodIndex(     df_macro.apply(lambda ser: f\"{int(ser['year'])}Q{int(ser['quarter'])}\", axis=1),     freq='Q', name='date' ) df_macro.drop(columns=['year', 'quarter'], inplace=True) df_macro.head(10) Out[2]: realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint date 1959Q1 2710.349 1707.4 286.898 470.045 1886.9 28.98 139.7 2.82 5.8 177.146 0.00 0.00 1959Q2 2778.801 1733.7 310.859 481.301 1919.7 29.15 141.7 3.08 5.1 177.830 2.34 0.74 1959Q3 2775.488 1751.8 289.226 491.260 1916.4 29.35 140.5 3.82 5.3 178.657 2.74 1.09 1959Q4 2785.204 1753.7 299.356 484.052 1931.3 29.37 140.0 4.33 5.6 179.386 0.27 4.06 1960Q1 2847.699 1770.5 331.722 462.199 1955.5 29.54 139.6 3.50 5.2 180.007 2.31 1.19 1960Q2 2834.390 1792.9 298.152 460.400 1966.1 29.55 140.2 2.68 5.2 180.671 0.14 2.55 1960Q3 2839.022 1785.8 296.375 474.676 1967.8 29.75 140.9 2.36 5.6 181.528 2.70 -0.34 1960Q4 2802.616 1788.2 259.764 476.434 1966.6 29.84 141.1 2.29 6.3 182.287 1.21 1.08 1961Q1 2819.264 1787.7 266.405 475.854 1984.5 29.81 142.1 2.37 6.8 182.992 -0.40 2.77 1961Q2 2872.005 1814.3 286.246 480.328 2014.4 29.92 142.9 2.29 7.0 183.691 1.47 0.81 \u5909\u6570 \u8aac\u660e realgdp \u5b9f\u8cea\u56fd\u5185\u7dcf\u751f\u7523(\u76ee\u7684\u5909\u6570\u307e\u305f\u306f\u5185\u751f\u5909\u6570) realcons \u5b9f\u8cea\u500b\u4eba\u6d88\u8cbb\u652f\u51fa realinv \u5b9f\u8cea\u6c11\u9593\u56fd\u5185\u7dcf\u6295\u8cc7\u984d realgovt \u9023\u90a6\u653f\u5e9c\u306e\u5b9f\u8cea\u6d88\u8cbb\u652f\u51fa\u304a\u3088\u3073\u6295\u8cc7\u984d realdpi \u5b9f\u8cea\u500b\u4eba\u53ef\u51e6\u5206\u6240\u5f97 cpi \u56db\u534a\u671f\u672b\u6d88\u8cbb\u8005\u7269\u4fa1\u6307\u6570 m1 M1\u540d\u76ee\u8ca8\u5e63\u4f9b\u7d66\u91cf tbilrate 3\u30f6\u6708\u7269\u77ed\u671f\u56fd\u50b5\u306e\u56db\u534a\u671f\u6708\u6b21\u5e73\u5747 unemp \u5b63\u7bc0\u8abf\u6574\u5931\u696d\u7387 pop \u56db\u534a\u671f\u672b\u4eba\u53e3 infl \u30a4\u30f3\u30d5\u30ec\u7387 realint \u5b9f\u8cea\u91d1\u5229\u7387 In\u00a0[3]: Copied! <pre>fig, axes = plt.subplots(2, 3, figsize=(11, 6), sharex=True)\naxes = axes.flatten()\nfor i in range(6):\n    df_macro.iloc[:, i].plot(ax=axes[i])\n    axes[i].set_title(df_macro.columns[i])\nfig.tight_layout()\n</pre> fig, axes = plt.subplots(2, 3, figsize=(11, 6), sharex=True) axes = axes.flatten() for i in range(6):     df_macro.iloc[:, i].plot(ax=axes[i])     axes[i].set_title(df_macro.columns[i]) fig.tight_layout() In\u00a0[29]: Copied! <pre># \u76ee\u7684\u5909\u6570\u3001\u5916\u751f\u5909\u6570\u3092\u5b9a\u7fa9\n# \u30b7\u30f3\u30d7\u30eb\u306e\u305f\u3081\u5916\u751f\u5909\u6570\u306f5\u3064\u306b\u5236\u9650\ntarget = df_macro[['realgdp']]\nexog = df_macro.iloc[:, 1:6]\n\n# train, test\u306b\u308f\u3051\u308b\ntarget_train = target[:200]\nexog_train = exog[:200]\n</pre> # \u76ee\u7684\u5909\u6570\u3001\u5916\u751f\u5909\u6570\u3092\u5b9a\u7fa9 # \u30b7\u30f3\u30d7\u30eb\u306e\u305f\u3081\u5916\u751f\u5909\u6570\u306f5\u3064\u306b\u5236\u9650 target = df_macro[['realgdp']] exog = df_macro.iloc[:, 1:6]  # train, test\u306b\u308f\u3051\u308b target_train = target[:200] exog_train = exog[:200] In\u00a0[31]: Copied! <pre># \u5b9a\u5e38\u6027\u306e\u78ba\u8a8d\nfor d in [0, 1]:\n    ADF_result = adfuller(np.diff(target['realgdp'], n=d))\n    print(f'\u5b9f\u8ceaGDP({d}\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\n    print(f'ADF Statistic: {ADF_result[0]:.3f}')\n    print(f'p-value: {ADF_result[1]:.3f}')\n</pre> # \u5b9a\u5e38\u6027\u306e\u78ba\u8a8d for d in [0, 1]:     ADF_result = adfuller(np.diff(target['realgdp'], n=d))     print(f'\u5b9f\u8ceaGDP({d}\u6b21\u5dee\u5206)ADF\u691c\u5b9a')     print(f'ADF Statistic: {ADF_result[0]:.3f}')     print(f'p-value: {ADF_result[1]:.3f}') <pre>\u5b9f\u8ceaGDP(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: 1.750\np-value: 0.998\n\u5b9f\u8ceaGDP(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -6.306\np-value: 0.000\n</pre> In\u00a0[32]: Copied! <pre># \u4e00\u610f\u306aSARIMAX\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570\ndef optimize_SARIMAX(\n    endog: Union[pd.Series, list], \n    exog: Union[pd.Series, list],\n    orders: list, \n    d: int,\n    D: int,\n    s: int\n) -&gt; pd.DataFrame:\n    \"\"\"\n    endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\n    exog: \u5916\u751f\u5909\u6570\n    orders: SARIMAX(p, d, q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b\n    d: \u548c\u5206\u6b21\u6570\n    D: \u5b63\u7bc0\u5dee\u5206\u306e\u6b21\u6570\n    s: \u983b\u5ea6\n    \"\"\"\n    res = []\n    for order in tqdm(orders):\n        try:\n            model = SARIMAX(\n                endog, \n                order=(order[0], d, order[1]),\n                seasonal_order=(order[2], D, order[3], s),\n                simple_differencing=False\n            ).fit(disp=False)\n            res.append([order, model.aic])\n        except:\n            continue\n    df_res = (\n        pd.DataFrame(res, columns=['(p,q,P,Q)', 'AIC'])\n        .sort_values(by='AIC', ascending=True)\n        .reset_index(drop=True)\n    )\n    return df_res\n</pre> # \u4e00\u610f\u306aSARIMAX\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570 def optimize_SARIMAX(     endog: Union[pd.Series, list],      exog: Union[pd.Series, list],     orders: list,      d: int,     D: int,     s: int ) -&gt; pd.DataFrame:     \"\"\"     endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf     exog: \u5916\u751f\u5909\u6570     orders: SARIMAX(p, d, q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b     d: \u548c\u5206\u6b21\u6570     D: \u5b63\u7bc0\u5dee\u5206\u306e\u6b21\u6570     s: \u983b\u5ea6     \"\"\"     res = []     for order in tqdm(orders):         try:             model = SARIMAX(                 endog,                  order=(order[0], d, order[1]),                 seasonal_order=(order[2], D, order[3], s),                 simple_differencing=False             ).fit(disp=False)             res.append([order, model.aic])         except:             continue     df_res = (         pd.DataFrame(res, columns=['(p,q,P,Q)', 'AIC'])         .sort_values(by='AIC', ascending=True)         .reset_index(drop=True)     )     return df_res In\u00a0[33]: Copied! <pre>d = 1\nD = 0\ns = 4\norders = list(product(range(4), repeat=4)) # p,q,P,Q\u3092\u305d\u308c\u305e\u308c0~4\u307e\u3067\u52d5\u304b\u3059\noptimize_SARIMAX(target_train, exog_train, orders, d, D, s)\n</pre> d = 1 D = 0 s = 4 orders = list(product(range(4), repeat=4)) # p,q,P,Q\u3092\u305d\u308c\u305e\u308c0~4\u307e\u3067\u52d5\u304b\u3059 optimize_SARIMAX(target_train, exog_train, orders, d, D, s) <pre>  0%|          | 0/256 [00:00&lt;?, ?it/s]</pre> Out[33]: (p,q,P,Q) AIC 0 (0, 1, 2, 3) 266.143045 1 (3, 2, 3, 0) 2145.460369 2 (3, 3, 1, 1) 2145.820653 3 (3, 3, 0, 1) 2145.979871 4 (3, 1, 3, 0) 2146.012430 ... ... ... 247 (0, 0, 0, 2) 2234.293056 248 (0, 0, 0, 3) 2235.079581 249 (0, 1, 0, 0) 2242.318380 250 (0, 0, 0, 1) 2250.954180 251 (0, 0, 0, 0) 2297.594527 <p>252 rows \u00d7 2 columns</p> In\u00a0[34]: Copied! <pre># (p,q,P,Q)=(0,1,2,3)\u304c\u306a\u305c\u304b\u4e00\u756a\u5c0f\u3055\u3044\n# \u30aa\u30fc\u30c0\u30fc\u3082\u5c0f\u3055\u3059\u304e\u308b\u6c17\u304c\u3059\u308b\u306e\u3067\u7121\u8996\u3059\u308b\nmodel = SARIMAX(target_train, exog_train, order=(3,1,3), seasonal_order=(0,0,0,4), simple_differencing=False).fit(disp=False)\nprint(model.summary())\n</pre> # (p,q,P,Q)=(0,1,2,3)\u304c\u306a\u305c\u304b\u4e00\u756a\u5c0f\u3055\u3044 # \u30aa\u30fc\u30c0\u30fc\u3082\u5c0f\u3055\u3059\u304e\u308b\u6c17\u304c\u3059\u308b\u306e\u3067\u7121\u8996\u3059\u308b model = SARIMAX(target_train, exog_train, order=(3,1,3), seasonal_order=(0,0,0,4), simple_differencing=False).fit(disp=False) print(model.summary()) <pre>                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                realgdp   No. Observations:                  200\nModel:               SARIMAX(3, 1, 3)   Log Likelihood                -859.412\nDate:                Fri, 17 Nov 2023   AIC                           1742.824\nTime:                        20:11:00   BIC                           1782.344\nSample:                    03-31-1959   HQIC                          1758.819\n                         - 12-31-2008                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nrealcons       0.9708      0.045     21.505      0.000       0.882       1.059\nrealinv        1.0132      0.033     30.693      0.000       0.948       1.078\nrealgovt       0.7283      0.127      5.728      0.000       0.479       0.977\nrealdpi        0.0102      0.025      0.408      0.683      -0.039       0.059\ncpi            5.8719      1.299      4.520      0.000       3.326       8.418\nar.L1          1.0636      0.402      2.646      0.008       0.276       1.852\nar.L2          0.4957      0.708      0.700      0.484      -0.892       1.883\nar.L3         -0.6766      0.340     -1.990      0.047      -1.343      -0.010\nma.L1         -1.1028      0.434     -2.541      0.011      -1.953      -0.252\nma.L2         -0.3284      0.775     -0.424      0.672      -1.847       1.191\nma.L3          0.6518      0.407      1.600      0.110      -0.146       1.450\nsigma2       331.2909     30.734     10.779      0.000     271.053     391.529\n===================================================================================\nLjung-Box (L1) (Q):                   0.01   Jarque-Bera (JB):                13.34\nProb(Q):                              0.92   Prob(JB):                         0.00\nHeteroskedasticity (H):               3.57   Skew:                             0.32\nProb(H) (two-sided):                  0.00   Kurtosis:                         4.10\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n</pre> In\u00a0[35]: Copied! <pre>model.plot_diagnostics(figsize=(10, 8))\nplt.tight_layout()\n</pre> model.plot_diagnostics(figsize=(10, 8)) plt.tight_layout() In\u00a0[36]: Copied! <pre># \u6b8b\u5dee\u304c\u7121\u76f8\u95a2\u3067\u3042\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u308b\nacorr_ljungbox(model.resid, np.arange(1, 11))\n</pre> # \u6b8b\u5dee\u304c\u7121\u76f8\u95a2\u3067\u3042\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u308b acorr_ljungbox(model.resid, np.arange(1, 11)) Out[36]: lb_stat lb_pvalue 1 0.092071 0.761560 2 0.198871 0.905348 3 0.200938 0.977438 4 1.257216 0.868591 5 1.258003 0.939193 6 1.285134 0.972471 7 1.476883 0.983099 8 1.486838 0.992924 9 1.859312 0.993502 10 1.866005 0.997268 In\u00a0[81]: Copied! <pre>def rolling_forecast(\n    endog: Union[pd.Series, list],\n    exog: Union[pd.Series, list],\n    train_len: int,\n    horizon: int,\n    window: int,\n    method: str\n) -&gt; list:\n    \n    total_len = train_len + horizon\n    preds = []\n    \n    if method == 'last':\n        for i in range(train_len, total_len, window):\n            val_last = endog.iloc[i-1, 0]\n            preds.extend(val_last for _ in range(window))\n    elif method == 'SARIMAX':\n        for i in range(train_len, total_len, window):\n            model = SARIMAX(\n                endog[:i],\n                exog[:i],\n                order=(3,1,3),\n                seasonal_order=(0,0,0,4),\n                simple_differencing=False\n            ).fit(disp=False)\n            pred = model.forecast(window, exog=exog[i:i+window]).values\n            preds.extend(pred)\n    else:\n        raise ValueError('choose method form [last, SARIMAX]')\n    return preds\n</pre> def rolling_forecast(     endog: Union[pd.Series, list],     exog: Union[pd.Series, list],     train_len: int,     horizon: int,     window: int,     method: str ) -&gt; list:          total_len = train_len + horizon     preds = []          if method == 'last':         for i in range(train_len, total_len, window):             val_last = endog.iloc[i-1, 0]             preds.extend(val_last for _ in range(window))     elif method == 'SARIMAX':         for i in range(train_len, total_len, window):             model = SARIMAX(                 endog[:i],                 exog[:i],                 order=(3,1,3),                 seasonal_order=(0,0,0,4),                 simple_differencing=False             ).fit(disp=False)             pred = model.forecast(window, exog=exog[i:i+window]).values             preds.extend(pred)     else:         raise ValueError('choose method form [last, SARIMAX]')     return preds In\u00a0[82]: Copied! <pre>target_train = target[:196]\ntarget_test = target[196:]\n\nTRAIN_LEN = len(target_train)\nHORIZON = len(target_test)\nWINDOW = 1\n\ntarget_test['pred_last'] = rolling_forecast(target, exog, TRAIN_LEN, HORIZON, WINDOW, 'last')\ntarget_test['pred_SARIMAX'] = rolling_forecast(target, exog, TRAIN_LEN, HORIZON, WINDOW, 'SARIMAX')\n</pre> target_train = target[:196] target_test = target[196:]  TRAIN_LEN = len(target_train) HORIZON = len(target_test) WINDOW = 1  target_test['pred_last'] = rolling_forecast(target, exog, TRAIN_LEN, HORIZON, WINDOW, 'last') target_test['pred_SARIMAX'] = rolling_forecast(target, exog, TRAIN_LEN, HORIZON, WINDOW, 'SARIMAX') In\u00a0[86]: Copied! <pre>def mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred - y_true) / y_true))\n\nxs = ['pred_last', 'pred_SARIMAX']\nys = [mape(target_test['realgdp'], target_test[x]) for x in xs]\n\nplt.bar(xs, ys)\nplt.xlabel('Baseline')\nplt.ylabel('MAPE')\n# set y value to percentage\nplt.gca().yaxis.set_major_formatter(lambda y, _: '{:.01%}'.format(y))\n</pre> def mape(y_true, y_pred):     return np.mean(np.abs((y_pred - y_true) / y_true))  xs = ['pred_last', 'pred_SARIMAX'] ys = [mape(target_test['realgdp'], target_test[x]) for x in xs]  plt.bar(xs, ys) plt.xlabel('Baseline') plt.ylabel('MAPE') # set y value to percentage plt.gca().yaxis.set_major_formatter(lambda y, _: '{:.01%}'.format(y)) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter09.html#sarimax","title":"SARIMAX\u30e2\u30c7\u30eb\u3092\u8abf\u3079\u308b\u00b6","text":""},{"location":"chapter09.html#sarimaxgdp","title":"SARIMAX\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u5b9f\u8ceaGDP\u3092\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter10.html","title":"\u7b2c10\u7ae0 \u8907\u6570\u306e\u6642\u7cfb\u5217\u306e\u4e88\u6e2c","text":"In\u00a0[47]: Copied! <pre>from typing import Union\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller, grangercausalitytests\nfrom statsmodels.tsa.statespace.varmax import VARMAX\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> from typing import Union import pandas as pd import numpy as np import matplotlib.pyplot as plt import statsmodels.api as sm from statsmodels.tsa.stattools import adfuller, grangercausalitytests from statsmodels.tsa.statespace.varmax import VARMAX from statsmodels.stats.diagnostic import acorr_ljungbox from tqdm.auto import tqdm import warnings warnings.filterwarnings('ignore') In\u00a0[6]: Copied! <pre>df_macro = sm.datasets.macrodata.load_pandas().data\ndf_macro.index = pd.PeriodIndex(\n    df_macro.apply(lambda ser: f\"{int(ser['year'])}Q{int(ser['quarter'])}\", axis=1),\n    freq='Q', name='date'\n)\ndf_macro.drop(columns=['year', 'quarter'], inplace=True)\ndf_macro = df_macro[['realdpi', 'realcons']]\ndf_macro.head(10)\n</pre> df_macro = sm.datasets.macrodata.load_pandas().data df_macro.index = pd.PeriodIndex(     df_macro.apply(lambda ser: f\"{int(ser['year'])}Q{int(ser['quarter'])}\", axis=1),     freq='Q', name='date' ) df_macro.drop(columns=['year', 'quarter'], inplace=True) df_macro = df_macro[['realdpi', 'realcons']] df_macro.head(10) Out[6]: realdpi realcons date 1959Q1 1886.9 1707.4 1959Q2 1919.7 1733.7 1959Q3 1916.4 1751.8 1959Q4 1931.3 1753.7 1960Q1 1955.5 1770.5 1960Q2 1966.1 1792.9 1960Q3 1967.8 1785.8 1960Q4 1966.6 1788.2 1961Q1 1984.5 1787.7 1961Q2 2014.4 1814.3 In\u00a0[9]: Copied! <pre>fig, axes = plt.subplots(2, 1, sharex=True)\nfor i in range(2):\n    axes[i].set_title(df_macro.columns[i])\n    df_macro.iloc[:, i].plot(ax=axes[i])\nfig.tight_layout()\n</pre> fig, axes = plt.subplots(2, 1, sharex=True) for i in range(2):     axes[i].set_title(df_macro.columns[i])     df_macro.iloc[:, i].plot(ax=axes[i]) fig.tight_layout() In\u00a0[14]: Copied! <pre># \u5b9a\u5e38\u6027\u306e\u78ba\u8a8d\nfor i in range(2):\n    for d in [0, 1]:\n        ADF_result = adfuller(np.diff(df_macro.iloc[:, i], n=d))\n        print(f'{df_macro.columns[i]}({d}\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\n        print(f'ADF Statistic: {ADF_result[0]:.3f}')\n        print(f'p-value: {ADF_result[1]:.3f}')\n    print('===================')\n</pre> # \u5b9a\u5e38\u6027\u306e\u78ba\u8a8d for i in range(2):     for d in [0, 1]:         ADF_result = adfuller(np.diff(df_macro.iloc[:, i], n=d))         print(f'{df_macro.columns[i]}({d}\u6b21\u5dee\u5206)ADF\u691c\u5b9a')         print(f'ADF Statistic: {ADF_result[0]:.3f}')         print(f'p-value: {ADF_result[1]:.3f}')     print('===================') <pre>realdpi(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: 2.986\np-value: 1.000\nrealdpi(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -8.865\np-value: 0.000\n===================\nrealcons(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: 1.550\np-value: 0.998\nrealcons(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -4.204\np-value: 0.001\n===================\n</pre> In\u00a0[17]: Copied! <pre># VAR\u30e2\u30c7\u30eb\u3092\u9069\u5408\u3055\u305b\u3066\u6700\u3082AIC\u304c\u5c0f\u3055\u304f\u306a\u308b\u30e2\u30c7\u30eb\u3092\u9078\u629e\u3059\u308b\u95a2\u6570\ndef optimize_VAR(endog: Union[pd.Series, list]) -&gt; pd.DataFrame:\n    res = []\n    # \u6b21\u6570\u309214\u307e\u3067\u5909\u5316\n    for i in tqdm(range(15)):\n        try:\n            model = VARMAX(endog, order=(i, 0)).fit(disp=False)\n        except:\n            continue\n        res.append([i, model.aic])\n    return pd.DataFrame(res, columns=['p', 'AIC']).sort_values('AIC').reset_index(drop=True)\n</pre> # VAR\u30e2\u30c7\u30eb\u3092\u9069\u5408\u3055\u305b\u3066\u6700\u3082AIC\u304c\u5c0f\u3055\u304f\u306a\u308b\u30e2\u30c7\u30eb\u3092\u9078\u629e\u3059\u308b\u95a2\u6570 def optimize_VAR(endog: Union[pd.Series, list]) -&gt; pd.DataFrame:     res = []     # \u6b21\u6570\u309214\u307e\u3067\u5909\u5316     for i in tqdm(range(15)):         try:             model = VARMAX(endog, order=(i, 0)).fit(disp=False)         except:             continue         res.append([i, model.aic])     return pd.DataFrame(res, columns=['p', 'AIC']).sort_values('AIC').reset_index(drop=True) In\u00a0[24]: Copied! <pre>train = df_macro.diff()[1:][:162]\ntest = df_macro.diff()[1:][162:]\n\noptimize_VAR(train)\n</pre> train = df_macro.diff()[1:][:162] test = df_macro.diff()[1:][162:]  optimize_VAR(train) <pre>  0%|          | 0/15 [00:00&lt;?, ?it/s]</pre> Out[24]: p AIC 0 3 3123.070078 1 5 3123.708523 2 6 3126.855594 3 4 3129.194047 4 2 3130.091668 5 7 3133.398358 6 1 3134.333343 7 8 3137.149011 8 9 3140.367177 9 10 3147.838680 10 11 3153.454928 11 12 3157.704333 12 13 3164.096650 13 14 3167.014463 In\u00a0[37]: Copied! <pre># p=3\u304c\u6700\u9069\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u305f\n# \u30b0\u30ec\u30f3\u30b8\u30e3\u30fc\u56e0\u679c\u6027\u306e\u691c\u5b9a\nprint(f'{df_macro.columns[1]} Granger-causes {df_macro.columns[0]}?')\ngranger1 = grangercausalitytests(\n    df_macro.diff().iloc[1:, [0, 1]],\n    maxlag=[3],\n)\nprint(f'\\n{df_macro.columns[0]} Granger-causes {df_macro.columns[1]}?')\ngranger1 = grangercausalitytests(\n    df_macro.diff().iloc[1:, [1, 0]],\n    maxlag=[3],\n)\n</pre> # p=3\u304c\u6700\u9069\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u305f # \u30b0\u30ec\u30f3\u30b8\u30e3\u30fc\u56e0\u679c\u6027\u306e\u691c\u5b9a print(f'{df_macro.columns[1]} Granger-causes {df_macro.columns[0]}?') granger1 = grangercausalitytests(     df_macro.diff().iloc[1:, [0, 1]],     maxlag=[3], ) print(f'\\n{df_macro.columns[0]} Granger-causes {df_macro.columns[1]}?') granger1 = grangercausalitytests(     df_macro.diff().iloc[1:, [1, 0]],     maxlag=[3], ) <pre>realcons Granger-causes realdpi?\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=9.2363  , p=0.0000  , df_denom=192, df_num=3\nssr based chi2 test:   chi2=28.7191 , p=0.0000  , df=3\nlikelihood ratio test: chi2=26.8268 , p=0.0000  , df=3\nparameter F test:         F=9.2363  , p=0.0000  , df_denom=192, df_num=3\n\nrealdpi Granger-causes realcons?\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=2.8181  , p=0.0403  , df_denom=192, df_num=3\nssr based chi2 test:   chi2=8.7625  , p=0.0326  , df=3\nlikelihood ratio test: chi2=8.5751  , p=0.0355  , df=3\nparameter F test:         F=2.8181  , p=0.0403  , df_denom=192, df_num=3\n</pre> In\u00a0[40]: Copied! <pre># p\u5024\u304c\u3044\u305a\u308c\u30820.05\u3088\u308a\u3082\u5c0f\u3055\u3044\u306e\u3067\u5b9f\u969b\u306b\u4e88\u6e2c\u306b\u9032\u3080\nmodel = VARMAX(train, order=(3, 0)).fit(disp=False)\n</pre> # p\u5024\u304c\u3044\u305a\u308c\u30820.05\u3088\u308a\u3082\u5c0f\u3055\u3044\u306e\u3067\u5b9f\u969b\u306b\u4e88\u6e2c\u306b\u9032\u3080 model = VARMAX(train, order=(3, 0)).fit(disp=False) In\u00a0[45]: Copied! <pre># \u6b8b\u5dee\u5206\u6790\nfor i in range(2):\n    model.plot_diagnostics(figsize=(10, 8), variable=i)\n    plt.tight_layout()\n</pre> # \u6b8b\u5dee\u5206\u6790 for i in range(2):     model.plot_diagnostics(figsize=(10, 8), variable=i)     plt.tight_layout() In\u00a0[57]: Copied! <pre># \u76f8\u95a2\u304c\u306a\u3044\u304b\u306e\u691c\u5b9a\u3082\u884c\u3046\nfor i in range(2):\n    print(f'{df_macro.columns[i]}')\n    display(acorr_ljungbox(model.resid.iloc[:, i], np.arange(1, 11)))\n</pre> # \u76f8\u95a2\u304c\u306a\u3044\u304b\u306e\u691c\u5b9a\u3082\u884c\u3046 for i in range(2):     print(f'{df_macro.columns[i]}')     display(acorr_ljungbox(model.resid.iloc[:, i], np.arange(1, 11))) <pre>realdpi\n</pre> lb_stat lb_pvalue 1 0.011641 0.914079 2 0.029410 0.985403 3 0.075796 0.994575 4 0.378020 0.984237 5 9.142892 0.103499 6 9.165748 0.164469 7 9.270487 0.233815 8 10.812127 0.212573 9 13.318498 0.148716 10 15.287698 0.121920 <pre>realcons\n</pre> lb_stat lb_pvalue 1 0.023757 0.877505 2 0.068698 0.966234 3 0.068777 0.995301 4 0.235628 0.993582 5 0.377556 0.995925 6 0.787230 0.992412 7 1.015086 0.994581 8 2.150240 0.976056 9 2.436944 0.982531 10 2.442617 0.991686 In\u00a0[58]: Copied! <pre># p\u5024\u304c\u3044\u305a\u308c\u30820.05\u3088\u308a\u3082\u5927\u304d\u304b\u3063\u305f\u306e\u3067\u7121\u76f8\u95a2\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3067\u304d\u305f\n# \u3053\u306e\u307e\u307e\u4e88\u6e2c\u3092\u884c\u304a\u3046\ndef rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window: int, method: str) -&gt; list:\n    total_len = train_len + horizon\n    idx_end = train_len\n    \n    preds_realdpi = []\n    preds_realcons = []\n    \n    if method == 'VAR':\n        for i in range(train_len, total_len, window):\n            model = VARMAX(df[:i], order=(3, 0)).fit(disp=False)\n            pred = model.get_prediction(0, i + window - 1)\n            preds_realdpi.extend(\n                pred.predicted_mean.iloc[-window:]['realdpi']\n            )\n            preds_realcons.extend(\n                pred.predicted_mean.iloc[-window:]['realcons']\n            )\n    elif method == 'last':\n        for i in range(train_len, total_len, window):\n            preds_realdpi.extend(\n                df[:i].iloc[-1]['realdpi'] for _ in range(window)\n            )\n            preds_realcons.extend(\n                df[:i].iloc[-1]['realcons'] for _ in range(window)\n            )\n    return preds_realdpi, preds_realcons\n</pre> # p\u5024\u304c\u3044\u305a\u308c\u30820.05\u3088\u308a\u3082\u5927\u304d\u304b\u3063\u305f\u306e\u3067\u7121\u76f8\u95a2\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3067\u304d\u305f # \u3053\u306e\u307e\u307e\u4e88\u6e2c\u3092\u884c\u304a\u3046 def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window: int, method: str) -&gt; list:     total_len = train_len + horizon     idx_end = train_len          preds_realdpi = []     preds_realcons = []          if method == 'VAR':         for i in range(train_len, total_len, window):             model = VARMAX(df[:i], order=(3, 0)).fit(disp=False)             pred = model.get_prediction(0, i + window - 1)             preds_realdpi.extend(                 pred.predicted_mean.iloc[-window:]['realdpi']             )             preds_realcons.extend(                 pred.predicted_mean.iloc[-window:]['realcons']             )     elif method == 'last':         for i in range(train_len, total_len, window):             preds_realdpi.extend(                 df[:i].iloc[-1]['realdpi'] for _ in range(window)             )             preds_realcons.extend(                 df[:i].iloc[-1]['realcons'] for _ in range(window)             )     return preds_realdpi, preds_realcons In\u00a0[62]: Copied! <pre>TRAIN_LEN = len(train)\nHORIZON = len(test)\nWINDOW = 4\n\npreds_realdpi_VAR, preds_realcons_VAR = rolling_forecast(\n    df_macro.diff()[1:], TRAIN_LEN, HORIZON, WINDOW, 'VAR'\n)\npreds_realdpi_last, preds_realcons_last = rolling_forecast(\n    df_macro.diff()[1:], TRAIN_LEN, HORIZON, WINDOW, 'last'\n)\n</pre> TRAIN_LEN = len(train) HORIZON = len(test) WINDOW = 4  preds_realdpi_VAR, preds_realcons_VAR = rolling_forecast(     df_macro.diff()[1:], TRAIN_LEN, HORIZON, WINDOW, 'VAR' ) preds_realdpi_last, preds_realcons_last = rolling_forecast(     df_macro.diff()[1:], TRAIN_LEN, HORIZON, WINDOW, 'last' ) In\u00a0[74]: Copied! <pre># \u548c\u5206\u306b\u5909\u63db\ntest = df_macro[163:]\ntest['realdpi_pred_VAR'] = df_macro.iloc[162, 0] + np.cumsum(preds_realdpi_VAR)\ntest['realcons_pred_VAR'] = df_macro.iloc[162, 1] + np.cumsum(preds_realcons_VAR)\ntest['realdpi_pred_last'] = df_macro.iloc[162, 0] + np.cumsum(preds_realdpi_last)\ntest['realcons_pred_last'] = df_macro.iloc[162, 1] + np.cumsum(preds_realcons_last)\ntest.head()\n</pre> # \u548c\u5206\u306b\u5909\u63db test = df_macro[163:] test['realdpi_pred_VAR'] = df_macro.iloc[162, 0] + np.cumsum(preds_realdpi_VAR) test['realcons_pred_VAR'] = df_macro.iloc[162, 1] + np.cumsum(preds_realcons_VAR) test['realdpi_pred_last'] = df_macro.iloc[162, 0] + np.cumsum(preds_realdpi_last) test['realcons_pred_last'] = df_macro.iloc[162, 1] + np.cumsum(preds_realcons_last) test.head() Out[74]: realdpi realcons realdpi_pred_VAR realcons_pred_VAR realdpi_pred_last realcons_pred_last date 1999Q4 7887.7 7389.2 7829.224941 7354.323079 7815.8 7372.9 2000Q1 8053.4 7501.3 7897.190077 7430.252000 7865.7 7459.4 2000Q2 8135.9 7571.8 7960.564871 7496.624306 7915.6 7545.9 2000Q3 8222.3 7645.9 8014.757071 7556.606726 7965.5 7632.4 2000Q4 8234.6 7713.5 8080.701763 7630.989437 8051.9 7706.5 In\u00a0[88]: Copied! <pre># plot\nfig, axes = plt.subplots(2, 1, sharex=True)\n# realdpi\naxes[0].set_title('realdpi')\ndf_macro['realdpi']['1995':].plot(ax=axes[0], label='actual')\naxes[0].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\ntest['realdpi_pred_VAR'].plot(ax=axes[0], label='VAR',ls='dashed')\ntest['realdpi_pred_last'].plot(ax=axes[0], label='last',ls='dashed')\naxes[0].legend()\n# realcons\naxes[1].set_title('realcons')\ndf_macro['realcons']['1995':].plot(ax=axes[1], label='actual')\naxes[1].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\ntest['realcons_pred_VAR'].plot(ax=axes[1], label='VAR',ls='dashed')\ntest['realcons_pred_last'].plot(ax=axes[1], label='last',ls='dashed')\naxes[1].legend()\nfig.tight_layout()\n</pre> # plot fig, axes = plt.subplots(2, 1, sharex=True) # realdpi axes[0].set_title('realdpi') df_macro['realdpi']['1995':].plot(ax=axes[0], label='actual') axes[0].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) test['realdpi_pred_VAR'].plot(ax=axes[0], label='VAR',ls='dashed') test['realdpi_pred_last'].plot(ax=axes[0], label='last',ls='dashed') axes[0].legend() # realcons axes[1].set_title('realcons') df_macro['realcons']['1995':].plot(ax=axes[1], label='actual') axes[1].axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) test['realcons_pred_VAR'].plot(ax=axes[1], label='VAR',ls='dashed') test['realcons_pred_last'].plot(ax=axes[1], label='last',ls='dashed') axes[1].legend() fig.tight_layout() In\u00a0[95]: Copied! <pre>def mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred - y_true) / y_true))\n\nfig, axes = plt.subplots(1, 2)\n# realdpi\naxes[0].set_title('realdpi')\nxs = ['realdpi_pred_VAR', 'realdpi_pred_last']\nys = [mape(test['realdpi'], test[x]) for x in xs]\naxes[0].bar(xs, ys)\naxes[0].yaxis.set_major_formatter(lambda y, _: '{:.01%}'.format(y))\n# realcons\naxes[1].set_title('realcons')\nxs = ['realcons_pred_VAR', 'realcons_pred_last']\nys = [mape(test['realcons'], test[x]) for x in xs]\naxes[1].bar(xs, ys)\naxes[1].yaxis.set_major_formatter(lambda y, _: '{:.01%}'.format(y))\nfig.tight_layout()\n</pre> def mape(y_true, y_pred):     return np.mean(np.abs((y_pred - y_true) / y_true))  fig, axes = plt.subplots(1, 2) # realdpi axes[0].set_title('realdpi') xs = ['realdpi_pred_VAR', 'realdpi_pred_last'] ys = [mape(test['realdpi'], test[x]) for x in xs] axes[0].bar(xs, ys) axes[0].yaxis.set_major_formatter(lambda y, _: '{:.01%}'.format(y)) # realcons axes[1].set_title('realcons') xs = ['realcons_pred_VAR', 'realcons_pred_last'] ys = [mape(test['realcons'], test[x]) for x in xs] axes[1].bar(xs, ys) axes[1].yaxis.set_major_formatter(lambda y, _: '{:.01%}'.format(y)) fig.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter10.html#var","title":"\u30d9\u30af\u30c8\u30eb\u81ea\u5df1\u56de\u5e30(VAR)\u30e2\u30c7\u30eb\u3092\u8abf\u3079\u308b\u00b6","text":""},{"location":"chapter10.html#varp","title":"VAR(p)\u30e2\u30c7\u30eb\u306e\u30e2\u30c7\u30eb\u5316\u624b\u7d9a\u304d\u3092\u8a2d\u8a08\u3059\u308b\u00b6","text":""},{"location":"chapter10.html","title":"\u5b9f\u8cea\u53ef\u51e6\u5206\u6240\u5f97\u3068\u5b9f\u8cea\u6d88\u8cbb\u652f\u51fa\u3092\u4e88\u6e2c\u3059\u308b\u00b6","text":""},{"location":"chapter11.html","title":"\u7b2c11\u7ae0 \u30ad\u30e3\u30c3\u30d7\u30b9\u30c8\u30fc\u30f3\uff1a\u30aa\u30fc\u30b9\u30c8\u30e9\u30ea\u30a2\u306e\u6297\u7cd6\u5c3f\u75c5\u85ac\u51e6\u65b9\u6570\u306e\u4e88\u6e2c","text":"In\u00a0[1]: Copied! <pre>from typing import Union\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import product\nfrom statsmodels.tsa.seasonal import STL\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> from typing import Union import numpy as np import pandas as pd import matplotlib.pyplot as plt from itertools import product from statsmodels.tsa.seasonal import STL from statsmodels.tsa.stattools import adfuller from statsmodels.tsa.statespace.sarimax import SARIMAX from statsmodels.stats.diagnostic import acorr_ljungbox from tqdm.auto import tqdm import warnings warnings.filterwarnings('ignore') In\u00a0[4]: Copied! <pre>url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/AusAntidiabeticDrug.csv'\ndf = pd.read_csv(url, index_col='ds', parse_dates=True)\n</pre> url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/AusAntidiabeticDrug.csv' df = pd.read_csv(url, index_col='ds', parse_dates=True) In\u00a0[9]: Copied! <pre>decomposition = STL(df['y'], period=12).fit()\ndecomposition.plot()\nplt.tight_layout()\n</pre> decomposition = STL(df['y'], period=12).fit() decomposition.plot() plt.tight_layout() In\u00a0[15]: Copied! <pre># \u4e00\u610f\u306aSARIMA(p,q)\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570\ndef optimize_SARIMA(endog: Union[pd.Series, list], orders: list, d: int, D: int, s: int) -&gt; pd.DataFrame:\n    \"\"\"\n    endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\n    orders: ARIMA(p, d, q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b\n    d: \u548c\u5206\u6b21\u6570\n    D: \u5b63\u7bc0\u5dee\u5206\u306e\u6b21\u6570\n    s: \u983b\u5ea6\n    \"\"\"\n    res = []\n    for order in tqdm(orders):\n        try:\n            model = SARIMAX(\n                endog, \n                order=(order[0], d, order[1]),\n                seasonal_order=(order[2], D, order[3], s), \n                simple_differencing=False\n            ).fit(disp=False)\n            res.append([order, model.aic])\n        except:\n            continue\n    df_res = (\n        pd.DataFrame(res, columns=['(p,q,P,Q)', 'AIC'])\n        .sort_values(by='AIC', ascending=True)\n        .reset_index(drop=True)\n    )\n    return df_res\n</pre> # \u4e00\u610f\u306aSARIMA(p,q)\u30e2\u30c7\u30eb\u3092\u3059\u3079\u3066\u9069\u5408\u3055\u305b\u308b\u95a2\u6570 def optimize_SARIMA(endog: Union[pd.Series, list], orders: list, d: int, D: int, s: int) -&gt; pd.DataFrame:     \"\"\"     endog: \u6642\u7cfb\u5217\u30c7\u30fc\u30bf     orders: ARIMA(p, d, q)\u306ep\u3068q\u306e\u5024\u306e\u7d44\u307f\u5408\u308f\u305b     d: \u548c\u5206\u6b21\u6570     D: \u5b63\u7bc0\u5dee\u5206\u306e\u6b21\u6570     s: \u983b\u5ea6     \"\"\"     res = []     for order in tqdm(orders):         try:             model = SARIMAX(                 endog,                  order=(order[0], d, order[1]),                 seasonal_order=(order[2], D, order[3], s),                  simple_differencing=False             ).fit(disp=False)             res.append([order, model.aic])         except:             continue     df_res = (         pd.DataFrame(res, columns=['(p,q,P,Q)', 'AIC'])         .sort_values(by='AIC', ascending=True)         .reset_index(drop=True)     )     return df_res  In\u00a0[11]: Copied! <pre># \u5b9a\u5e38\u6027\u306e\u78ba\u8a8d\nADF_result = adfuller(df['y'])\nprint(f'\u51e6\u65b9\u6570(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\nADF_result = adfuller(np.diff(df['y']))\nprint(f'\u51e6\u65b9\u6570(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n# \u5b63\u7bc0\u5dee\u5206\u3092\u4f7f\u3046\u3068m=12\nADF_result = adfuller(np.diff(np.diff(df['y']), n=12))\nprint(f'\u51e6\u65b9\u6570(1\u6b21\u5dee\u5206\u3001\u5b63\u7bc01\u6b21\u5dee\u5206)ADF\u691c\u5b9a')\nprint(f'ADF Statistic: {ADF_result[0]:.3f}')\nprint(f'p-value: {ADF_result[1]:.3f}')\n</pre> # \u5b9a\u5e38\u6027\u306e\u78ba\u8a8d ADF_result = adfuller(df['y']) print(f'\u51e6\u65b9\u6570(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}') ADF_result = adfuller(np.diff(df['y'])) print(f'\u51e6\u65b9\u6570(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}') # \u5b63\u7bc0\u5dee\u5206\u3092\u4f7f\u3046\u3068m=12 ADF_result = adfuller(np.diff(np.diff(df['y']), n=12)) print(f'\u51e6\u65b9\u6570(1\u6b21\u5dee\u5206\u3001\u5b63\u7bc01\u6b21\u5dee\u5206)ADF\u691c\u5b9a') print(f'ADF Statistic: {ADF_result[0]:.3f}') print(f'p-value: {ADF_result[1]:.3f}') <pre>\u51e6\u65b9\u6570(0\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: 3.145\np-value: 1.000\n\u51e6\u65b9\u6570(1\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -2.495\np-value: 0.117\n\u51e6\u65b9\u6570(1\u6b21\u5dee\u5206\u3001\u5b63\u7bc01\u6b21\u5dee\u5206)ADF\u691c\u5b9a\nADF Statistic: -19.848\np-value: 0.000\n</pre> In\u00a0[17]: Copied! <pre># d=1, D=1, s=12\u3067\u826f\u3044\u3053\u3068\u304c\u308f\u304b\u308b\nd=1\nD=1\ns=12\n# df\u3092train, test\u306b\u5206\u3051\u308b\ntrain = df[:168]\ntest = df[168:]\n# \u6700\u9069\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u63a2\u7d22\u3059\u308b\n# p,q,P,Q\u30920~4\u307e\u3067\u52d5\u304b\u3059\norders = list(product(range(5), repeat=4))\n# \u63a2\u7d22\noptimize_SARIMA(train, orders, d, D, s)\n</pre> # d=1, D=1, s=12\u3067\u826f\u3044\u3053\u3068\u304c\u308f\u304b\u308b d=1 D=1 s=12 # df\u3092train, test\u306b\u5206\u3051\u308b train = df[:168] test = df[168:] # \u6700\u9069\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u63a2\u7d22\u3059\u308b # p,q,P,Q\u30920~4\u307e\u3067\u52d5\u304b\u3059 orders = list(product(range(5), repeat=4)) # \u63a2\u7d22 optimize_SARIMA(train, orders, d, D, s) <pre>  0%|          | 0/625 [00:00&lt;?, ?it/s]</pre> Out[17]: (p,q,P,Q) AIC 0 (3, 1, 1, 3) 270.951096 1 (2, 4, 1, 3) 271.219926 2 (3, 3, 1, 3) 271.406306 3 (4, 1, 1, 3) 272.145292 4 (0, 4, 1, 3) 272.152379 ... ... ... 620 (0, 0, 0, 4) 351.680917 621 (0, 0, 2, 4) 353.250982 622 (0, 0, 0, 1) 354.751719 623 (0, 0, 1, 0) 357.173706 624 (0, 0, 0, 0) 357.531871 <p>625 rows \u00d7 2 columns</p> In\u00a0[20]: Copied! <pre># \u30c6\u30ad\u30b9\u30c8\u306f\u6700\u9069\u306a\u306e\u306fp=2,q=3,P=1,Q=3\u3068\u8a00\u3063\u3066\u3044\u308b\n# \u304c\u3001\u8a33\u6ce8\u306b\u3042\u308b\u3088\u3046\u306b\u624b\u5143\u3067\u78ba\u8a8d\u3057\u3066\u307f\u308b\u3068p=3,q=1,P=1,Q=3\u304c\u6700\u9069\u3068\u306a\u308b\n# \u3072\u3068\u307e\u305a\u30c6\u30ad\u30b9\u30c8\u306b\u5408\u308f\u305b\u3088\u3046\nmodel = SARIMAX(train, order=(2,d,3), seasonal_order=(1,D,3,s), simple_differencing=False).fit(disp=False)\n\nmodel.plot_diagnostics(figsize=(10, 8))\nplt.tight_layout()\n</pre> # \u30c6\u30ad\u30b9\u30c8\u306f\u6700\u9069\u306a\u306e\u306fp=2,q=3,P=1,Q=3\u3068\u8a00\u3063\u3066\u3044\u308b # \u304c\u3001\u8a33\u6ce8\u306b\u3042\u308b\u3088\u3046\u306b\u624b\u5143\u3067\u78ba\u8a8d\u3057\u3066\u307f\u308b\u3068p=3,q=1,P=1,Q=3\u304c\u6700\u9069\u3068\u306a\u308b # \u3072\u3068\u307e\u305a\u30c6\u30ad\u30b9\u30c8\u306b\u5408\u308f\u305b\u3088\u3046 model = SARIMAX(train, order=(2,d,3), seasonal_order=(1,D,3,s), simple_differencing=False).fit(disp=False)  model.plot_diagnostics(figsize=(10, 8)) plt.tight_layout() In\u00a0[22]: Copied! <pre># \u6b8b\u5dee\u5206\u6790\n# \u3059\u3079\u30660.05\u3092\u8d85\u3048\u3066\u3044\u308b\u306e\u554f\u984c\u306a\u304f\u6b21\u306b\u9032\u3080\nacorr_ljungbox(model.resid, np.arange(1, 11))\n</pre> # \u6b8b\u5dee\u5206\u6790 # \u3059\u3079\u30660.05\u3092\u8d85\u3048\u3066\u3044\u308b\u306e\u554f\u984c\u306a\u304f\u6b21\u306b\u9032\u3080 acorr_ljungbox(model.resid, np.arange(1, 11)) Out[22]: lb_stat lb_pvalue 1 1.434320 0.231061 2 1.697549 0.427939 3 2.266920 0.518888 4 3.808044 0.432607 5 4.352727 0.499826 6 4.369483 0.626808 7 9.175681 0.240288 8 9.239538 0.322494 9 9.422606 0.399213 10 9.422607 0.492520 In\u00a0[33]: Copied! <pre>def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window: int, method: str) -&gt; list:\n    total_len = train_len + horizon\n    preds = []\n    \n    if method == 'last_season':\n        for i in range(train_len, total_len, window):\n            preds.extend(df['y'][i-window:i].values)\n        return preds\n    elif method == 'SARIMA':\n        for i in range(train_len, total_len, window):\n            model = SARIMAX(\n                df['y'][:i],\n                order=(2,d,3),\n                seasonal_order=(1,D,3,s),\n                simple_differencing=False\n            ).fit(disp=False)\n            pred = model.forecast(window)\n            preds.extend(pred.values)\n        return preds\n    else:\n        raise ValueError('method must be \"last_season\" or \"SARIMA\".')\n</pre> def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window: int, method: str) -&gt; list:     total_len = train_len + horizon     preds = []          if method == 'last_season':         for i in range(train_len, total_len, window):             preds.extend(df['y'][i-window:i].values)         return preds     elif method == 'SARIMA':         for i in range(train_len, total_len, window):             model = SARIMAX(                 df['y'][:i],                 order=(2,d,3),                 seasonal_order=(1,D,3,s),                 simple_differencing=False             ).fit(disp=False)             pred = model.forecast(window)             preds.extend(pred.values)         return preds     else:         raise ValueError('method must be \"last_season\" or \"SARIMA\".') In\u00a0[34]: Copied! <pre>TRAIN_LEN = len(train)\nHORIZON = len(test)\nWINDOW = 12\nmethods = ['last_season', 'SARIMA']\nfor method in methods:\n    test[method] = rolling_forecast(df, TRAIN_LEN, HORIZON, WINDOW, method)\n</pre> TRAIN_LEN = len(train) HORIZON = len(test) WINDOW = 12 methods = ['last_season', 'SARIMA'] for method in methods:     test[method] = rolling_forecast(df, TRAIN_LEN, HORIZON, WINDOW, method) In\u00a0[43]: Copied! <pre>fig, ax = plt.subplots()\ndf['y'][-60:].plot(ax=ax, label='actual')\nax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5)\nfor method in methods:\n    test[method].plot(ax=ax, label=method, ls='dashed')\nax.legend()\nfig.autofmt_xdate()\nplt.tight_layout()\n</pre> fig, ax = plt.subplots() df['y'][-60:].plot(ax=ax, label='actual') ax.axvspan(test.index[0], test.index[-1], color='tab:orange', alpha=0.5) for method in methods:     test[method].plot(ax=ax, label=method, ls='dashed') ax.legend() fig.autofmt_xdate() plt.tight_layout() In\u00a0[44]: Copied! <pre># mape\u3092\u4f7f\u3063\u3066\u6027\u80fd\u6bd4\u8f03\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred - y_true) / y_true))\n\nplt.xlabel('Methods')\nplt.ylabel('MAPE')\nplt.bar(\n    methods,\n    [mape(test['y'], test[method]) for method in methods]\n)\nplt.gca().yaxis.set_major_formatter(lambda y, _: '{:.0%}'.format(y))\nplt.show()\n</pre> # mape\u3092\u4f7f\u3063\u3066\u6027\u80fd\u6bd4\u8f03 def mape(y_true, y_pred):     return np.mean(np.abs((y_pred - y_true) / y_true))  plt.xlabel('Methods') plt.ylabel('MAPE') plt.bar(     methods,     [mape(test['y'], test[method]) for method in methods] ) plt.gca().yaxis.set_major_formatter(lambda y, _: '{:.0%}'.format(y)) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter11.html","title":"\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\u3068\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u00b6","text":""},{"location":"chapter11.html","title":"\u6642\u7cfb\u5217\u3068\u305d\u306e\u6210\u5206\u3092\u53ef\u8996\u5316\u3059\u308b\u00b6","text":""},{"location":"chapter11.html","title":"\u30c7\u30fc\u30bf\u3092\u30e2\u30c7\u30eb\u5316\u3059\u308b\u00b6","text":""},{"location":"chapter11.html","title":"\u30e2\u30c7\u30eb\u306b\u3088\u308b\u4e88\u6e2c\u5024\u306e\u751f\u6210\u3068\u6027\u80fd\u306e\u8a55\u4fa1\u00b6","text":""},{"location":"chapter12.html","title":"\u7b2c12\u7ae0 \u6642\u7cfb\u5217\u4e88\u6e2c\u306e\u305f\u3081\u306e\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0","text":"In\u00a0[32]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime as dt\nfrom sklearn.preprocessing import MinMaxScaler\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import datetime as dt from sklearn.preprocessing import MinMaxScaler In\u00a0[9]: Copied! <pre>url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/metro_interstate_traffic_volume_preprocessed.csv'\ndf = pd.read_csv(url, index_col='date_time', parse_dates=True)\nprint(df.columns)\n</pre> url = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/metro_interstate_traffic_volume_preprocessed.csv' df = pd.read_csv(url, index_col='date_time', parse_dates=True) print(df.columns) <pre>Index(['temp', 'rain_1h', 'snow_1h', 'clouds_all', 'traffic_volume'], dtype='object')\n</pre> <p>\u90fd\u5e02\u570f\u306e\u5dde\u9593\u9ad8\u901f\u9053\u8def\u4ea4\u901a\u91cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u7279\u5fb4\u91cf</p> \u7279\u5fb4\u91cf \u8aac\u660e <code>date_time</code> CST\u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u3067\u8a18\u9332\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u65e5\u4ed8\u3068\u6642\u523b <code>temp</code> 1\u6642\u9593\u306b\u8a18\u9332\u3055\u308c\u305f\u5e73\u5747\u6c17\u6e29(\u5358\u4f4d\u306f\u30b1\u30eb\u30d3\u30f3) <code>rain_1h</code> 1\u6642\u9593\u306b\u964d\u3063\u305f\u96e8\u306e\u91cf(\u5358\u4f4d\u306f\u30df\u30ea\u30e1\u30fc\u30c8\u30eb) <code>snow_1h</code> 1\u6642\u9593\u306b\u964d\u3063\u305f\u96ea\u306e\u91cf(\u5358\u4f4d\u306f\u30df\u30ea\u30e1\u30fc\u30c8\u30eb) <code>clouds_all</code> 1\u6642\u9593\u306e\u96f2\u306e\u91cf\u306e\u5272\u5408 <code>traffic_volume</code> 1\u6642\u9593\u306b\u5831\u544a\u3055\u308c\u305f\u5dde\u9593\u9ad8\u901f\u9053\u8def94\u53f7\u7dda\u306e\u897f\u884c\u304d\u8eca\u7dda\u306e\u4ea4\u901a\u91cf In\u00a0[21]: Copied! <pre>fig, axes = plt.subplots(3, 1)\n# traffic volume\ndf['traffic_volume'].iloc[:400].plot(ax=axes[0])\naxes[0].set_ylabel('Traffic Volume')\n# temp: yearly\ndf['temp'].plot(ax=axes[1])\naxes[1].set_ylabel('Temperature (K)')\n# temp: daily\ndf['temp'].iloc[:400].plot(ax=axes[2])\naxes[2].set_ylabel('Temperature (K)')\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(3, 1) # traffic volume df['traffic_volume'].iloc[:400].plot(ax=axes[0]) axes[0].set_ylabel('Traffic Volume') # temp: yearly df['temp'].plot(ax=axes[1]) axes[1].set_ylabel('Temperature (K)') # temp: daily df['temp'].iloc[:400].plot(ax=axes[2]) axes[2].set_ylabel('Temperature (K)') plt.tight_layout() In\u00a0[28]: Copied! <pre># rain_1h, snow_1h\u306e\u53d6\u308b\u5024\u304c\u307b\u3068\u3093\u3069\u30bc\u30ed\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308b\n# \u3053\u308c\u306f\u7279\u5fb4\u91cf\u3068\u3057\u3066\u52b9\u304b\u306a\u3044\u306e\u3067\u843d\u3068\u3059\ndisplay(df.describe().transpose())\ndf.drop(columns=['rain_1h', 'snow_1h'], inplace=True)\n</pre> # rain_1h, snow_1h\u306e\u53d6\u308b\u5024\u304c\u307b\u3068\u3093\u3069\u30bc\u30ed\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308b # \u3053\u308c\u306f\u7279\u5fb4\u91cf\u3068\u3057\u3066\u52b9\u304b\u306a\u3044\u306e\u3067\u843d\u3068\u3059 display(df.describe().transpose()) df.drop(columns=['rain_1h', 'snow_1h'], inplace=True) count mean std min 25% 50% 75% max temp 17551.0 281.416203 12.688262 243.39 272.22 282.41 291.89 310.07 rain_1h 17551.0 0.025523 0.259794 0.00 0.00 0.00 0.00 10.60 snow_1h 17551.0 0.000000 0.000000 0.00 0.00 0.00 0.00 0.00 clouds_all 17551.0 42.034129 39.065960 0.00 1.00 40.00 90.00 100.00 traffic_volume 17551.0 3321.484588 1969.223949 113.00 1298.00 3518.00 4943.00 7280.00 In\u00a0[35]: Copied! <pre># \u65e5\u4ed8\u3092timestamp\u8868\u793a\u306b\u3057\u3066cos,sin\u306b\u5909\u63db\u3059\u308b\nseconds_per_day = 24 * 60 * 60\ndf['day_sin'] = df.index.map(dt.datetime.timestamp).map(lambda x: np.sin(x * 2 * np.pi / seconds_per_day)).values\ndf['day_cos'] = df.index.map(dt.datetime.timestamp).map(lambda x: np.cos(x * 2 * np.pi / seconds_per_day)).values\n</pre> # \u65e5\u4ed8\u3092timestamp\u8868\u793a\u306b\u3057\u3066cos,sin\u306b\u5909\u63db\u3059\u308b seconds_per_day = 24 * 60 * 60 df['day_sin'] = df.index.map(dt.datetime.timestamp).map(lambda x: np.sin(x * 2 * np.pi / seconds_per_day)).values df['day_cos'] = df.index.map(dt.datetime.timestamp).map(lambda x: np.cos(x * 2 * np.pi / seconds_per_day)).values In\u00a0[40]: Copied! <pre># \u65e5\u6642\u3092\u5186\u5468\u4e0a\u7b49\u9593\u9694\u306b\u4e26\u3079\u308b\u3053\u3068\u304c\u51fa\u6765\u305f\ndf.sample(100).plot.scatter('day_cos', 'day_sin').set_aspect('equal')\n</pre> # \u65e5\u6642\u3092\u5186\u5468\u4e0a\u7b49\u9593\u9694\u306b\u4e26\u3079\u308b\u3053\u3068\u304c\u51fa\u6765\u305f df.sample(100).plot.scatter('day_cos', 'day_sin').set_aspect('equal') In\u00a0[42]: Copied! <pre># train, validation, test\u30927:2:1\u3067\u5206\u5272\nn = len(df)\n\ndf_train = df[:int(0.7 * n)]\ndf_val = df[int(0.7 * n):int(0.9 * n)]\ndf_test = df[int(0.9 * n):]\n</pre> # train, validation, test\u30927:2:1\u3067\u5206\u5272 n = len(df)  df_train = df[:int(0.7 * n)] df_val = df[int(0.7 * n):int(0.9 * n)] df_test = df[int(0.9 * n):] In\u00a0[54]: Copied! <pre>scaler = MinMaxScaler()\nscaler.fit(df_train) # \u8a13\u7df4\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u307f\u3067\u9069\u5408\n\ndf_train[df_train.columns] = scaler.transform(df_train)\ndf_val[df_val.columns] = scaler.transform(df_val)\ndf_test[df_test.columns] = scaler.transform(df_test)\n</pre> scaler = MinMaxScaler() scaler.fit(df_train) # \u8a13\u7df4\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u307f\u3067\u9069\u5408  df_train[df_train.columns] = scaler.transform(df_train) df_val[df_val.columns] = scaler.transform(df_val) df_test[df_test.columns] = scaler.transform(df_test) <pre>/tmp/ipykernel_9678/4046014528.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_train[df_train.columns] = scaler.transform(df_train)\n/tmp/ipykernel_9678/4046014528.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_val[df_val.columns] = scaler.transform(df_val)\n/tmp/ipykernel_9678/4046014528.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_test[df_test.columns] = scaler.transform(df_test)\n</pre> In\u00a0[55]: Copied! <pre># \u3053\u3053\u3067\u306fgithub\u306e\u30c7\u30fc\u30bf\u3092\u6709\u96e3\u304f\u5229\u7528\u3055\u305b\u3066\u3082\u3089\u3046\u306e\u3067\u4fdd\u5b58\u306f\u884c\u308f\u306a\u3044\n# df_train.to_csv('train.csv')\n# df_val.to_csv('val.csv')\n# df_test.to_csv('test.csv')\n</pre> # \u3053\u3053\u3067\u306fgithub\u306e\u30c7\u30fc\u30bf\u3092\u6709\u96e3\u304f\u5229\u7528\u3055\u305b\u3066\u3082\u3089\u3046\u306e\u3067\u4fdd\u5b58\u306f\u884c\u308f\u306a\u3044 # df_train.to_csv('train.csv') # df_val.to_csv('val.csv') # df_test.to_csv('test.csv') In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter12.html","title":"\u6642\u7cfb\u5217\u4e88\u6e2c\u306b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3092\u4f7f\u3046\u72b6\u6cc1\u00b6","text":""},{"location":"chapter12.html","title":"\u3055\u307e\u3056\u307e\u306a\u7a2e\u985e\u306e\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u30e2\u30c7\u30eb\u3092\u8abf\u3079\u308b\u00b6","text":""},{"location":"chapter12.html","title":"\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3092\u4f7f\u3063\u3066\u4e88\u6e2c\u3092\u884c\u3046\u305f\u3081\u306e\u6e96\u5099\u00b6","text":""},{"location":"chapter13.html","title":"\u7b2c13\u7ae0 \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u305f\u3081\u306e\u30c7\u30fc\u30bf\u30a6\u30a3\u30f3\u30c9\u30a6\u3068\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3","text":"In\u00a0[28]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.metrics import MeanAbsoluteError\n</pre> import pandas as pd import numpy as np import datetime as dt import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.losses import MeanSquaredError from tensorflow.keras.metrics import MeanAbsoluteError In\u00a0[2]: Copied! <pre>url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv'\nurl_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv'\nurl_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'\n\ndf_train = pd.read_csv(url_train, index_col=0)\ndf_val = pd.read_csv(url_val, index_col=0)\ndf_test = pd.read_csv(url_test, index_col=0)\n</pre> url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv' url_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv' url_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'  df_train = pd.read_csv(url_train, index_col=0) df_val = pd.read_csv(url_val, index_col=0) df_test = pd.read_csv(url_test, index_col=0) In\u00a0[30]: Copied! <pre>class DataWindow:\n    def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):\n        # window size\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n        self.total_window_size = input_width + shift\n        \n        # \u30c7\u30fc\u30bf\n        self.df_train = df_train\n        self.df_val = df_val\n        self.df_test = df_test\n        \n        # \u30e9\u30d9\u30eb\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}\n        \n        # \u30b9\u30e9\u30a4\u30b9\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n        \n        # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e\n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n    \n    def split_to_inputs_labels(self, features):\n        inputs = features[:, self.input_slice, :]\n        labels = features[:, self.labels_slice, :]\n        if self.label_columns is not None:\n            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n        inputs.set_shape([None, self.input_width, None])\n        labels.set_shape([None, self.label_width, None])\n        return inputs, labels\n    \n    def plot(self, plot_col: str, model=None, max_subplots=3):\n        inputs, labels = self.sample_batch\n        plt.figure(figsize=(12, 8))\n        plot_col_index = self.column_indices[plot_col]\n        n_max = min(max_subplots, len(inputs))\n        \n        for n in range(n_max):\n            plt.subplot(n_max, 1, n+1)\n            plt.ylabel(f'{plot_col} [scaled]')\n            plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)\n            \n            if self.label_columns:\n                label_col_index = self.label_columns_indices.get(plot_col, None)\n            else:\n                label_col_index = plot_col_index\n            \n            if label_col_index is None:\n                continue\n            \n            plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)\n            \n            if model is not None:\n                predictions = model(inputs)\n                plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)\n            \n            if n == 0:\n                plt.legend()\n        plt.xlabel('Time (h)')\n    \n    def make_dataset(self, data):\n        data = np.array(data, dtype=np.float32)\n        ds = tf.keras.utils.timeseries_dataset_from_array(\n            data=data,\n            targets=None,\n            sequence_length=self.total_window_size,\n            sequence_stride=1,\n            shuffle=True,\n            batch_size=32,\n        )\n        ds = ds.map(self.split_to_inputs_labels)\n        return ds\n    \n    @property\n    def train(self):\n        return self.make_dataset(self.df_train)\n    \n    @property\n    def val(self):\n        return self.make_dataset(self.df_val)\n    \n    @property\n    def test(self):\n        return self.make_dataset(self.df_test)\n    \n    @property\n    def sample_batch(self):\n        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n        result = getattr(self, '_sample_batch', None)\n        if result is None:\n            result = next(iter(self.train))\n            self._sample_batch = result\n        return result\n</pre> class DataWindow:     def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):         # window size         self.input_width = input_width         self.label_width = label_width         self.shift = shift         self.total_window_size = input_width + shift                  # \u30c7\u30fc\u30bf         self.df_train = df_train         self.df_val = df_val         self.df_test = df_test                  # \u30e9\u30d9\u30eb         self.label_columns = label_columns         if label_columns is not None:             self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}         self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}                  # \u30b9\u30e9\u30a4\u30b9         self.input_slice = slice(0, input_width)         self.input_indices = np.arange(self.total_window_size)[self.input_slice]                  # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e         self.label_start = self.total_window_size - self.label_width         self.labels_slice = slice(self.label_start, None)         self.label_indices = np.arange(self.total_window_size)[self.labels_slice]          def split_to_inputs_labels(self, features):         inputs = features[:, self.input_slice, :]         labels = features[:, self.labels_slice, :]         if self.label_columns is not None:             labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)         inputs.set_shape([None, self.input_width, None])         labels.set_shape([None, self.label_width, None])         return inputs, labels          def plot(self, plot_col: str, model=None, max_subplots=3):         inputs, labels = self.sample_batch         plt.figure(figsize=(12, 8))         plot_col_index = self.column_indices[plot_col]         n_max = min(max_subplots, len(inputs))                  for n in range(n_max):             plt.subplot(n_max, 1, n+1)             plt.ylabel(f'{plot_col} [scaled]')             plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)                          if self.label_columns:                 label_col_index = self.label_columns_indices.get(plot_col, None)             else:                 label_col_index = plot_col_index                          if label_col_index is None:                 continue                          plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)                          if model is not None:                 predictions = model(inputs)                 plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)                          if n == 0:                 plt.legend()         plt.xlabel('Time (h)')          def make_dataset(self, data):         data = np.array(data, dtype=np.float32)         ds = tf.keras.utils.timeseries_dataset_from_array(             data=data,             targets=None,             sequence_length=self.total_window_size,             sequence_stride=1,             shuffle=True,             batch_size=32,         )         ds = ds.map(self.split_to_inputs_labels)         return ds          @property     def train(self):         return self.make_dataset(self.df_train)          @property     def val(self):         return self.make_dataset(self.df_val)          @property     def test(self):         return self.make_dataset(self.df_test)          @property     def sample_batch(self):         \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"         result = getattr(self, '_sample_batch', None)         if result is None:             result = next(iter(self.train))             self._sample_batch = result         return result In\u00a0[49]: Copied! <pre># \u307e\u305a\u306f\u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u524d\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u4e88\u6e2c\u7d50\u679c\u3068\u3059\u308b\nsingle_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\nwide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\n</pre> # \u307e\u305a\u306f\u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u524d\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u4e88\u6e2c\u7d50\u679c\u3068\u3059\u308b single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) In\u00a0[50]: Copied! <pre>class Baseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n        \n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        elif isinstance(self.label_index, list):\n            tensors = []\n            for index in self.label_index:\n                res = inputs[:, :, index]\n                res = res[:, :, tf.newaxis]\n                tensors.append(res)\n            return tf.concat(tensors, axis=-1)\n        else:\n            res = inputs[:, :, self.label_index]\n            return res[:, :, tf.newaxis]\n</pre> class Baseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index              def call(self, inputs):         if self.label_index is None:             return inputs         elif isinstance(self.label_index, list):             tensors = []             for index in self.label_index:                 res = inputs[:, :, index]                 res = res[:, :, tf.newaxis]                 tensors.append(res)             return tf.concat(tensors, axis=-1)         else:             res = inputs[:, :, self.label_index]             return res[:, :, tf.newaxis] In\u00a0[51]: Copied! <pre>column_indices = {name: i for i, name in enumerate(df_train.columns)}\nbaseline_last = Baseline(column_indices['traffic_volume'])\nbaseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n</pre> column_indices = {name: i for i, name in enumerate(df_train.columns)} baseline_last = Baseline(column_indices['traffic_volume']) baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()]) In\u00a0[52]: Copied! <pre>performane_val = {}\nperformance_test = {}\n\nperformane_val['Baseline - Last'] = baseline_last.evaluate(single_step_window.val)\nperformance_test['Baseline - Last'] = baseline_last.evaluate(single_step_window.test, verbose=0)\n</pre> performane_val = {} performance_test = {}  performane_val['Baseline - Last'] = baseline_last.evaluate(single_step_window.val) performance_test['Baseline - Last'] = baseline_last.evaluate(single_step_window.test, verbose=0) <pre>110/110 [==============================] - 0s 2ms/step - loss: 684054.1250 - mean_absolute_error: 595.2283\n</pre> In\u00a0[53]: Copied! <pre># single_step_window.plot('traffic_volume', baseline_last)\u3060\u3068\u3042\u307e\u308a\u72b6\u6cc1\u304c\u898b\u3048\u306a\u3044\u306e\u3067wide_window\u306b\u3057\u3066\u3044\u304f\nwide_window.plot('traffic_volume', baseline_last)\n</pre> # single_step_window.plot('traffic_volume', baseline_last)\u3060\u3068\u3042\u307e\u308a\u72b6\u6cc1\u304c\u898b\u3048\u306a\u3044\u306e\u3067wide_window\u306b\u3057\u3066\u3044\u304f wide_window.plot('traffic_volume', baseline_last) In\u00a0[60]: Copied! <pre># \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7\u306e\u5834\u5408\nmulti_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\n\nclass MultiStepLastBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        if self.label_index is None:\n            return tf.tile(inputs[:, -1:, :], [1, 24, 1])\n        return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])\n    \nclass RepeatBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        return inputs[:, :, self.label_index:]\n</pre> # \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7\u306e\u5834\u5408 multi_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])  class MultiStepLastBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         if self.label_index is None:             return tf.tile(inputs[:, -1:, :], [1, 24, 1])         return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])      class RepeatBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         return inputs[:, :, self.label_index:] In\u00a0[61]: Copied! <pre>ms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\nms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nms_val_performance = {}\nms_test_performance = {}\n\nms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val)\nms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test)\nms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val)\nms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test)\n\n\nmulti_window.plot('traffic_volume', ms_baseline_last)\nmulti_window.plot('traffic_volume', ms_baseline_repeat)\n</pre> ms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume']) ms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()]) ms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume']) ms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  ms_val_performance = {} ms_test_performance = {}  ms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val) ms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test) ms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val) ms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test)   multi_window.plot('traffic_volume', ms_baseline_last) multi_window.plot('traffic_volume', ms_baseline_repeat) <pre>109/109 [==============================] - 1s 5ms/step - loss: 12769637.0000 - mean_absolute_error: 2974.6951\n54/54 [==============================] - 0s 5ms/step - loss: 12498208.0000 - mean_absolute_error: 2964.0754\n109/109 [==============================] - 2s 3ms/step - loss: 10588796.0000 - mean_absolute_error: 2431.1047\n54/54 [==============================] - 0s 3ms/step - loss: 10346166.0000 - mean_absolute_error: 2403.8494\n</pre> In\u00a0[63]: Copied! <pre># \u591a\u51fa\u529b\u306e\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u30e2\u30c7\u30eb\ncol_names = ['temp', 'traffic_volume']\nmo_single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)\nmo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)\n\nmo_baseline_last = Baseline(label_index=[column_indices[col] for col in col_names])\nmo_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nmo_val_performance = {}\nmo_test_performance = {}\n\n# \u30c6\u30ad\u30b9\u30c8\u3060\u3068mo_wide_window\u3092\u4f7f\u3063\u3066evaluate\u3092\u3057\u3066\u3044\u308b\u304c\u672c\u6765\u7684\u306b\u306fmo_single_step_window\u3092\u4f7f\u3046\u5fc5\u8981\u304c\u3042\u308b\nmo_val_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)\nmo_test_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)\n\nfor col in col_names:\n    mo_wide_window.plot(col, mo_baseline_last)\n</pre> # \u591a\u51fa\u529b\u306e\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u30e2\u30c7\u30eb col_names = ['temp', 'traffic_volume'] mo_single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names) mo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)  mo_baseline_last = Baseline(label_index=[column_indices[col] for col in col_names]) mo_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  mo_val_performance = {} mo_test_performance = {}  # \u30c6\u30ad\u30b9\u30c8\u3060\u3068mo_wide_window\u3092\u4f7f\u3063\u3066evaluate\u3092\u3057\u3066\u3044\u308b\u304c\u672c\u6765\u7684\u306b\u306fmo_single_step_window\u3092\u4f7f\u3046\u5fc5\u8981\u304c\u3042\u308b mo_val_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val) mo_test_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)  for col in col_names:     mo_wide_window.plot(col, mo_baseline_last) <pre>110/110 [==============================] - 0s 2ms/step - loss: 342027.7812 - mean_absolute_error: 298.0399\n110/110 [==============================] - 0s 2ms/step - loss: 342027.8438 - mean_absolute_error: 298.0400\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter13.html","title":"\u30c7\u30fc\u30bf\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u4f5c\u6210\u3059\u308b\u00b6","text":""},{"location":"chapter13.html","title":"\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3059\u308b\u00b6","text":""},{"location":"chapter14.html","title":"\u7b2c14\u7ae0 \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u624b\u307b\u3069\u304d","text":"In\u00a0[10]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.metrics import MeanAbsoluteError\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.losses import MeanSquaredError from tensorflow.keras.metrics import MeanAbsoluteError from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.optimizers import Adam from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense In\u00a0[2]: Copied! <pre>url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv'\nurl_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv'\nurl_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'\n\ndf_train = pd.read_csv(url_train, index_col=0)\ndf_val = pd.read_csv(url_val, index_col=0)\ndf_test = pd.read_csv(url_test, index_col=0)\n</pre> url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv' url_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv' url_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'  df_train = pd.read_csv(url_train, index_col=0) df_val = pd.read_csv(url_val, index_col=0) df_test = pd.read_csv(url_test, index_col=0) In\u00a0[6]: Copied! <pre>class DataWindow:\n    def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):\n        # window size\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n        self.total_window_size = input_width + shift\n        \n        # \u30c7\u30fc\u30bf\n        self.df_train = df_train\n        self.df_val = df_val\n        self.df_test = df_test\n        \n        # \u30e9\u30d9\u30eb\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}\n        \n        # \u30b9\u30e9\u30a4\u30b9\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n        \n        # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e\n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n    \n    def split_to_inputs_labels(self, features):\n        inputs = features[:, self.input_slice, :]\n        labels = features[:, self.labels_slice, :]\n        if self.label_columns is not None:\n            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n        inputs.set_shape([None, self.input_width, None])\n        labels.set_shape([None, self.label_width, None])\n        return inputs, labels\n    \n    def plot(self, plot_col: str, model=None, max_subplots=3):\n        inputs, labels = self.sample_batch\n        plt.figure(figsize=(12, 8))\n        plot_col_index = self.column_indices[plot_col]\n        n_max = min(max_subplots, len(inputs))\n        \n        for n in range(n_max):\n            plt.subplot(n_max, 1, n+1)\n            plt.ylabel(f'{plot_col} [scaled]')\n            plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)\n            \n            if self.label_columns:\n                label_col_index = self.label_columns_indices.get(plot_col, None)\n            else:\n                label_col_index = plot_col_index\n            \n            if label_col_index is None:\n                continue\n            \n            plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)\n            \n            if model is not None:\n                predictions = model(inputs)\n                plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)\n            \n            if n == 0:\n                plt.legend()\n        plt.xlabel('Time (h)')\n    \n    def make_dataset(self, data):\n        data = np.array(data, dtype=np.float32)\n        ds = tf.keras.utils.timeseries_dataset_from_array(\n            data=data,\n            targets=None,\n            sequence_length=self.total_window_size,\n            sequence_stride=1,\n            shuffle=True,\n            batch_size=32,\n        )\n        ds = ds.map(self.split_to_inputs_labels)\n        return ds\n    \n    @property\n    def train(self):\n        return self.make_dataset(self.df_train)\n    \n    @property\n    def val(self):\n        return self.make_dataset(self.df_val)\n    \n    @property\n    def test(self):\n        return self.make_dataset(self.df_test)\n    \n    @property\n    def sample_batch(self):\n        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n        result = getattr(self, '_sample_batch', None)\n        if result is None:\n            result = next(iter(self.train))\n            self._sample_batch = result\n        return result\n</pre> class DataWindow:     def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):         # window size         self.input_width = input_width         self.label_width = label_width         self.shift = shift         self.total_window_size = input_width + shift                  # \u30c7\u30fc\u30bf         self.df_train = df_train         self.df_val = df_val         self.df_test = df_test                  # \u30e9\u30d9\u30eb         self.label_columns = label_columns         if label_columns is not None:             self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}         self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}                  # \u30b9\u30e9\u30a4\u30b9         self.input_slice = slice(0, input_width)         self.input_indices = np.arange(self.total_window_size)[self.input_slice]                  # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e         self.label_start = self.total_window_size - self.label_width         self.labels_slice = slice(self.label_start, None)         self.label_indices = np.arange(self.total_window_size)[self.labels_slice]          def split_to_inputs_labels(self, features):         inputs = features[:, self.input_slice, :]         labels = features[:, self.labels_slice, :]         if self.label_columns is not None:             labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)         inputs.set_shape([None, self.input_width, None])         labels.set_shape([None, self.label_width, None])         return inputs, labels          def plot(self, plot_col: str, model=None, max_subplots=3):         inputs, labels = self.sample_batch         plt.figure(figsize=(12, 8))         plot_col_index = self.column_indices[plot_col]         n_max = min(max_subplots, len(inputs))                  for n in range(n_max):             plt.subplot(n_max, 1, n+1)             plt.ylabel(f'{plot_col} [scaled]')             plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)                          if self.label_columns:                 label_col_index = self.label_columns_indices.get(plot_col, None)             else:                 label_col_index = plot_col_index                          if label_col_index is None:                 continue                          plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)                          if model is not None:                 predictions = model(inputs)                 plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)                          if n == 0:                 plt.legend()         plt.xlabel('Time (h)')          def make_dataset(self, data):         data = np.array(data, dtype=np.float32)         ds = tf.keras.utils.timeseries_dataset_from_array(             data=data,             targets=None,             sequence_length=self.total_window_size,             sequence_stride=1,             shuffle=True,             batch_size=32,         )         ds = ds.map(self.split_to_inputs_labels)         return ds          @property     def train(self):         return self.make_dataset(self.df_train)          @property     def val(self):         return self.make_dataset(self.df_val)          @property     def test(self):         return self.make_dataset(self.df_test)          @property     def sample_batch(self):         \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"         result = getattr(self, '_sample_batch', None)         if result is None:             result = next(iter(self.train))             self._sample_batch = result         return result In\u00a0[7]: Copied! <pre># models\nclass Baseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n        \n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        elif isinstance(self.label_index, list):\n            tensors = []\n            for index in self.label_index:\n                res = inputs[:, :, index]\n                res = res[:, :, tf.newaxis]\n                tensors.append(res)\n            return tf.concat(tensors, axis=-1)\n        else:\n            res = inputs[:, :, self.label_index]\n            return res[:, :, tf.newaxis]\n\nclass MultiStepLastBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        if self.label_index is None:\n            return tf.tile(inputs[:, -1:, :], [1, 24, 1])\n        return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])\n    \nclass RepeatBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        return inputs[:, :, self.label_index:]\n</pre> # models class Baseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index              def call(self, inputs):         if self.label_index is None:             return inputs         elif isinstance(self.label_index, list):             tensors = []             for index in self.label_index:                 res = inputs[:, :, index]                 res = res[:, :, tf.newaxis]                 tensors.append(res)             return tf.concat(tensors, axis=-1)         else:             res = inputs[:, :, self.label_index]             return res[:, :, tf.newaxis]  class MultiStepLastBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         if self.label_index is None:             return tf.tile(inputs[:, -1:, :], [1, 24, 1])         return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])      class RepeatBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         return inputs[:, :, self.label_index:] In\u00a0[41]: Copied! <pre># \u30b7\u30f3\u30b0\u30eb\u30b9\u30c6\u30c3\u30d7\nsingle_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\nwide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\ncolumn_indices = {name: i for i, name in enumerate(df_train.columns)}\nbaseline_last = Baseline(column_indices['traffic_volume'])\nbaseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nval_performance = {}\ntest_performance = {}\n\nval_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.val)\ntest_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.test, verbose=0)\n\n# \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7\nmulti_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\nms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\nms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nms_val_performance = {}\nms_test_performance = {}\n\nms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val)\nms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test)\nms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val)\nms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test)\n\n# \u591a\u5909\u6570\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\ncol_names = ['temp', 'traffic_volume']\nmo_single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)\nmo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)\nmo_baseline_last = Baseline(label_index=[column_indices[col] for col in col_names])\nmo_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nmo_val_performance = {}\nmo_test_performance = {}\n\nmo_val_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)\nmo_test_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)\n</pre> # \u30b7\u30f3\u30b0\u30eb\u30b9\u30c6\u30c3\u30d7 single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) column_indices = {name: i for i, name in enumerate(df_train.columns)} baseline_last = Baseline(column_indices['traffic_volume']) baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  val_performance = {} test_performance = {}  val_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.val) test_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.test, verbose=0)  # \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7 multi_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) ms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume']) ms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()]) ms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume']) ms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  ms_val_performance = {} ms_test_performance = {}  ms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val) ms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test) ms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val) ms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test)  # \u591a\u5909\u6570\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8 col_names = ['temp', 'traffic_volume'] mo_single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names) mo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names) mo_baseline_last = Baseline(label_index=[column_indices[col] for col in col_names]) mo_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  mo_val_performance = {} mo_test_performance = {}  mo_val_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val) mo_test_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val) <pre>110/110 [==============================] - 1s 3ms/step - loss: 0.0133 - mean_absolute_error: 0.0831\n109/109 [==============================] - 0s 3ms/step - loss: 0.1875 - mean_absolute_error: 0.3522\n54/54 [==============================] - 0s 3ms/step - loss: 0.1814 - mean_absolute_error: 0.3473\n109/109 [==============================] - 0s 3ms/step - loss: 0.2065 - mean_absolute_error: 0.3473\n54/54 [==============================] - 0s 3ms/step - loss: 0.2018 - mean_absolute_error: 0.3413\n110/110 [==============================] - 0s 3ms/step - loss: 0.0069 - mean_absolute_error: 0.0482\n110/110 [==============================] - 0s 3ms/step - loss: 0.0069 - mean_absolute_error: 0.0482\n</pre> In\u00a0[42]: Copied! <pre>def compile_and_fit(model, window, patience=3, max_epochs=50):\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=patience,\n        mode='min'\n    )\n    model.compile(\n        loss=MeanSquaredError(),\n        optimizer=Adam(),\n        metrics=[MeanAbsoluteError()]\n    )\n    history = model.fit(\n        window.train,\n        epochs=max_epochs,\n        validation_data=window.val,\n        callbacks=[early_stopping]\n    )\n    return history\n</pre> def compile_and_fit(model, window, patience=3, max_epochs=50):     early_stopping = EarlyStopping(         monitor='val_loss',         patience=patience,         mode='min'     )     model.compile(         loss=MeanSquaredError(),         optimizer=Adam(),         metrics=[MeanAbsoluteError()]     )     history = model.fit(         window.train,         epochs=max_epochs,         validation_data=window.val,         callbacks=[early_stopping]     )     return history In\u00a0[43]: Copied! <pre>linear = Sequential([Dense(units=1)])\nhistory = compile_and_fit(linear, single_step_window)\nval_performance['Linear'] = linear.evaluate(single_step_window.val)\ntest_performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)\n</pre> linear = Sequential([Dense(units=1)]) history = compile_and_fit(linear, single_step_window) val_performance['Linear'] = linear.evaluate(single_step_window.val) test_performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0) <pre>Epoch 1/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.3284 - mean_absolute_error: 0.4560 - val_loss: 0.0977 - val_mean_absolute_error: 0.2534\nEpoch 2/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0829 - mean_absolute_error: 0.2325 - val_loss: 0.0570 - val_mean_absolute_error: 0.1893\nEpoch 3/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0478 - mean_absolute_error: 0.1743 - val_loss: 0.0325 - val_mean_absolute_error: 0.1410\nEpoch 4/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0279 - mean_absolute_error: 0.1319 - val_loss: 0.0200 - val_mean_absolute_error: 0.1100\nEpoch 5/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0184 - mean_absolute_error: 0.1060 - val_loss: 0.0147 - val_mean_absolute_error: 0.0930\nEpoch 6/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0146 - mean_absolute_error: 0.0922 - val_loss: 0.0128 - val_mean_absolute_error: 0.0844\nEpoch 7/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0131 - mean_absolute_error: 0.0851 - val_loss: 0.0119 - val_mean_absolute_error: 0.0798\nEpoch 8/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0124 - mean_absolute_error: 0.0813 - val_loss: 0.0114 - val_mean_absolute_error: 0.0772\nEpoch 9/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0118 - mean_absolute_error: 0.0786 - val_loss: 0.0109 - val_mean_absolute_error: 0.0749\nEpoch 10/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0113 - mean_absolute_error: 0.0762 - val_loss: 0.0105 - val_mean_absolute_error: 0.0728\nEpoch 11/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0108 - mean_absolute_error: 0.0740 - val_loss: 0.0101 - val_mean_absolute_error: 0.0708\nEpoch 12/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0104 - mean_absolute_error: 0.0721 - val_loss: 0.0098 - val_mean_absolute_error: 0.0692\nEpoch 13/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0101 - mean_absolute_error: 0.0706 - val_loss: 0.0095 - val_mean_absolute_error: 0.0675\nEpoch 14/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0098 - mean_absolute_error: 0.0693 - val_loss: 0.0093 - val_mean_absolute_error: 0.0670\nEpoch 15/50\n384/384 [==============================] - 3s 6ms/step - loss: 0.0096 - mean_absolute_error: 0.0685 - val_loss: 0.0092 - val_mean_absolute_error: 0.0656\nEpoch 16/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0095 - mean_absolute_error: 0.0680 - val_loss: 0.0091 - val_mean_absolute_error: 0.0657\nEpoch 17/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0094 - mean_absolute_error: 0.0676 - val_loss: 0.0091 - val_mean_absolute_error: 0.0656\nEpoch 18/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0676 - val_loss: 0.0091 - val_mean_absolute_error: 0.0649\nEpoch 19/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0675 - val_loss: 0.0091 - val_mean_absolute_error: 0.0645\nEpoch 20/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0093 - mean_absolute_error: 0.0675 - val_loss: 0.0091 - val_mean_absolute_error: 0.0652\nEpoch 21/50\n384/384 [==============================] - 3s 6ms/step - loss: 0.0093 - mean_absolute_error: 0.0676 - val_loss: 0.0091 - val_mean_absolute_error: 0.0652\nEpoch 22/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0675 - val_loss: 0.0091 - val_mean_absolute_error: 0.0648\nEpoch 23/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0676 - val_loss: 0.0091 - val_mean_absolute_error: 0.0645\n110/110 [==============================] - 0s 4ms/step - loss: 0.0091 - mean_absolute_error: 0.0645\n</pre> In\u00a0[44]: Copied! <pre>wide_window.plot('traffic_volume', linear)\n</pre> wide_window.plot('traffic_volume', linear) In\u00a0[45]: Copied! <pre>ms_linear = Sequential([Dense(units=1, kernel_initializer=tf.initializers.zeros)])\nhistory = compile_and_fit(ms_linear, multi_window)\nms_val_performance['Linear'] = ms_linear.evaluate(multi_window.val)\nms_test_performance['Linear'] = ms_linear.evaluate(multi_window.test)\n</pre> ms_linear = Sequential([Dense(units=1, kernel_initializer=tf.initializers.zeros)]) history = compile_and_fit(ms_linear, multi_window) ms_val_performance['Linear'] = ms_linear.evaluate(multi_window.val) ms_test_performance['Linear'] = ms_linear.evaluate(multi_window.test) <pre>Epoch 1/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0897 - mean_absolute_error: 0.2463 - val_loss: 0.0456 - val_mean_absolute_error: 0.1819\nEpoch 2/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0297 - mean_absolute_error: 0.1366 - val_loss: 0.0261 - val_mean_absolute_error: 0.1251\nEpoch 3/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0200 - mean_absolute_error: 0.1044 - val_loss: 0.0213 - val_mean_absolute_error: 0.1074\nEpoch 4/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0177 - mean_absolute_error: 0.0955 - val_loss: 0.0196 - val_mean_absolute_error: 0.1000\nEpoch 5/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0168 - mean_absolute_error: 0.0914 - val_loss: 0.0189 - val_mean_absolute_error: 0.0956\nEpoch 6/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0164 - mean_absolute_error: 0.0887 - val_loss: 0.0185 - val_mean_absolute_error: 0.0924\nEpoch 7/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0162 - mean_absolute_error: 0.0869 - val_loss: 0.0183 - val_mean_absolute_error: 0.0905\nEpoch 8/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0162 - mean_absolute_error: 0.0857 - val_loss: 0.0182 - val_mean_absolute_error: 0.0894\nEpoch 9/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0850 - val_loss: 0.0182 - val_mean_absolute_error: 0.0887\nEpoch 10/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0161 - mean_absolute_error: 0.0846 - val_loss: 0.0182 - val_mean_absolute_error: 0.0883\nEpoch 11/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0161 - mean_absolute_error: 0.0843 - val_loss: 0.0182 - val_mean_absolute_error: 0.0887\nEpoch 12/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0841 - val_loss: 0.0182 - val_mean_absolute_error: 0.0883\nEpoch 13/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0840 - val_loss: 0.0182 - val_mean_absolute_error: 0.0881\nEpoch 14/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0161 - mean_absolute_error: 0.0839 - val_loss: 0.0182 - val_mean_absolute_error: 0.0880\nEpoch 15/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0161 - mean_absolute_error: 0.0839 - val_loss: 0.0182 - val_mean_absolute_error: 0.0881\n109/109 [==============================] - 0s 3ms/step - loss: 0.0182 - mean_absolute_error: 0.0881\n54/54 [==============================] - 0s 2ms/step - loss: 0.0142 - mean_absolute_error: 0.0759\n</pre> In\u00a0[46]: Copied! <pre>multi_window.plot('traffic_volume', ms_linear)\n</pre> multi_window.plot('traffic_volume', ms_linear) In\u00a0[47]: Copied! <pre>mo_linear = Sequential([Dense(units=2)])\nhistory = compile_and_fit(mo_linear, mo_single_step_window)\nmo_val_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.val)\nmo_test_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.test)\n</pre> mo_linear = Sequential([Dense(units=2)]) history = compile_and_fit(mo_linear, mo_single_step_window) mo_val_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.val) mo_test_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.test) <pre>Epoch 1/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.2844 - mean_absolute_error: 0.4397 - val_loss: 0.1930 - val_mean_absolute_error: 0.3678\nEpoch 2/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.1347 - mean_absolute_error: 0.3038 - val_loss: 0.1113 - val_mean_absolute_error: 0.2788\nEpoch 3/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0795 - mean_absolute_error: 0.2296 - val_loss: 0.0696 - val_mean_absolute_error: 0.2200\nEpoch 4/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0513 - mean_absolute_error: 0.1817 - val_loss: 0.0473 - val_mean_absolute_error: 0.1811\nEpoch 5/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0358 - mean_absolute_error: 0.1510 - val_loss: 0.0335 - val_mean_absolute_error: 0.1521\nEpoch 6/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0259 - mean_absolute_error: 0.1284 - val_loss: 0.0242 - val_mean_absolute_error: 0.1290\nEpoch 7/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0188 - mean_absolute_error: 0.1092 - val_loss: 0.0173 - val_mean_absolute_error: 0.1085\nEpoch 8/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0138 - mean_absolute_error: 0.0924 - val_loss: 0.0126 - val_mean_absolute_error: 0.0910\nEpoch 9/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0103 - mean_absolute_error: 0.0778 - val_loss: 0.0093 - val_mean_absolute_error: 0.0756\nEpoch 10/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0080 - mean_absolute_error: 0.0660 - val_loss: 0.0074 - val_mean_absolute_error: 0.0634\nEpoch 11/50\n384/384 [==============================] - 1s 4ms/step - loss: 0.0067 - mean_absolute_error: 0.0568 - val_loss: 0.0061 - val_mean_absolute_error: 0.0532\nEpoch 12/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0059 - mean_absolute_error: 0.0501 - val_loss: 0.0055 - val_mean_absolute_error: 0.0464\nEpoch 13/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0055 - mean_absolute_error: 0.0457 - val_loss: 0.0052 - val_mean_absolute_error: 0.0429\nEpoch 14/50\n384/384 [==============================] - 1s 4ms/step - loss: 0.0053 - mean_absolute_error: 0.0431 - val_loss: 0.0050 - val_mean_absolute_error: 0.0403\nEpoch 15/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0052 - mean_absolute_error: 0.0416 - val_loss: 0.0048 - val_mean_absolute_error: 0.0395\nEpoch 16/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0051 - mean_absolute_error: 0.0407 - val_loss: 0.0048 - val_mean_absolute_error: 0.0383\nEpoch 17/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0050 - mean_absolute_error: 0.0402 - val_loss: 0.0047 - val_mean_absolute_error: 0.0384\nEpoch 18/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0050 - mean_absolute_error: 0.0399 - val_loss: 0.0047 - val_mean_absolute_error: 0.0382\nEpoch 19/50\n384/384 [==============================] - 1s 4ms/step - loss: 0.0050 - mean_absolute_error: 0.0397 - val_loss: 0.0047 - val_mean_absolute_error: 0.0383\nEpoch 20/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0049 - mean_absolute_error: 0.0396 - val_loss: 0.0047 - val_mean_absolute_error: 0.0382\nEpoch 21/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0396 - val_loss: 0.0047 - val_mean_absolute_error: 0.0391\nEpoch 22/50\n384/384 [==============================] - 1s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0396 - val_loss: 0.0046 - val_mean_absolute_error: 0.0380\nEpoch 23/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0381\nEpoch 24/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0386\nEpoch 25/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0049 - mean_absolute_error: 0.0396 - val_loss: 0.0046 - val_mean_absolute_error: 0.0381\nEpoch 26/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0380\nEpoch 27/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0377\nEpoch 28/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0378\nEpoch 29/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0381\n110/110 [==============================] - 0s 3ms/step - loss: 0.0046 - mean_absolute_error: 0.0381\n55/55 [==============================] - 0s 3ms/step - loss: 0.0043 - mean_absolute_error: 0.0361\n</pre> In\u00a0[48]: Copied! <pre>mo_wide_window.plot('traffic_volume', mo_linear)\n</pre> mo_wide_window.plot('traffic_volume', mo_linear) In\u00a0[49]: Copied! <pre>pd.DataFrame(history.history)[['loss', 'val_loss']].plot()\n</pre> pd.DataFrame(history.history)[['loss', 'val_loss']].plot() Out[49]: <pre>&lt;Axes: &gt;</pre> In\u00a0[50]: Copied! <pre>dense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=1)\n])\nhistory = compile_and_fit(dense, single_step_window)\nval_performance['Dense'] = dense.evaluate(single_step_window.val)\ntest_performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)\n</pre> dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=1) ]) history = compile_and_fit(dense, single_step_window) val_performance['Dense'] = dense.evaluate(single_step_window.val) test_performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0) <pre>Epoch 1/50\n384/384 [==============================] - 3s 6ms/step - loss: 0.0116 - mean_absolute_error: 0.0700 - val_loss: 0.0044 - val_mean_absolute_error: 0.0521\nEpoch 2/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0040 - mean_absolute_error: 0.0467 - val_loss: 0.0033 - val_mean_absolute_error: 0.0435\nEpoch 3/50\n384/384 [==============================] - 3s 6ms/step - loss: 0.0035 - mean_absolute_error: 0.0431 - val_loss: 0.0029 - val_mean_absolute_error: 0.0406\nEpoch 4/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0033 - mean_absolute_error: 0.0415 - val_loss: 0.0027 - val_mean_absolute_error: 0.0389\nEpoch 5/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0031 - mean_absolute_error: 0.0400 - val_loss: 0.0027 - val_mean_absolute_error: 0.0382\nEpoch 6/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0030 - mean_absolute_error: 0.0396 - val_loss: 0.0027 - val_mean_absolute_error: 0.0385\nEpoch 7/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0028 - mean_absolute_error: 0.0381 - val_loss: 0.0023 - val_mean_absolute_error: 0.0348\nEpoch 8/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0028 - mean_absolute_error: 0.0381 - val_loss: 0.0022 - val_mean_absolute_error: 0.0341\nEpoch 9/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0027 - mean_absolute_error: 0.0368 - val_loss: 0.0023 - val_mean_absolute_error: 0.0360\nEpoch 10/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0027 - mean_absolute_error: 0.0373 - val_loss: 0.0026 - val_mean_absolute_error: 0.0384\nEpoch 11/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0026 - mean_absolute_error: 0.0365 - val_loss: 0.0021 - val_mean_absolute_error: 0.0332\nEpoch 12/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0025 - mean_absolute_error: 0.0355 - val_loss: 0.0023 - val_mean_absolute_error: 0.0362\nEpoch 13/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0025 - mean_absolute_error: 0.0356 - val_loss: 0.0022 - val_mean_absolute_error: 0.0335\nEpoch 14/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0025 - mean_absolute_error: 0.0350 - val_loss: 0.0020 - val_mean_absolute_error: 0.0319\nEpoch 15/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0025 - mean_absolute_error: 0.0349 - val_loss: 0.0022 - val_mean_absolute_error: 0.0339\nEpoch 16/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0025 - mean_absolute_error: 0.0352 - val_loss: 0.0025 - val_mean_absolute_error: 0.0383\nEpoch 17/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0024 - mean_absolute_error: 0.0349 - val_loss: 0.0021 - val_mean_absolute_error: 0.0337\n110/110 [==============================] - 1s 4ms/step - loss: 0.0021 - mean_absolute_error: 0.0337\n</pre> In\u00a0[51]: Copied! <pre>wide_window.plot('traffic_volume', dense)\n</pre> wide_window.plot('traffic_volume', dense) In\u00a0[52]: Copied! <pre>pd.DataFrame(history.history)[['loss', 'val_loss']].plot()\n</pre> pd.DataFrame(history.history)[['loss', 'val_loss']].plot() Out[52]: <pre>&lt;Axes: &gt;</pre> In\u00a0[53]: Copied! <pre>ms_dense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\nhistory = compile_and_fit(ms_dense, multi_window)\nms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val)\nms_test_performance['Dense'] = ms_dense.evaluate(multi_window.test)\n</pre> ms_dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) history = compile_and_fit(ms_dense, multi_window) ms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val) ms_test_performance['Dense'] = ms_dense.evaluate(multi_window.test) <pre>Epoch 1/50\n</pre> <pre>383/383 [==============================] - 3s 7ms/step - loss: 0.0307 - mean_absolute_error: 0.1132 - val_loss: 0.0169 - val_mean_absolute_error: 0.0860\nEpoch 2/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0141 - mean_absolute_error: 0.0790 - val_loss: 0.0161 - val_mean_absolute_error: 0.0853\nEpoch 3/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0136 - mean_absolute_error: 0.0779 - val_loss: 0.0153 - val_mean_absolute_error: 0.0842\nEpoch 4/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0133 - mean_absolute_error: 0.0772 - val_loss: 0.0152 - val_mean_absolute_error: 0.0851\nEpoch 5/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0132 - mean_absolute_error: 0.0766 - val_loss: 0.0156 - val_mean_absolute_error: 0.0813\nEpoch 6/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0130 - mean_absolute_error: 0.0758 - val_loss: 0.0147 - val_mean_absolute_error: 0.0806\nEpoch 7/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0128 - mean_absolute_error: 0.0752 - val_loss: 0.0145 - val_mean_absolute_error: 0.0804\nEpoch 8/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0127 - mean_absolute_error: 0.0749 - val_loss: 0.0145 - val_mean_absolute_error: 0.0799\nEpoch 9/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0128 - mean_absolute_error: 0.0751 - val_loss: 0.0145 - val_mean_absolute_error: 0.0788\nEpoch 10/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0127 - mean_absolute_error: 0.0747 - val_loss: 0.0143 - val_mean_absolute_error: 0.0793\nEpoch 11/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0126 - mean_absolute_error: 0.0743 - val_loss: 0.0143 - val_mean_absolute_error: 0.0794\nEpoch 12/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0126 - mean_absolute_error: 0.0744 - val_loss: 0.0143 - val_mean_absolute_error: 0.0785\nEpoch 13/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0125 - mean_absolute_error: 0.0741 - val_loss: 0.0144 - val_mean_absolute_error: 0.0775\nEpoch 14/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0125 - mean_absolute_error: 0.0740 - val_loss: 0.0142 - val_mean_absolute_error: 0.0782\nEpoch 15/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0125 - mean_absolute_error: 0.0739 - val_loss: 0.0142 - val_mean_absolute_error: 0.0781\nEpoch 16/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0125 - mean_absolute_error: 0.0742 - val_loss: 0.0142 - val_mean_absolute_error: 0.0775\nEpoch 17/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0124 - mean_absolute_error: 0.0738 - val_loss: 0.0142 - val_mean_absolute_error: 0.0801\nEpoch 18/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0124 - mean_absolute_error: 0.0736 - val_loss: 0.0142 - val_mean_absolute_error: 0.0793\nEpoch 19/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0123 - mean_absolute_error: 0.0734 - val_loss: 0.0143 - val_mean_absolute_error: 0.0815\nEpoch 20/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0123 - mean_absolute_error: 0.0732 - val_loss: 0.0145 - val_mean_absolute_error: 0.0823\nEpoch 21/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0122 - mean_absolute_error: 0.0731 - val_loss: 0.0140 - val_mean_absolute_error: 0.0766\nEpoch 22/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0122 - mean_absolute_error: 0.0732 - val_loss: 0.0139 - val_mean_absolute_error: 0.0777\nEpoch 23/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0121 - mean_absolute_error: 0.0727 - val_loss: 0.0141 - val_mean_absolute_error: 0.0767\nEpoch 24/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0121 - mean_absolute_error: 0.0726 - val_loss: 0.0140 - val_mean_absolute_error: 0.0756\nEpoch 25/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0121 - mean_absolute_error: 0.0725 - val_loss: 0.0142 - val_mean_absolute_error: 0.0751\n109/109 [==============================] - 1s 4ms/step - loss: 0.0142 - mean_absolute_error: 0.0751\n54/54 [==============================] - 0s 4ms/step - loss: 0.0096 - mean_absolute_error: 0.0618\n</pre> In\u00a0[54]: Copied! <pre>multi_window.plot('traffic_volume', ms_dense)\n</pre> multi_window.plot('traffic_volume', ms_dense) In\u00a0[55]: Copied! <pre>pd.DataFrame(history.history)[['loss', 'val_loss']].plot()\n</pre> pd.DataFrame(history.history)[['loss', 'val_loss']].plot() Out[55]: <pre>&lt;Axes: &gt;</pre> In\u00a0[56]: Copied! <pre>mo_dense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=2)\n])\nhistory = compile_and_fit(mo_dense, mo_single_step_window)\nmo_val_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.val)\nmo_test_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.test)\n</pre> mo_dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=2) ]) history = compile_and_fit(mo_dense, mo_single_step_window) mo_val_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.val) mo_test_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.test) <pre>Epoch 1/50\n384/384 [==============================] - 3s 5ms/step - loss: 0.0127 - mean_absolute_error: 0.0642 - val_loss: 0.0031 - val_mean_absolute_error: 0.0349\nEpoch 2/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0028 - mean_absolute_error: 0.0324 - val_loss: 0.0023 - val_mean_absolute_error: 0.0302\nEpoch 3/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0024 - mean_absolute_error: 0.0299 - val_loss: 0.0022 - val_mean_absolute_error: 0.0304\nEpoch 4/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0022 - mean_absolute_error: 0.0285 - val_loss: 0.0017 - val_mean_absolute_error: 0.0262\nEpoch 5/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0020 - mean_absolute_error: 0.0277 - val_loss: 0.0016 - val_mean_absolute_error: 0.0255\nEpoch 6/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0020 - mean_absolute_error: 0.0277 - val_loss: 0.0016 - val_mean_absolute_error: 0.0258\nEpoch 7/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0019 - mean_absolute_error: 0.0270 - val_loss: 0.0015 - val_mean_absolute_error: 0.0256\nEpoch 8/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0019 - mean_absolute_error: 0.0264 - val_loss: 0.0015 - val_mean_absolute_error: 0.0242\nEpoch 9/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0018 - mean_absolute_error: 0.0260 - val_loss: 0.0015 - val_mean_absolute_error: 0.0243\nEpoch 10/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0018 - mean_absolute_error: 0.0257 - val_loss: 0.0018 - val_mean_absolute_error: 0.0288\nEpoch 11/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0017 - mean_absolute_error: 0.0255 - val_loss: 0.0014 - val_mean_absolute_error: 0.0239\nEpoch 12/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0017 - mean_absolute_error: 0.0249 - val_loss: 0.0014 - val_mean_absolute_error: 0.0240\nEpoch 13/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0017 - mean_absolute_error: 0.0250 - val_loss: 0.0013 - val_mean_absolute_error: 0.0233\nEpoch 14/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0016 - mean_absolute_error: 0.0244 - val_loss: 0.0013 - val_mean_absolute_error: 0.0230\nEpoch 15/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0016 - mean_absolute_error: 0.0241 - val_loss: 0.0014 - val_mean_absolute_error: 0.0274\nEpoch 16/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0016 - mean_absolute_error: 0.0242 - val_loss: 0.0014 - val_mean_absolute_error: 0.0238\nEpoch 17/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0016 - mean_absolute_error: 0.0243 - val_loss: 0.0012 - val_mean_absolute_error: 0.0227\nEpoch 18/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0016 - mean_absolute_error: 0.0240 - val_loss: 0.0012 - val_mean_absolute_error: 0.0224\nEpoch 19/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0016 - mean_absolute_error: 0.0241 - val_loss: 0.0013 - val_mean_absolute_error: 0.0236\nEpoch 20/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0015 - mean_absolute_error: 0.0235 - val_loss: 0.0015 - val_mean_absolute_error: 0.0271\nEpoch 21/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0015 - mean_absolute_error: 0.0237 - val_loss: 0.0013 - val_mean_absolute_error: 0.0239\n110/110 [==============================] - 0s 3ms/step - loss: 0.0013 - mean_absolute_error: 0.0239\n55/55 [==============================] - 0s 3ms/step - loss: 9.0741e-04 - mean_absolute_error: 0.0203\n</pre> In\u00a0[57]: Copied! <pre>mo_wide_window.plot('traffic_volume', mo_dense)\n</pre> mo_wide_window.plot('traffic_volume', mo_dense) In\u00a0[58]: Copied! <pre>pd.DataFrame(history.history)[['loss', 'val_loss']].plot()\n</pre> pd.DataFrame(history.history)[['loss', 'val_loss']].plot() Out[58]: <pre>&lt;Axes: &gt;</pre> In\u00a0[72]: Copied! <pre># single step\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Single Step')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(val_performance.keys(), [v[1] for v in val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(test_performance.keys(), [v[1] for v in test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.legend()\nplt.tight_layout()\n</pre> # single step\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Single Step') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(val_performance.keys(), [v[1] for v in val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(test_performance.keys(), [v[1] for v in test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.legend() plt.tight_layout() In\u00a0[73]: Copied! <pre># multi step\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Multi Step')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(ms_val_performance.keys(), [v[1] for v in ms_val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(ms_test_performance.keys(), [v[1] for v in ms_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.legend()\nplt.tight_layout()\n</pre> # multi step\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Multi Step') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(ms_val_performance.keys(), [v[1] for v in ms_val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(ms_test_performance.keys(), [v[1] for v in ms_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.legend() plt.tight_layout() In\u00a0[74]: Copied! <pre># multi output\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Multi Output')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(mo_val_performance.keys(), [v[1] for v in mo_val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(mo_test_performance.keys(), [v[1] for v in mo_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.legend()\nplt.tight_layout()\n</pre> # multi output\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Multi Output') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(mo_val_performance.keys(), [v[1] for v in mo_val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(mo_test_performance.keys(), [v[1] for v in mo_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.legend() plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter14.html","title":"\u7dda\u5f62\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3059\u308b\u00b6","text":""},{"location":"chapter14.html","title":"\u30b7\u30f3\u30b0\u30eb\u30b9\u30c6\u30c3\u30d7\u306e\u7dda\u5f62\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3059\u308b\u00b6","text":""},{"location":"chapter14.html","title":"\u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7\u306e\u7dda\u5f62\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3059\u308b\u00b6","text":""},{"location":"chapter14.html","title":"\u591a\u51fa\u529b\u306e\u7dda\u5f62\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3059\u308b\u00b6","text":""},{"location":"chapter14.html#dnn","title":"\u30c7\u30a3\u30fc\u30d7\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(DNN)\u3092\u5b9f\u88c5\u3059\u308b\u00b6","text":""},{"location":"chapter15.html","title":"\u7b2c15\u7ae0 LSTM\u3067\u904e\u53bb\u3092\u8a18\u61b6\u3059\u308b","text":"In\u00a0[9]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.metrics import MeanAbsoluteError\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.losses import MeanSquaredError from tensorflow.keras.metrics import MeanAbsoluteError from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.optimizers import Adam from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense, LSTM In\u00a0[2]: Copied! <pre>url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv'\nurl_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv'\nurl_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'\n\ndf_train = pd.read_csv(url_train, index_col=0)\ndf_val = pd.read_csv(url_val, index_col=0)\ndf_test = pd.read_csv(url_test, index_col=0)\n</pre> url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv' url_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv' url_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'  df_train = pd.read_csv(url_train, index_col=0) df_val = pd.read_csv(url_val, index_col=0) df_test = pd.read_csv(url_test, index_col=0) In\u00a0[3]: Copied! <pre>class DataWindow:\n    def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):\n        # window size\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n        self.total_window_size = input_width + shift\n        \n        # \u30c7\u30fc\u30bf\n        self.df_train = df_train\n        self.df_val = df_val\n        self.df_test = df_test\n        \n        # \u30e9\u30d9\u30eb\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}\n        \n        # \u30b9\u30e9\u30a4\u30b9\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n        \n        # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e\n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n    \n    def split_to_inputs_labels(self, features):\n        inputs = features[:, self.input_slice, :]\n        labels = features[:, self.labels_slice, :]\n        if self.label_columns is not None:\n            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n        inputs.set_shape([None, self.input_width, None])\n        labels.set_shape([None, self.label_width, None])\n        return inputs, labels\n    \n    def plot(self, plot_col: str, model=None, max_subplots=3):\n        inputs, labels = self.sample_batch\n        plt.figure(figsize=(12, 8))\n        plot_col_index = self.column_indices[plot_col]\n        n_max = min(max_subplots, len(inputs))\n        \n        for n in range(n_max):\n            plt.subplot(n_max, 1, n+1)\n            plt.ylabel(f'{plot_col} [scaled]')\n            plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)\n            \n            if self.label_columns:\n                label_col_index = self.label_columns_indices.get(plot_col, None)\n            else:\n                label_col_index = plot_col_index\n            \n            if label_col_index is None:\n                continue\n            \n            plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)\n            \n            if model is not None:\n                predictions = model(inputs)\n                plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)\n            \n            if n == 0:\n                plt.legend()\n        plt.xlabel('Time (h)')\n    \n    def make_dataset(self, data):\n        data = np.array(data, dtype=np.float32)\n        ds = tf.keras.utils.timeseries_dataset_from_array(\n            data=data,\n            targets=None,\n            sequence_length=self.total_window_size,\n            sequence_stride=1,\n            shuffle=True,\n            batch_size=32,\n        )\n        ds = ds.map(self.split_to_inputs_labels)\n        return ds\n    \n    @property\n    def train(self):\n        return self.make_dataset(self.df_train)\n    \n    @property\n    def val(self):\n        return self.make_dataset(self.df_val)\n    \n    @property\n    def test(self):\n        return self.make_dataset(self.df_test)\n    \n    @property\n    def sample_batch(self):\n        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n        result = getattr(self, '_sample_batch', None)\n        if result is None:\n            result = next(iter(self.train))\n            self._sample_batch = result\n        return result\n</pre> class DataWindow:     def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):         # window size         self.input_width = input_width         self.label_width = label_width         self.shift = shift         self.total_window_size = input_width + shift                  # \u30c7\u30fc\u30bf         self.df_train = df_train         self.df_val = df_val         self.df_test = df_test                  # \u30e9\u30d9\u30eb         self.label_columns = label_columns         if label_columns is not None:             self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}         self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}                  # \u30b9\u30e9\u30a4\u30b9         self.input_slice = slice(0, input_width)         self.input_indices = np.arange(self.total_window_size)[self.input_slice]                  # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e         self.label_start = self.total_window_size - self.label_width         self.labels_slice = slice(self.label_start, None)         self.label_indices = np.arange(self.total_window_size)[self.labels_slice]          def split_to_inputs_labels(self, features):         inputs = features[:, self.input_slice, :]         labels = features[:, self.labels_slice, :]         if self.label_columns is not None:             labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)         inputs.set_shape([None, self.input_width, None])         labels.set_shape([None, self.label_width, None])         return inputs, labels          def plot(self, plot_col: str, model=None, max_subplots=3):         inputs, labels = self.sample_batch         plt.figure(figsize=(12, 8))         plot_col_index = self.column_indices[plot_col]         n_max = min(max_subplots, len(inputs))                  for n in range(n_max):             plt.subplot(n_max, 1, n+1)             plt.ylabel(f'{plot_col} [scaled]')             plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)                          if self.label_columns:                 label_col_index = self.label_columns_indices.get(plot_col, None)             else:                 label_col_index = plot_col_index                          if label_col_index is None:                 continue                          plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)                          if model is not None:                 predictions = model(inputs)                 plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)                          if n == 0:                 plt.legend()         plt.xlabel('Time (h)')          def make_dataset(self, data):         data = np.array(data, dtype=np.float32)         ds = tf.keras.utils.timeseries_dataset_from_array(             data=data,             targets=None,             sequence_length=self.total_window_size,             sequence_stride=1,             shuffle=True,             batch_size=32,         )         ds = ds.map(self.split_to_inputs_labels)         return ds          @property     def train(self):         return self.make_dataset(self.df_train)          @property     def val(self):         return self.make_dataset(self.df_val)          @property     def test(self):         return self.make_dataset(self.df_test)          @property     def sample_batch(self):         \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"         result = getattr(self, '_sample_batch', None)         if result is None:             result = next(iter(self.train))             self._sample_batch = result         return result In\u00a0[5]: Copied! <pre># for training\ndef compile_and_fit(model, window, patience=3, max_epochs=50):\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=patience,\n        mode='min'\n    )\n    model.compile(\n        loss=MeanSquaredError(),\n        optimizer=Adam(),\n        metrics=[MeanAbsoluteError()]\n    )\n    history = model.fit(\n        window.train,\n        epochs=max_epochs,\n        validation_data=window.val,\n        callbacks=[early_stopping]\n    )\n    return history\n</pre> # for training def compile_and_fit(model, window, patience=3, max_epochs=50):     early_stopping = EarlyStopping(         monitor='val_loss',         patience=patience,         mode='min'     )     model.compile(         loss=MeanSquaredError(),         optimizer=Adam(),         metrics=[MeanAbsoluteError()]     )     history = model.fit(         window.train,         epochs=max_epochs,         validation_data=window.val,         callbacks=[early_stopping]     )     return history In\u00a0[7]: Copied! <pre># models\nclass Baseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n        \n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        elif isinstance(self.label_index, list):\n            tensors = []\n            for index in self.label_index:\n                res = inputs[:, :, index]\n                res = res[:, :, tf.newaxis]\n                tensors.append(res)\n            return tf.concat(tensors, axis=-1)\n        else:\n            res = inputs[:, :, self.label_index]\n            return res[:, :, tf.newaxis]\n\nclass MultiStepLastBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        if self.label_index is None:\n            return tf.tile(inputs[:, -1:, :], [1, 24, 1])\n        return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])\n    \nclass RepeatBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        return inputs[:, :, self.label_index:]\n\n# \u7dda\u5f62\u30e2\u30c7\u30eb\nlinear = Sequential([Dense(units=1)])\nms_linear = Sequential([Dense(units=1, kernel_initializer=tf.initializers.zeros)])\nmo_linear = Sequential([Dense(units=2)])\n# DNN\ndense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=1)\n])\nms_dense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\nmo_dense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=2)\n])\n</pre> # models class Baseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index              def call(self, inputs):         if self.label_index is None:             return inputs         elif isinstance(self.label_index, list):             tensors = []             for index in self.label_index:                 res = inputs[:, :, index]                 res = res[:, :, tf.newaxis]                 tensors.append(res)             return tf.concat(tensors, axis=-1)         else:             res = inputs[:, :, self.label_index]             return res[:, :, tf.newaxis]  class MultiStepLastBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         if self.label_index is None:             return tf.tile(inputs[:, -1:, :], [1, 24, 1])         return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])      class RepeatBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         return inputs[:, :, self.label_index:]  # \u7dda\u5f62\u30e2\u30c7\u30eb linear = Sequential([Dense(units=1)]) ms_linear = Sequential([Dense(units=1, kernel_initializer=tf.initializers.zeros)]) mo_linear = Sequential([Dense(units=2)]) # DNN dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=1) ]) ms_dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) mo_dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=2) ]) In\u00a0[8]: Copied! <pre># \u30b7\u30f3\u30b0\u30eb\u30b9\u30c6\u30c3\u30d7\nsingle_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\nwide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\ncolumn_indices = {name: i for i, name in enumerate(df_train.columns)}\nbaseline_last = Baseline(column_indices['traffic_volume'])\nbaseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nval_performance = {}\ntest_performance = {}\n\n## \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\nval_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.val)\ntest_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.test, verbose=0)\n## \u7dda\u5f62\u30e2\u30c7\u30eb\nhistory = compile_and_fit(linear, single_step_window)\nval_performance['Linear'] = linear.evaluate(single_step_window.val)\ntest_performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)\n## DNN\nhistory = compile_and_fit(dense, single_step_window)\nval_performance['Dense'] = dense.evaluate(single_step_window.val)\ntest_performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)\n\n# \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7\nmulti_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\nms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\nms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nms_val_performance = {}\nms_test_performance = {}\n\n## \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\nms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val)\nms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test)\nms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val)\nms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test)\n## \u7dda\u5f62\u30e2\u30c7\u30eb\nhistory = compile_and_fit(ms_linear, multi_window)\nms_val_performance['Linear'] = ms_linear.evaluate(multi_window.val)\nms_test_performance['Linear'] = ms_linear.evaluate(multi_window.test)\n## DNN\nhistory = compile_and_fit(ms_dense, multi_window)\nms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val)\nms_test_performance['Dense'] = ms_dense.evaluate(multi_window.test)\n\n# \u591a\u5909\u6570\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\ncol_names = ['temp', 'traffic_volume']\nmo_single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)\nmo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)\nmo_baseline_last = Baseline(label_index=[column_indices[col] for col in col_names])\nmo_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nmo_val_performance = {}\nmo_test_performance = {}\n\n## \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\nmo_val_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)\nmo_test_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)\n## \u7dda\u5f62\u30e2\u30c7\u30eb\nhistory = compile_and_fit(mo_linear, mo_single_step_window)\nmo_val_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.val)\nmo_test_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.test)\n## DNN\nhistory = compile_and_fit(mo_dense, mo_single_step_window)\nmo_val_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.val)\nmo_test_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.test)\n</pre> # \u30b7\u30f3\u30b0\u30eb\u30b9\u30c6\u30c3\u30d7 single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) column_indices = {name: i for i, name in enumerate(df_train.columns)} baseline_last = Baseline(column_indices['traffic_volume']) baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  val_performance = {} test_performance = {}  ## \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3 val_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.val) test_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.test, verbose=0) ## \u7dda\u5f62\u30e2\u30c7\u30eb history = compile_and_fit(linear, single_step_window) val_performance['Linear'] = linear.evaluate(single_step_window.val) test_performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0) ## DNN history = compile_and_fit(dense, single_step_window) val_performance['Dense'] = dense.evaluate(single_step_window.val) test_performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)  # \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7 multi_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) ms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume']) ms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()]) ms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume']) ms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  ms_val_performance = {} ms_test_performance = {}  ## \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3 ms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val) ms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test) ms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val) ms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test) ## \u7dda\u5f62\u30e2\u30c7\u30eb history = compile_and_fit(ms_linear, multi_window) ms_val_performance['Linear'] = ms_linear.evaluate(multi_window.val) ms_test_performance['Linear'] = ms_linear.evaluate(multi_window.test) ## DNN history = compile_and_fit(ms_dense, multi_window) ms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val) ms_test_performance['Dense'] = ms_dense.evaluate(multi_window.test)  # \u591a\u5909\u6570\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8 col_names = ['temp', 'traffic_volume'] mo_single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names) mo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names) mo_baseline_last = Baseline(label_index=[column_indices[col] for col in col_names]) mo_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  mo_val_performance = {} mo_test_performance = {}  ## \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3 mo_val_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val) mo_test_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val) ## \u7dda\u5f62\u30e2\u30c7\u30eb history = compile_and_fit(mo_linear, mo_single_step_window) mo_val_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.val) mo_test_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.test) ## DNN history = compile_and_fit(mo_dense, mo_single_step_window) mo_val_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.val) mo_test_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.test) <pre>110/110 [==============================] - 0s 2ms/step - loss: 0.0133 - mean_absolute_error: 0.0831\nEpoch 1/50\n384/384 [==============================] - 2s 3ms/step - loss: 0.1478 - mean_absolute_error: 0.3296 - val_loss: 0.0949 - val_mean_absolute_error: 0.2637\nEpoch 2/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0575 - mean_absolute_error: 0.1931 - val_loss: 0.0423 - val_mean_absolute_error: 0.1688\nEpoch 3/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0287 - mean_absolute_error: 0.1313 - val_loss: 0.0253 - val_mean_absolute_error: 0.1276\nEpoch 4/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0192 - mean_absolute_error: 0.1063 - val_loss: 0.0182 - val_mean_absolute_error: 0.1055\nEpoch 5/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0150 - mean_absolute_error: 0.0927 - val_loss: 0.0146 - val_mean_absolute_error: 0.0928\nEpoch 6/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0128 - mean_absolute_error: 0.0843 - val_loss: 0.0124 - val_mean_absolute_error: 0.0836\nEpoch 7/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0115 - mean_absolute_error: 0.0791 - val_loss: 0.0112 - val_mean_absolute_error: 0.0770\nEpoch 8/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0107 - mean_absolute_error: 0.0756 - val_loss: 0.0104 - val_mean_absolute_error: 0.0726\nEpoch 9/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0102 - mean_absolute_error: 0.0732 - val_loss: 0.0099 - val_mean_absolute_error: 0.0712\nEpoch 10/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0098 - mean_absolute_error: 0.0715 - val_loss: 0.0096 - val_mean_absolute_error: 0.0685\nEpoch 11/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0096 - mean_absolute_error: 0.0702 - val_loss: 0.0094 - val_mean_absolute_error: 0.0679\nEpoch 12/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0095 - mean_absolute_error: 0.0694 - val_loss: 0.0093 - val_mean_absolute_error: 0.0674\nEpoch 13/50\n384/384 [==============================] - 4s 9ms/step - loss: 0.0094 - mean_absolute_error: 0.0688 - val_loss: 0.0092 - val_mean_absolute_error: 0.0666\nEpoch 14/50\n384/384 [==============================] - 3s 8ms/step - loss: 0.0093 - mean_absolute_error: 0.0685 - val_loss: 0.0091 - val_mean_absolute_error: 0.0678\nEpoch 15/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0682 - val_loss: 0.0091 - val_mean_absolute_error: 0.0679\nEpoch 16/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0680 - val_loss: 0.0091 - val_mean_absolute_error: 0.0686\nEpoch 17/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0680 - val_loss: 0.0091 - val_mean_absolute_error: 0.0665\nEpoch 18/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0093 - mean_absolute_error: 0.0678 - val_loss: 0.0091 - val_mean_absolute_error: 0.0666\nEpoch 19/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0091 - val_mean_absolute_error: 0.0677\nEpoch 20/50\n384/384 [==============================] - 3s 8ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0090 - val_mean_absolute_error: 0.0665\nEpoch 21/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0091 - val_mean_absolute_error: 0.0673\nEpoch 22/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0090 - val_mean_absolute_error: 0.0663\nEpoch 23/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0090 - val_mean_absolute_error: 0.0664\nEpoch 24/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0678 - val_loss: 0.0090 - val_mean_absolute_error: 0.0663\nEpoch 25/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0091 - val_mean_absolute_error: 0.0675\n110/110 [==============================] - 1s 6ms/step - loss: 0.0091 - mean_absolute_error: 0.0675\nEpoch 1/50\n384/384 [==============================] - 3s 5ms/step - loss: 0.0121 - mean_absolute_error: 0.0733 - val_loss: 0.0050 - val_mean_absolute_error: 0.0526\nEpoch 2/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0048 - mean_absolute_error: 0.0510 - val_loss: 0.0043 - val_mean_absolute_error: 0.0504\nEpoch 3/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0041 - mean_absolute_error: 0.0470 - val_loss: 0.0035 - val_mean_absolute_error: 0.0439\nEpoch 4/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0038 - mean_absolute_error: 0.0449 - val_loss: 0.0031 - val_mean_absolute_error: 0.0416\nEpoch 5/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0035 - mean_absolute_error: 0.0430 - val_loss: 0.0037 - val_mean_absolute_error: 0.0484\nEpoch 6/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0034 - mean_absolute_error: 0.0425 - val_loss: 0.0027 - val_mean_absolute_error: 0.0385\nEpoch 7/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0032 - mean_absolute_error: 0.0406 - val_loss: 0.0026 - val_mean_absolute_error: 0.0373\nEpoch 8/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0032 - mean_absolute_error: 0.0409 - val_loss: 0.0026 - val_mean_absolute_error: 0.0372\nEpoch 9/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0029 - mean_absolute_error: 0.0385 - val_loss: 0.0035 - val_mean_absolute_error: 0.0438\nEpoch 10/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0029 - mean_absolute_error: 0.0386 - val_loss: 0.0023 - val_mean_absolute_error: 0.0351\nEpoch 11/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0028 - mean_absolute_error: 0.0379 - val_loss: 0.0026 - val_mean_absolute_error: 0.0374\nEpoch 12/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0029 - mean_absolute_error: 0.0381 - val_loss: 0.0022 - val_mean_absolute_error: 0.0342\nEpoch 13/50\n384/384 [==============================] - 3s 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0372 - val_loss: 0.0023 - val_mean_absolute_error: 0.0346\nEpoch 14/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0027 - mean_absolute_error: 0.0366 - val_loss: 0.0022 - val_mean_absolute_error: 0.0343\nEpoch 15/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0026 - mean_absolute_error: 0.0359 - val_loss: 0.0021 - val_mean_absolute_error: 0.0328\nEpoch 16/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0026 - mean_absolute_error: 0.0354 - val_loss: 0.0022 - val_mean_absolute_error: 0.0342\nEpoch 17/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0026 - mean_absolute_error: 0.0357 - val_loss: 0.0020 - val_mean_absolute_error: 0.0323\nEpoch 18/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0026 - mean_absolute_error: 0.0358 - val_loss: 0.0021 - val_mean_absolute_error: 0.0337\nEpoch 19/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0025 - mean_absolute_error: 0.0351 - val_loss: 0.0019 - val_mean_absolute_error: 0.0316\nEpoch 20/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0025 - mean_absolute_error: 0.0348 - val_loss: 0.0022 - val_mean_absolute_error: 0.0356\nEpoch 21/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0024 - mean_absolute_error: 0.0346 - val_loss: 0.0023 - val_mean_absolute_error: 0.0354\nEpoch 22/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0024 - mean_absolute_error: 0.0345 - val_loss: 0.0020 - val_mean_absolute_error: 0.0327\n110/110 [==============================] - 0s 3ms/step - loss: 0.0020 - mean_absolute_error: 0.0327\n109/109 [==============================] - 0s 2ms/step - loss: 0.1875 - mean_absolute_error: 0.3522\n54/54 [==============================] - 0s 2ms/step - loss: 0.1814 - mean_absolute_error: 0.3473\n109/109 [==============================] - 0s 2ms/step - loss: 0.2065 - mean_absolute_error: 0.3473\n54/54 [==============================] - 0s 2ms/step - loss: 0.2018 - mean_absolute_error: 0.3413\nEpoch 1/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0901 - mean_absolute_error: 0.2467 - val_loss: 0.0459 - val_mean_absolute_error: 0.1824\nEpoch 2/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0299 - mean_absolute_error: 0.1371 - val_loss: 0.0261 - val_mean_absolute_error: 0.1253\nEpoch 3/50\n383/383 [==============================] - 1s 3ms/step - loss: 0.0201 - mean_absolute_error: 0.1045 - val_loss: 0.0213 - val_mean_absolute_error: 0.1074\nEpoch 4/50\n383/383 [==============================] - 1s 3ms/step - loss: 0.0177 - mean_absolute_error: 0.0955 - val_loss: 0.0196 - val_mean_absolute_error: 0.1000\nEpoch 5/50\n383/383 [==============================] - 1s 3ms/step - loss: 0.0168 - mean_absolute_error: 0.0914 - val_loss: 0.0189 - val_mean_absolute_error: 0.0957\nEpoch 6/50\n383/383 [==============================] - 1s 3ms/step - loss: 0.0164 - mean_absolute_error: 0.0887 - val_loss: 0.0185 - val_mean_absolute_error: 0.0926\nEpoch 7/50\n383/383 [==============================] - 1s 3ms/step - loss: 0.0162 - mean_absolute_error: 0.0869 - val_loss: 0.0183 - val_mean_absolute_error: 0.0907\nEpoch 8/50\n383/383 [==============================] - 3s 8ms/step - loss: 0.0161 - mean_absolute_error: 0.0857 - val_loss: 0.0182 - val_mean_absolute_error: 0.0896\nEpoch 9/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0161 - mean_absolute_error: 0.0850 - val_loss: 0.0182 - val_mean_absolute_error: 0.0890\nEpoch 10/50\n383/383 [==============================] - 3s 9ms/step - loss: 0.0161 - mean_absolute_error: 0.0846 - val_loss: 0.0182 - val_mean_absolute_error: 0.0888\nEpoch 11/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0161 - mean_absolute_error: 0.0843 - val_loss: 0.0182 - val_mean_absolute_error: 0.0887\nEpoch 12/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0841 - val_loss: 0.0182 - val_mean_absolute_error: 0.0891\nEpoch 13/50\n383/383 [==============================] - 1s 4ms/step - loss: 0.0161 - mean_absolute_error: 0.0840 - val_loss: 0.0182 - val_mean_absolute_error: 0.0886\n109/109 [==============================] - 0s 3ms/step - loss: 0.0182 - mean_absolute_error: 0.0886\n54/54 [==============================] - 0s 5ms/step - loss: 0.0142 - mean_absolute_error: 0.0766\nEpoch 1/50\n383/383 [==============================] - 4s 8ms/step - loss: 0.0304 - mean_absolute_error: 0.1116 - val_loss: 0.0168 - val_mean_absolute_error: 0.0857\nEpoch 2/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0142 - mean_absolute_error: 0.0789 - val_loss: 0.0161 - val_mean_absolute_error: 0.0833\nEpoch 3/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0138 - mean_absolute_error: 0.0784 - val_loss: 0.0156 - val_mean_absolute_error: 0.0823\nEpoch 4/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0135 - mean_absolute_error: 0.0775 - val_loss: 0.0154 - val_mean_absolute_error: 0.0817\nEpoch 5/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0134 - mean_absolute_error: 0.0777 - val_loss: 0.0152 - val_mean_absolute_error: 0.0816\nEpoch 6/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0131 - mean_absolute_error: 0.0767 - val_loss: 0.0149 - val_mean_absolute_error: 0.0829\nEpoch 7/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0130 - mean_absolute_error: 0.0763 - val_loss: 0.0146 - val_mean_absolute_error: 0.0809\nEpoch 8/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0130 - mean_absolute_error: 0.0761 - val_loss: 0.0146 - val_mean_absolute_error: 0.0812\nEpoch 9/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0129 - mean_absolute_error: 0.0760 - val_loss: 0.0146 - val_mean_absolute_error: 0.0798\nEpoch 10/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0129 - mean_absolute_error: 0.0759 - val_loss: 0.0147 - val_mean_absolute_error: 0.0804\nEpoch 11/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0128 - mean_absolute_error: 0.0754 - val_loss: 0.0145 - val_mean_absolute_error: 0.0790\nEpoch 12/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0128 - mean_absolute_error: 0.0756 - val_loss: 0.0148 - val_mean_absolute_error: 0.0800\nEpoch 13/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0127 - mean_absolute_error: 0.0753 - val_loss: 0.0146 - val_mean_absolute_error: 0.0804\nEpoch 14/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0127 - mean_absolute_error: 0.0751 - val_loss: 0.0146 - val_mean_absolute_error: 0.0812\n109/109 [==============================] - 0s 3ms/step - loss: 0.0146 - mean_absolute_error: 0.0812\n54/54 [==============================] - 0s 3ms/step - loss: 0.0100 - mean_absolute_error: 0.0668\n110/110 [==============================] - 1s 3ms/step - loss: 0.0069 - mean_absolute_error: 0.0482\n110/110 [==============================] - 0s 3ms/step - loss: 0.0069 - mean_absolute_error: 0.0482\nEpoch 1/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.1803 - mean_absolute_error: 0.3456 - val_loss: 0.0670 - val_mean_absolute_error: 0.2038\nEpoch 2/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0444 - mean_absolute_error: 0.1666 - val_loss: 0.0279 - val_mean_absolute_error: 0.1320\nEpoch 3/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0207 - mean_absolute_error: 0.1105 - val_loss: 0.0148 - val_mean_absolute_error: 0.0911\nEpoch 4/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0119 - mean_absolute_error: 0.0790 - val_loss: 0.0095 - val_mean_absolute_error: 0.0684\nEpoch 5/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0084 - mean_absolute_error: 0.0632 - val_loss: 0.0072 - val_mean_absolute_error: 0.0569\nEpoch 6/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0069 - mean_absolute_error: 0.0557 - val_loss: 0.0061 - val_mean_absolute_error: 0.0508\nEpoch 7/50\n384/384 [==============================] - 3s 6ms/step - loss: 0.0061 - mean_absolute_error: 0.0518 - val_loss: 0.0056 - val_mean_absolute_error: 0.0478\nEpoch 8/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0057 - mean_absolute_error: 0.0495 - val_loss: 0.0053 - val_mean_absolute_error: 0.0462\nEpoch 9/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0055 - mean_absolute_error: 0.0478 - val_loss: 0.0051 - val_mean_absolute_error: 0.0450\nEpoch 10/50\n384/384 [==============================] - 4s 10ms/step - loss: 0.0053 - mean_absolute_error: 0.0463 - val_loss: 0.0049 - val_mean_absolute_error: 0.0439\nEpoch 11/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0052 - mean_absolute_error: 0.0449 - val_loss: 0.0048 - val_mean_absolute_error: 0.0421\nEpoch 12/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0051 - mean_absolute_error: 0.0435 - val_loss: 0.0048 - val_mean_absolute_error: 0.0406\nEpoch 13/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0050 - mean_absolute_error: 0.0422 - val_loss: 0.0047 - val_mean_absolute_error: 0.0400\nEpoch 14/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0050 - mean_absolute_error: 0.0412 - val_loss: 0.0047 - val_mean_absolute_error: 0.0387\nEpoch 15/50\n384/384 [==============================] - 3s 8ms/step - loss: 0.0050 - mean_absolute_error: 0.0405 - val_loss: 0.0047 - val_mean_absolute_error: 0.0388\nEpoch 16/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0049 - mean_absolute_error: 0.0399 - val_loss: 0.0046 - val_mean_absolute_error: 0.0383\nEpoch 17/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0049 - mean_absolute_error: 0.0396 - val_loss: 0.0046 - val_mean_absolute_error: 0.0377\nEpoch 18/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0380\nEpoch 19/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0378\nEpoch 20/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0375\nEpoch 21/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0394 - val_loss: 0.0046 - val_mean_absolute_error: 0.0380\nEpoch 22/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0381\n110/110 [==============================] - 0s 3ms/step - loss: 0.0046 - mean_absolute_error: 0.0381\n55/55 [==============================] - 0s 2ms/step - loss: 0.0043 - mean_absolute_error: 0.0360\nEpoch 1/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0108 - mean_absolute_error: 0.0606 - val_loss: 0.0030 - val_mean_absolute_error: 0.0351\nEpoch 2/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0029 - mean_absolute_error: 0.0333 - val_loss: 0.0024 - val_mean_absolute_error: 0.0324\nEpoch 3/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0023 - mean_absolute_error: 0.0300 - val_loss: 0.0018 - val_mean_absolute_error: 0.0282\nEpoch 4/50\n384/384 [==============================] - 3s 8ms/step - loss: 0.0021 - mean_absolute_error: 0.0290 - val_loss: 0.0017 - val_mean_absolute_error: 0.0274\nEpoch 5/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0021 - mean_absolute_error: 0.0284 - val_loss: 0.0023 - val_mean_absolute_error: 0.0315\nEpoch 6/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0019 - mean_absolute_error: 0.0272 - val_loss: 0.0015 - val_mean_absolute_error: 0.0253\nEpoch 7/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0019 - mean_absolute_error: 0.0271 - val_loss: 0.0015 - val_mean_absolute_error: 0.0248\nEpoch 8/50\n384/384 [==============================] - 3s 6ms/step - loss: 0.0019 - mean_absolute_error: 0.0267 - val_loss: 0.0016 - val_mean_absolute_error: 0.0260\nEpoch 9/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0018 - mean_absolute_error: 0.0263 - val_loss: 0.0014 - val_mean_absolute_error: 0.0248\nEpoch 10/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0018 - mean_absolute_error: 0.0262 - val_loss: 0.0014 - val_mean_absolute_error: 0.0248\nEpoch 11/50\n384/384 [==============================] - 4s 11ms/step - loss: 0.0018 - mean_absolute_error: 0.0258 - val_loss: 0.0016 - val_mean_absolute_error: 0.0260\nEpoch 12/50\n384/384 [==============================] - 5s 13ms/step - loss: 0.0017 - mean_absolute_error: 0.0255 - val_loss: 0.0014 - val_mean_absolute_error: 0.0236\nEpoch 13/50\n384/384 [==============================] - 3s 8ms/step - loss: 0.0017 - mean_absolute_error: 0.0254 - val_loss: 0.0014 - val_mean_absolute_error: 0.0237\nEpoch 14/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0017 - mean_absolute_error: 0.0248 - val_loss: 0.0013 - val_mean_absolute_error: 0.0235\nEpoch 15/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0017 - mean_absolute_error: 0.0249 - val_loss: 0.0013 - val_mean_absolute_error: 0.0230\nEpoch 16/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0016 - mean_absolute_error: 0.0248 - val_loss: 0.0014 - val_mean_absolute_error: 0.0238\nEpoch 17/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0017 - mean_absolute_error: 0.0248 - val_loss: 0.0014 - val_mean_absolute_error: 0.0250\nEpoch 18/50\n384/384 [==============================] - 3s 8ms/step - loss: 0.0016 - mean_absolute_error: 0.0243 - val_loss: 0.0014 - val_mean_absolute_error: 0.0246\n110/110 [==============================] - 1s 4ms/step - loss: 0.0014 - mean_absolute_error: 0.0246\n55/55 [==============================] - 0s 4ms/step - loss: 0.0010 - mean_absolute_error: 0.0217\n</pre> In\u00a0[18]: Copied! <pre>lstm_model = Sequential([\n    LSTM(32, return_sequences=True),\n    Dense(units=1)\n])\nhistory = compile_and_fit(lstm_model, wide_window)\nval_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\ntest_performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)\n</pre> lstm_model = Sequential([     LSTM(32, return_sequences=True),     Dense(units=1) ]) history = compile_and_fit(lstm_model, wide_window) val_performance['LSTM'] = lstm_model.evaluate(wide_window.val) test_performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0) <pre>Epoch 1/50\n384/384 [==============================] - 8s 16ms/step - loss: 0.0316 - mean_absolute_error: 0.1258 - val_loss: 0.0108 - val_mean_absolute_error: 0.0755\nEpoch 2/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0072 - mean_absolute_error: 0.0607 - val_loss: 0.0050 - val_mean_absolute_error: 0.0506\nEpoch 3/50\n384/384 [==============================] - 9s 24ms/step - loss: 0.0046 - mean_absolute_error: 0.0477 - val_loss: 0.0038 - val_mean_absolute_error: 0.0437\nEpoch 4/50\n384/384 [==============================] - 13s 34ms/step - loss: 0.0039 - mean_absolute_error: 0.0432 - val_loss: 0.0033 - val_mean_absolute_error: 0.0401\nEpoch 5/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0034 - mean_absolute_error: 0.0401 - val_loss: 0.0030 - val_mean_absolute_error: 0.0382\nEpoch 6/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0032 - mean_absolute_error: 0.0383 - val_loss: 0.0028 - val_mean_absolute_error: 0.0370\nEpoch 7/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0030 - mean_absolute_error: 0.0374 - val_loss: 0.0026 - val_mean_absolute_error: 0.0359\nEpoch 8/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0029 - mean_absolute_error: 0.0368 - val_loss: 0.0025 - val_mean_absolute_error: 0.0351\nEpoch 9/50\n384/384 [==============================] - 9s 23ms/step - loss: 0.0028 - mean_absolute_error: 0.0363 - val_loss: 0.0025 - val_mean_absolute_error: 0.0350\nEpoch 10/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0028 - mean_absolute_error: 0.0359 - val_loss: 0.0024 - val_mean_absolute_error: 0.0339\nEpoch 11/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0027 - mean_absolute_error: 0.0355 - val_loss: 0.0025 - val_mean_absolute_error: 0.0348\nEpoch 12/50\n384/384 [==============================] - 7s 17ms/step - loss: 0.0026 - mean_absolute_error: 0.0350 - val_loss: 0.0024 - val_mean_absolute_error: 0.0345\nEpoch 13/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0026 - mean_absolute_error: 0.0347 - val_loss: 0.0023 - val_mean_absolute_error: 0.0336\nEpoch 14/50\n384/384 [==============================] - 7s 17ms/step - loss: 0.0025 - mean_absolute_error: 0.0343 - val_loss: 0.0022 - val_mean_absolute_error: 0.0327\nEpoch 15/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0025 - mean_absolute_error: 0.0341 - val_loss: 0.0022 - val_mean_absolute_error: 0.0326\nEpoch 16/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0024 - mean_absolute_error: 0.0336 - val_loss: 0.0021 - val_mean_absolute_error: 0.0319\nEpoch 17/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0024 - mean_absolute_error: 0.0333 - val_loss: 0.0021 - val_mean_absolute_error: 0.0330\nEpoch 18/50\n384/384 [==============================] - 9s 24ms/step - loss: 0.0024 - mean_absolute_error: 0.0330 - val_loss: 0.0020 - val_mean_absolute_error: 0.0309\nEpoch 19/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0023 - mean_absolute_error: 0.0326 - val_loss: 0.0020 - val_mean_absolute_error: 0.0315\nEpoch 20/50\n384/384 [==============================] - 10s 25ms/step - loss: 0.0023 - mean_absolute_error: 0.0324 - val_loss: 0.0020 - val_mean_absolute_error: 0.0314\nEpoch 21/50\n384/384 [==============================] - 6s 17ms/step - loss: 0.0023 - mean_absolute_error: 0.0321 - val_loss: 0.0020 - val_mean_absolute_error: 0.0308\nEpoch 22/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0022 - mean_absolute_error: 0.0320 - val_loss: 0.0020 - val_mean_absolute_error: 0.0325\nEpoch 23/50\n384/384 [==============================] - 10s 25ms/step - loss: 0.0022 - mean_absolute_error: 0.0317 - val_loss: 0.0019 - val_mean_absolute_error: 0.0309\nEpoch 24/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0022 - mean_absolute_error: 0.0315 - val_loss: 0.0019 - val_mean_absolute_error: 0.0300\nEpoch 25/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0021 - mean_absolute_error: 0.0312 - val_loss: 0.0019 - val_mean_absolute_error: 0.0304\nEpoch 26/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0021 - mean_absolute_error: 0.0310 - val_loss: 0.0018 - val_mean_absolute_error: 0.0295\nEpoch 27/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0021 - mean_absolute_error: 0.0308 - val_loss: 0.0018 - val_mean_absolute_error: 0.0297\nEpoch 28/50\n384/384 [==============================] - 10s 25ms/step - loss: 0.0021 - mean_absolute_error: 0.0307 - val_loss: 0.0019 - val_mean_absolute_error: 0.0303\nEpoch 29/50\n384/384 [==============================] - 8s 22ms/step - loss: 0.0021 - mean_absolute_error: 0.0305 - val_loss: 0.0018 - val_mean_absolute_error: 0.0297\nEpoch 30/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0020 - mean_absolute_error: 0.0303 - val_loss: 0.0018 - val_mean_absolute_error: 0.0298\nEpoch 31/50\n384/384 [==============================] - 11s 29ms/step - loss: 0.0020 - mean_absolute_error: 0.0301 - val_loss: 0.0018 - val_mean_absolute_error: 0.0296\nEpoch 32/50\n384/384 [==============================] - 8s 19ms/step - loss: 0.0020 - mean_absolute_error: 0.0299 - val_loss: 0.0017 - val_mean_absolute_error: 0.0288\nEpoch 33/50\n384/384 [==============================] - 7s 17ms/step - loss: 0.0020 - mean_absolute_error: 0.0299 - val_loss: 0.0019 - val_mean_absolute_error: 0.0310\nEpoch 34/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0020 - mean_absolute_error: 0.0297 - val_loss: 0.0017 - val_mean_absolute_error: 0.0289\nEpoch 35/50\n384/384 [==============================] - 6s 17ms/step - loss: 0.0019 - mean_absolute_error: 0.0295 - val_loss: 0.0017 - val_mean_absolute_error: 0.0291\n109/109 [==============================] - 1s 7ms/step - loss: 0.0017 - mean_absolute_error: 0.0291\n</pre> In\u00a0[11]: Copied! <pre>ms_lstm_model = Sequential([\n    LSTM(32, return_sequences=True),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\nhistory = compile_and_fit(ms_lstm_model, multi_window)\nms_val_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.val)\nms_test_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.test)\n</pre> ms_lstm_model = Sequential([     LSTM(32, return_sequences=True),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) history = compile_and_fit(ms_lstm_model, multi_window) ms_val_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.val) ms_test_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.test) <pre>Epoch 1/50\n383/383 [==============================] - 9s 19ms/step - loss: 0.0504 - mean_absolute_error: 0.1617 - val_loss: 0.0206 - val_mean_absolute_error: 0.0996\nEpoch 2/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0169 - mean_absolute_error: 0.0894 - val_loss: 0.0181 - val_mean_absolute_error: 0.0922\nEpoch 3/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0153 - mean_absolute_error: 0.0837 - val_loss: 0.0170 - val_mean_absolute_error: 0.0870\nEpoch 4/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0147 - mean_absolute_error: 0.0811 - val_loss: 0.0165 - val_mean_absolute_error: 0.0851\nEpoch 5/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0143 - mean_absolute_error: 0.0798 - val_loss: 0.0161 - val_mean_absolute_error: 0.0831\nEpoch 6/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0140 - mean_absolute_error: 0.0787 - val_loss: 0.0157 - val_mean_absolute_error: 0.0826\nEpoch 7/50\n383/383 [==============================] - 11s 27ms/step - loss: 0.0137 - mean_absolute_error: 0.0778 - val_loss: 0.0155 - val_mean_absolute_error: 0.0811\nEpoch 8/50\n383/383 [==============================] - 10s 27ms/step - loss: 0.0135 - mean_absolute_error: 0.0771 - val_loss: 0.0152 - val_mean_absolute_error: 0.0830\nEpoch 9/50\n383/383 [==============================] - 10s 25ms/step - loss: 0.0133 - mean_absolute_error: 0.0764 - val_loss: 0.0151 - val_mean_absolute_error: 0.0817\nEpoch 10/50\n383/383 [==============================] - 11s 28ms/step - loss: 0.0132 - mean_absolute_error: 0.0758 - val_loss: 0.0150 - val_mean_absolute_error: 0.0815\nEpoch 11/50\n383/383 [==============================] - 10s 27ms/step - loss: 0.0131 - mean_absolute_error: 0.0756 - val_loss: 0.0148 - val_mean_absolute_error: 0.0818\nEpoch 12/50\n383/383 [==============================] - 10s 27ms/step - loss: 0.0130 - mean_absolute_error: 0.0752 - val_loss: 0.0147 - val_mean_absolute_error: 0.0785\nEpoch 13/50\n383/383 [==============================] - 10s 27ms/step - loss: 0.0129 - mean_absolute_error: 0.0746 - val_loss: 0.0148 - val_mean_absolute_error: 0.0784\nEpoch 14/50\n383/383 [==============================] - 10s 26ms/step - loss: 0.0128 - mean_absolute_error: 0.0743 - val_loss: 0.0146 - val_mean_absolute_error: 0.0808\nEpoch 15/50\n383/383 [==============================] - 11s 28ms/step - loss: 0.0128 - mean_absolute_error: 0.0741 - val_loss: 0.0144 - val_mean_absolute_error: 0.0781\nEpoch 16/50\n383/383 [==============================] - 9s 25ms/step - loss: 0.0127 - mean_absolute_error: 0.0740 - val_loss: 0.0145 - val_mean_absolute_error: 0.0787\nEpoch 17/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0126 - mean_absolute_error: 0.0734 - val_loss: 0.0143 - val_mean_absolute_error: 0.0786\nEpoch 18/50\n383/383 [==============================] - 11s 28ms/step - loss: 0.0124 - mean_absolute_error: 0.0728 - val_loss: 0.0141 - val_mean_absolute_error: 0.0782\nEpoch 19/50\n383/383 [==============================] - 11s 28ms/step - loss: 0.0124 - mean_absolute_error: 0.0725 - val_loss: 0.0141 - val_mean_absolute_error: 0.0765\nEpoch 20/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0123 - mean_absolute_error: 0.0720 - val_loss: 0.0139 - val_mean_absolute_error: 0.0787\nEpoch 21/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0121 - mean_absolute_error: 0.0715 - val_loss: 0.0141 - val_mean_absolute_error: 0.0775\nEpoch 22/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0120 - mean_absolute_error: 0.0712 - val_loss: 0.0139 - val_mean_absolute_error: 0.0770\nEpoch 23/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0120 - mean_absolute_error: 0.0711 - val_loss: 0.0138 - val_mean_absolute_error: 0.0777\nEpoch 24/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0119 - mean_absolute_error: 0.0707 - val_loss: 0.0135 - val_mean_absolute_error: 0.0753\nEpoch 25/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0118 - mean_absolute_error: 0.0704 - val_loss: 0.0134 - val_mean_absolute_error: 0.0751\nEpoch 26/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0117 - mean_absolute_error: 0.0698 - val_loss: 0.0132 - val_mean_absolute_error: 0.0752\nEpoch 27/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0116 - mean_absolute_error: 0.0695 - val_loss: 0.0132 - val_mean_absolute_error: 0.0710\nEpoch 28/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0114 - mean_absolute_error: 0.0690 - val_loss: 0.0127 - val_mean_absolute_error: 0.0718\nEpoch 29/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0114 - mean_absolute_error: 0.0689 - val_loss: 0.0127 - val_mean_absolute_error: 0.0716\nEpoch 30/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0111 - mean_absolute_error: 0.0679 - val_loss: 0.0126 - val_mean_absolute_error: 0.0717\nEpoch 31/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0109 - mean_absolute_error: 0.0674 - val_loss: 0.0122 - val_mean_absolute_error: 0.0704\nEpoch 32/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0109 - mean_absolute_error: 0.0670 - val_loss: 0.0123 - val_mean_absolute_error: 0.0686\nEpoch 33/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0108 - mean_absolute_error: 0.0667 - val_loss: 0.0122 - val_mean_absolute_error: 0.0714\nEpoch 34/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0106 - mean_absolute_error: 0.0660 - val_loss: 0.0121 - val_mean_absolute_error: 0.0686\nEpoch 35/50\n383/383 [==============================] - 8s 19ms/step - loss: 0.0105 - mean_absolute_error: 0.0658 - val_loss: 0.0120 - val_mean_absolute_error: 0.0690\nEpoch 36/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0105 - mean_absolute_error: 0.0655 - val_loss: 0.0117 - val_mean_absolute_error: 0.0688\nEpoch 37/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0104 - mean_absolute_error: 0.0655 - val_loss: 0.0117 - val_mean_absolute_error: 0.0692\nEpoch 38/50\n383/383 [==============================] - 8s 21ms/step - loss: 0.0104 - mean_absolute_error: 0.0651 - val_loss: 0.0118 - val_mean_absolute_error: 0.0663\nEpoch 39/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0103 - mean_absolute_error: 0.0646 - val_loss: 0.0117 - val_mean_absolute_error: 0.0670\nEpoch 40/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0103 - mean_absolute_error: 0.0646 - val_loss: 0.0124 - val_mean_absolute_error: 0.0699\nEpoch 41/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0102 - mean_absolute_error: 0.0644 - val_loss: 0.0115 - val_mean_absolute_error: 0.0688\nEpoch 42/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0102 - mean_absolute_error: 0.0645 - val_loss: 0.0119 - val_mean_absolute_error: 0.0708\nEpoch 43/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0101 - mean_absolute_error: 0.0640 - val_loss: 0.0115 - val_mean_absolute_error: 0.0683\nEpoch 44/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0101 - mean_absolute_error: 0.0638 - val_loss: 0.0115 - val_mean_absolute_error: 0.0682\nEpoch 45/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0101 - mean_absolute_error: 0.0636 - val_loss: 0.0116 - val_mean_absolute_error: 0.0710\nEpoch 46/50\n383/383 [==============================] - 7s 19ms/step - loss: 0.0100 - mean_absolute_error: 0.0635 - val_loss: 0.0115 - val_mean_absolute_error: 0.0671\nEpoch 47/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0100 - mean_absolute_error: 0.0634 - val_loss: 0.0115 - val_mean_absolute_error: 0.0689\n109/109 [==============================] - 1s 8ms/step - loss: 0.0115 - mean_absolute_error: 0.0689\n54/54 [==============================] - 0s 7ms/step - loss: 0.0082 - mean_absolute_error: 0.0551\n</pre> In\u00a0[12]: Copied! <pre>mo_lstm_model = Sequential([\n    LSTM(32, return_sequences=True),\n    Dense(units=2)\n])\nhistory = compile_and_fit(mo_lstm_model, mo_wide_window)\nmo_val_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.val)\nmo_test_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.test)\n</pre> mo_lstm_model = Sequential([     LSTM(32, return_sequences=True),     Dense(units=2) ]) history = compile_and_fit(mo_lstm_model, mo_wide_window) mo_val_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.val) mo_test_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.test) <pre>Epoch 1/50\n384/384 [==============================] - 9s 20ms/step - loss: 0.0354 - mean_absolute_error: 0.1212 - val_loss: 0.0121 - val_mean_absolute_error: 0.0698\nEpoch 2/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0073 - mean_absolute_error: 0.0555 - val_loss: 0.0051 - val_mean_absolute_error: 0.0450\nEpoch 3/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0041 - mean_absolute_error: 0.0409 - val_loss: 0.0033 - val_mean_absolute_error: 0.0373\nEpoch 4/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0031 - mean_absolute_error: 0.0354 - val_loss: 0.0026 - val_mean_absolute_error: 0.0328\nEpoch 5/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0027 - mean_absolute_error: 0.0327 - val_loss: 0.0022 - val_mean_absolute_error: 0.0303\nEpoch 6/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0025 - mean_absolute_error: 0.0308 - val_loss: 0.0020 - val_mean_absolute_error: 0.0289\nEpoch 7/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0023 - mean_absolute_error: 0.0295 - val_loss: 0.0019 - val_mean_absolute_error: 0.0279\nEpoch 8/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0021 - mean_absolute_error: 0.0283 - val_loss: 0.0018 - val_mean_absolute_error: 0.0270\nEpoch 9/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0020 - mean_absolute_error: 0.0273 - val_loss: 0.0017 - val_mean_absolute_error: 0.0261\nEpoch 10/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0019 - mean_absolute_error: 0.0266 - val_loss: 0.0016 - val_mean_absolute_error: 0.0257\nEpoch 11/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0018 - mean_absolute_error: 0.0258 - val_loss: 0.0015 - val_mean_absolute_error: 0.0245\nEpoch 12/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0018 - mean_absolute_error: 0.0251 - val_loss: 0.0015 - val_mean_absolute_error: 0.0243\nEpoch 13/50\n384/384 [==============================] - 9s 24ms/step - loss: 0.0017 - mean_absolute_error: 0.0246 - val_loss: 0.0014 - val_mean_absolute_error: 0.0240\nEpoch 14/50\n384/384 [==============================] - 8s 22ms/step - loss: 0.0017 - mean_absolute_error: 0.0241 - val_loss: 0.0014 - val_mean_absolute_error: 0.0237\nEpoch 15/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0016 - mean_absolute_error: 0.0238 - val_loss: 0.0014 - val_mean_absolute_error: 0.0236\nEpoch 16/50\n384/384 [==============================] - 9s 23ms/step - loss: 0.0016 - mean_absolute_error: 0.0236 - val_loss: 0.0013 - val_mean_absolute_error: 0.0228\nEpoch 17/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0016 - mean_absolute_error: 0.0234 - val_loss: 0.0013 - val_mean_absolute_error: 0.0226\nEpoch 18/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0016 - mean_absolute_error: 0.0231 - val_loss: 0.0013 - val_mean_absolute_error: 0.0229\nEpoch 19/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0229 - val_loss: 0.0013 - val_mean_absolute_error: 0.0226\nEpoch 20/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0228 - val_loss: 0.0012 - val_mean_absolute_error: 0.0219\nEpoch 21/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0227 - val_loss: 0.0012 - val_mean_absolute_error: 0.0216\nEpoch 22/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0226 - val_loss: 0.0012 - val_mean_absolute_error: 0.0219\nEpoch 23/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0224 - val_loss: 0.0012 - val_mean_absolute_error: 0.0221\nEpoch 24/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0014 - mean_absolute_error: 0.0224 - val_loss: 0.0012 - val_mean_absolute_error: 0.0217\nEpoch 25/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0014 - mean_absolute_error: 0.0222 - val_loss: 0.0011 - val_mean_absolute_error: 0.0209\nEpoch 26/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0014 - mean_absolute_error: 0.0220 - val_loss: 0.0011 - val_mean_absolute_error: 0.0211\nEpoch 27/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0014 - mean_absolute_error: 0.0219 - val_loss: 0.0012 - val_mean_absolute_error: 0.0212\nEpoch 28/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0014 - mean_absolute_error: 0.0217 - val_loss: 0.0012 - val_mean_absolute_error: 0.0220\n109/109 [==============================] - 1s 8ms/step - loss: 0.0012 - mean_absolute_error: 0.0220\n55/55 [==============================] - 0s 8ms/step - loss: 9.0453e-04 - mean_absolute_error: 0.0194\n</pre> In\u00a0[19]: Copied! <pre># single step\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Single Step')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(val_performance.keys(), [v[1] for v in val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(test_performance.keys(), [v[1] for v in test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.legend()\nplt.tight_layout()\n</pre> # single step\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Single Step') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(val_performance.keys(), [v[1] for v in val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(test_performance.keys(), [v[1] for v in test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.legend() plt.tight_layout() In\u00a0[14]: Copied! <pre># multi step\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Multi Step')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(ms_val_performance.keys(), [v[1] for v in ms_val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(ms_test_performance.keys(), [v[1] for v in ms_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.legend()\nplt.tight_layout()\n</pre> # multi step\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Multi Step') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(ms_val_performance.keys(), [v[1] for v in ms_val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(ms_test_performance.keys(), [v[1] for v in ms_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.legend() plt.tight_layout() In\u00a0[15]: Copied! <pre># multi output\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Multi Output')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(mo_val_performance.keys(), [v[1] for v in mo_val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(mo_test_performance.keys(), [v[1] for v in mo_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.legend()\nplt.tight_layout()\n</pre> # multi output\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Multi Output') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(mo_val_performance.keys(), [v[1] for v in mo_val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(mo_test_performance.keys(), [v[1] for v in mo_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.legend() plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter15.html#rnn","title":"\u30ea\u30ab\u30ec\u30f3\u30c8\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(RNN)\u3092\u63a2\u7d22\u3059\u308b\u00b6","text":""},{"location":"chapter15.html#lstm","title":"LSTM\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u8abf\u3079\u308b\u00b6","text":""},{"location":"chapter15.html#lstm","title":"LSTM\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u5b9f\u88c5\u3059\u308b\u00b6","text":""},{"location":"chapter16.html","title":"\u7b2c16\u7ae0 CNN\u3092\u4f7f\u3063\u305f\u6642\u7cfb\u5217\u306e\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0","text":"In\u00a0[8]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.metrics import MeanAbsoluteError\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Conv1D\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.losses import MeanSquaredError from tensorflow.keras.metrics import MeanAbsoluteError from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.optimizers import Adam from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense, LSTM, Conv1D In\u00a0[2]: Copied! <pre>url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv'\nurl_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv'\nurl_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'\n\ndf_train = pd.read_csv(url_train, index_col=0)\ndf_val = pd.read_csv(url_val, index_col=0)\ndf_test = pd.read_csv(url_test, index_col=0)\n</pre> url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv' url_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv' url_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'  df_train = pd.read_csv(url_train, index_col=0) df_val = pd.read_csv(url_val, index_col=0) df_test = pd.read_csv(url_test, index_col=0) In\u00a0[3]: Copied! <pre>class DataWindow:\n    def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):\n        # window size\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n        self.total_window_size = input_width + shift\n        \n        # \u30c7\u30fc\u30bf\n        self.df_train = df_train\n        self.df_val = df_val\n        self.df_test = df_test\n        \n        # \u30e9\u30d9\u30eb\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}\n        \n        # \u30b9\u30e9\u30a4\u30b9\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n        \n        # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e\n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n    \n    def split_to_inputs_labels(self, features):\n        inputs = features[:, self.input_slice, :]\n        labels = features[:, self.labels_slice, :]\n        if self.label_columns is not None:\n            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n        inputs.set_shape([None, self.input_width, None])\n        labels.set_shape([None, self.label_width, None])\n        return inputs, labels\n    \n    def plot(self, plot_col: str, model=None, max_subplots=3):\n        inputs, labels = self.sample_batch\n        plt.figure(figsize=(12, 8))\n        plot_col_index = self.column_indices[plot_col]\n        n_max = min(max_subplots, len(inputs))\n        \n        for n in range(n_max):\n            plt.subplot(n_max, 1, n+1)\n            plt.ylabel(f'{plot_col} [scaled]')\n            plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)\n            \n            if self.label_columns:\n                label_col_index = self.label_columns_indices.get(plot_col, None)\n            else:\n                label_col_index = plot_col_index\n            \n            if label_col_index is None:\n                continue\n            \n            plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)\n            \n            if model is not None:\n                predictions = model(inputs)\n                plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)\n            \n            if n == 0:\n                plt.legend()\n        plt.xlabel('Time (h)')\n    \n    def make_dataset(self, data):\n        data = np.array(data, dtype=np.float32)\n        ds = tf.keras.utils.timeseries_dataset_from_array(\n            data=data,\n            targets=None,\n            sequence_length=self.total_window_size,\n            sequence_stride=1,\n            shuffle=True,\n            batch_size=32,\n        )\n        ds = ds.map(self.split_to_inputs_labels)\n        return ds\n    \n    @property\n    def train(self):\n        return self.make_dataset(self.df_train)\n    \n    @property\n    def val(self):\n        return self.make_dataset(self.df_val)\n    \n    @property\n    def test(self):\n        return self.make_dataset(self.df_test)\n    \n    @property\n    def sample_batch(self):\n        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n        result = getattr(self, '_sample_batch', None)\n        if result is None:\n            result = next(iter(self.train))\n            self._sample_batch = result\n        return result\n</pre> class DataWindow:     def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):         # window size         self.input_width = input_width         self.label_width = label_width         self.shift = shift         self.total_window_size = input_width + shift                  # \u30c7\u30fc\u30bf         self.df_train = df_train         self.df_val = df_val         self.df_test = df_test                  # \u30e9\u30d9\u30eb         self.label_columns = label_columns         if label_columns is not None:             self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}         self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}                  # \u30b9\u30e9\u30a4\u30b9         self.input_slice = slice(0, input_width)         self.input_indices = np.arange(self.total_window_size)[self.input_slice]                  # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e         self.label_start = self.total_window_size - self.label_width         self.labels_slice = slice(self.label_start, None)         self.label_indices = np.arange(self.total_window_size)[self.labels_slice]          def split_to_inputs_labels(self, features):         inputs = features[:, self.input_slice, :]         labels = features[:, self.labels_slice, :]         if self.label_columns is not None:             labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)         inputs.set_shape([None, self.input_width, None])         labels.set_shape([None, self.label_width, None])         return inputs, labels          def plot(self, plot_col: str, model=None, max_subplots=3):         inputs, labels = self.sample_batch         plt.figure(figsize=(12, 8))         plot_col_index = self.column_indices[plot_col]         n_max = min(max_subplots, len(inputs))                  for n in range(n_max):             plt.subplot(n_max, 1, n+1)             plt.ylabel(f'{plot_col} [scaled]')             plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)                          if self.label_columns:                 label_col_index = self.label_columns_indices.get(plot_col, None)             else:                 label_col_index = plot_col_index                          if label_col_index is None:                 continue                          plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)                          if model is not None:                 predictions = model(inputs)                 plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)                          if n == 0:                 plt.legend()         plt.xlabel('Time (h)')          def make_dataset(self, data):         data = np.array(data, dtype=np.float32)         ds = tf.keras.utils.timeseries_dataset_from_array(             data=data,             targets=None,             sequence_length=self.total_window_size,             sequence_stride=1,             shuffle=True,             batch_size=32,         )         ds = ds.map(self.split_to_inputs_labels)         return ds          @property     def train(self):         return self.make_dataset(self.df_train)          @property     def val(self):         return self.make_dataset(self.df_val)          @property     def test(self):         return self.make_dataset(self.df_test)          @property     def sample_batch(self):         \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"         result = getattr(self, '_sample_batch', None)         if result is None:             result = next(iter(self.train))             self._sample_batch = result         return result In\u00a0[4]: Copied! <pre># for training\ndef compile_and_fit(model, window, patience=3, max_epochs=50):\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=patience,\n        mode='min'\n    )\n    model.compile(\n        loss=MeanSquaredError(),\n        optimizer=Adam(),\n        metrics=[MeanAbsoluteError()]\n    )\n    history = model.fit(\n        window.train,\n        epochs=max_epochs,\n        validation_data=window.val,\n        callbacks=[early_stopping]\n    )\n    return history\n</pre> # for training def compile_and_fit(model, window, patience=3, max_epochs=50):     early_stopping = EarlyStopping(         monitor='val_loss',         patience=patience,         mode='min'     )     model.compile(         loss=MeanSquaredError(),         optimizer=Adam(),         metrics=[MeanAbsoluteError()]     )     history = model.fit(         window.train,         epochs=max_epochs,         validation_data=window.val,         callbacks=[early_stopping]     )     return history In\u00a0[6]: Copied! <pre># models\nclass Baseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n        \n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        elif isinstance(self.label_index, list):\n            tensors = []\n            for index in self.label_index:\n                res = inputs[:, :, index]\n                res = res[:, :, tf.newaxis]\n                tensors.append(res)\n            return tf.concat(tensors, axis=-1)\n        else:\n            res = inputs[:, :, self.label_index]\n            return res[:, :, tf.newaxis]\n\nclass MultiStepLastBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        if self.label_index is None:\n            return tf.tile(inputs[:, -1:, :], [1, 24, 1])\n        return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])\n    \nclass RepeatBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        return inputs[:, :, self.label_index:]\n\n# \u7dda\u5f62\u30e2\u30c7\u30eb\nlinear = Sequential([Dense(units=1)])\nms_linear = Sequential([Dense(units=1, kernel_initializer=tf.initializers.zeros)])\nmo_linear = Sequential([Dense(units=2)])\n# DNN\ndense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=1)\n])\nms_dense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\nmo_dense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=2)\n])\n# LSTM\nlstm_model = Sequential([\n    LSTM(32, return_sequences=True),\n    Dense(units=1)\n])\nms_lstm_model = Sequential([\n    LSTM(32, return_sequences=True),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\nmo_lstm_model = Sequential([\n    LSTM(32, return_sequences=True),\n    Dense(units=2)\n])\n</pre> # models class Baseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index              def call(self, inputs):         if self.label_index is None:             return inputs         elif isinstance(self.label_index, list):             tensors = []             for index in self.label_index:                 res = inputs[:, :, index]                 res = res[:, :, tf.newaxis]                 tensors.append(res)             return tf.concat(tensors, axis=-1)         else:             res = inputs[:, :, self.label_index]             return res[:, :, tf.newaxis]  class MultiStepLastBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         if self.label_index is None:             return tf.tile(inputs[:, -1:, :], [1, 24, 1])         return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])      class RepeatBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         return inputs[:, :, self.label_index:]  # \u7dda\u5f62\u30e2\u30c7\u30eb linear = Sequential([Dense(units=1)]) ms_linear = Sequential([Dense(units=1, kernel_initializer=tf.initializers.zeros)]) mo_linear = Sequential([Dense(units=2)]) # DNN dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=1) ]) ms_dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) mo_dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=2) ]) # LSTM lstm_model = Sequential([     LSTM(32, return_sequences=True),     Dense(units=1) ]) ms_lstm_model = Sequential([     LSTM(32, return_sequences=True),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) mo_lstm_model = Sequential([     LSTM(32, return_sequences=True),     Dense(units=2) ]) In\u00a0[7]: Copied! <pre># \u30b7\u30f3\u30b0\u30eb\u30b9\u30c6\u30c3\u30d7\nsingle_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\nwide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\ncolumn_indices = {name: i for i, name in enumerate(df_train.columns)}\nbaseline_last = Baseline(column_indices['traffic_volume'])\nbaseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nval_performance = {}\ntest_performance = {}\n\n# \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7\nmulti_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\nms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\nms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nms_val_performance = {}\nms_test_performance = {}\n\n# \u591a\u5909\u6570\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\ncol_names = ['temp', 'traffic_volume']\nmo_single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)\nmo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)\nmo_baseline_last = Baseline(label_index=[column_indices[col] for col in col_names])\nmo_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nmo_val_performance = {}\nmo_test_performance = {}\n\n## \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\nval_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.val)\ntest_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.test, verbose=0)\nms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val)\nms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test)\nms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val)\nms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test)\nmo_val_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)\nmo_test_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)\n\n## \u7dda\u5f62\u30e2\u30c7\u30eb\nhistory = compile_and_fit(linear, single_step_window)\nval_performance['Linear'] = linear.evaluate(single_step_window.val)\ntest_performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)\nhistory = compile_and_fit(ms_linear, multi_window)\nms_val_performance['Linear'] = ms_linear.evaluate(multi_window.val)\nms_test_performance['Linear'] = ms_linear.evaluate(multi_window.test)\nhistory = compile_and_fit(mo_linear, mo_single_step_window)\nmo_val_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.val)\nmo_test_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.test)\n\n## DNN\nhistory = compile_and_fit(dense, single_step_window)\nval_performance['Dense'] = dense.evaluate(single_step_window.val)\ntest_performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)\nhistory = compile_and_fit(ms_dense, multi_window)\nms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val)\nms_test_performance['Dense'] = ms_dense.evaluate(multi_window.test)\nhistory = compile_and_fit(mo_dense, mo_single_step_window)\nmo_val_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.val)\nmo_test_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.test)\n\n## LSTM\nhistory = compile_and_fit(lstm_model, wide_window)\nval_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\ntest_performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)\nhistory = compile_and_fit(ms_lstm_model, multi_window)\nms_val_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.val)\nms_test_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.test)\nhistory = compile_and_fit(mo_lstm_model, mo_wide_window)\nmo_val_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.val)\nmo_test_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.test)\n</pre> # \u30b7\u30f3\u30b0\u30eb\u30b9\u30c6\u30c3\u30d7 single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) column_indices = {name: i for i, name in enumerate(df_train.columns)} baseline_last = Baseline(column_indices['traffic_volume']) baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  val_performance = {} test_performance = {}  # \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7 multi_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) ms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume']) ms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()]) ms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume']) ms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  ms_val_performance = {} ms_test_performance = {}  # \u591a\u5909\u6570\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8 col_names = ['temp', 'traffic_volume'] mo_single_step_window = DataWindow(input_width=1, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names) mo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names) mo_baseline_last = Baseline(label_index=[column_indices[col] for col in col_names]) mo_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  mo_val_performance = {} mo_test_performance = {}  ## \u30d9\u30fc\u30b9\u30e9\u30a4\u30f3 val_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.val) test_performance['Baseline - Last'] = baseline_last.evaluate(single_step_window.test, verbose=0) ms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val) ms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test) ms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val) ms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test) mo_val_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val) mo_test_performance['Baseline - Last'] = mo_baseline_last.evaluate(mo_single_step_window.val)  ## \u7dda\u5f62\u30e2\u30c7\u30eb history = compile_and_fit(linear, single_step_window) val_performance['Linear'] = linear.evaluate(single_step_window.val) test_performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0) history = compile_and_fit(ms_linear, multi_window) ms_val_performance['Linear'] = ms_linear.evaluate(multi_window.val) ms_test_performance['Linear'] = ms_linear.evaluate(multi_window.test) history = compile_and_fit(mo_linear, mo_single_step_window) mo_val_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.val) mo_test_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.test)  ## DNN history = compile_and_fit(dense, single_step_window) val_performance['Dense'] = dense.evaluate(single_step_window.val) test_performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0) history = compile_and_fit(ms_dense, multi_window) ms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val) ms_test_performance['Dense'] = ms_dense.evaluate(multi_window.test) history = compile_and_fit(mo_dense, mo_single_step_window) mo_val_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.val) mo_test_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.test)  ## LSTM history = compile_and_fit(lstm_model, wide_window) val_performance['LSTM'] = lstm_model.evaluate(wide_window.val) test_performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0) history = compile_and_fit(ms_lstm_model, multi_window) ms_val_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.val) ms_test_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.test) history = compile_and_fit(mo_lstm_model, mo_wide_window) mo_val_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.val) mo_test_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.test) <pre>110/110 [==============================] - 0s 3ms/step - loss: 0.0133 - mean_absolute_error: 0.0831\n109/109 [==============================] - 0s 2ms/step - loss: 0.1875 - mean_absolute_error: 0.3522\n54/54 [==============================] - 0s 2ms/step - loss: 0.1814 - mean_absolute_error: 0.3473\n109/109 [==============================] - 0s 2ms/step - loss: 0.2065 - mean_absolute_error: 0.3473\n54/54 [==============================] - 0s 2ms/step - loss: 0.2018 - mean_absolute_error: 0.3413\n110/110 [==============================] - 0s 2ms/step - loss: 0.0069 - mean_absolute_error: 0.0482\n110/110 [==============================] - 0s 2ms/step - loss: 0.0069 - mean_absolute_error: 0.0482\nEpoch 1/50\n384/384 [==============================] - 1s 3ms/step - loss: 0.2975 - mean_absolute_error: 0.4443 - val_loss: 0.0746 - val_mean_absolute_error: 0.2244\nEpoch 2/50\n384/384 [==============================] - 1s 3ms/step - loss: 0.0617 - mean_absolute_error: 0.2041 - val_loss: 0.0539 - val_mean_absolute_error: 0.1910\nEpoch 3/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0412 - mean_absolute_error: 0.1653 - val_loss: 0.0386 - val_mean_absolute_error: 0.1619\nEpoch 4/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0304 - mean_absolute_error: 0.1407 - val_loss: 0.0300 - val_mean_absolute_error: 0.1417\nEpoch 5/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0243 - mean_absolute_error: 0.1244 - val_loss: 0.0242 - val_mean_absolute_error: 0.1260\nEpoch 6/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0200 - mean_absolute_error: 0.1115 - val_loss: 0.0199 - val_mean_absolute_error: 0.1131\nEpoch 7/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0166 - mean_absolute_error: 0.1000 - val_loss: 0.0163 - val_mean_absolute_error: 0.1011\nEpoch 8/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0140 - mean_absolute_error: 0.0902 - val_loss: 0.0137 - val_mean_absolute_error: 0.0907\nEpoch 9/50\n384/384 [==============================] - 3s 8ms/step - loss: 0.0121 - mean_absolute_error: 0.0822 - val_loss: 0.0119 - val_mean_absolute_error: 0.0831\nEpoch 10/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0109 - mean_absolute_error: 0.0767 - val_loss: 0.0107 - val_mean_absolute_error: 0.0768\nEpoch 11/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0102 - mean_absolute_error: 0.0732 - val_loss: 0.0100 - val_mean_absolute_error: 0.0722\nEpoch 12/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0098 - mean_absolute_error: 0.0712 - val_loss: 0.0096 - val_mean_absolute_error: 0.0694\nEpoch 13/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0095 - mean_absolute_error: 0.0700 - val_loss: 0.0094 - val_mean_absolute_error: 0.0679\nEpoch 14/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0094 - mean_absolute_error: 0.0692 - val_loss: 0.0092 - val_mean_absolute_error: 0.0666\nEpoch 15/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0094 - mean_absolute_error: 0.0687 - val_loss: 0.0092 - val_mean_absolute_error: 0.0665\nEpoch 16/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0684 - val_loss: 0.0091 - val_mean_absolute_error: 0.0653\nEpoch 17/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0682 - val_loss: 0.0091 - val_mean_absolute_error: 0.0654\nEpoch 18/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0680 - val_loss: 0.0091 - val_mean_absolute_error: 0.0661\nEpoch 19/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0680 - val_loss: 0.0091 - val_mean_absolute_error: 0.0655\nEpoch 20/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0678 - val_loss: 0.0091 - val_mean_absolute_error: 0.0649\nEpoch 21/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0091 - val_mean_absolute_error: 0.0648\nEpoch 22/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0091 - val_mean_absolute_error: 0.0659\nEpoch 23/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0090 - val_mean_absolute_error: 0.0656\nEpoch 24/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0677 - val_loss: 0.0091 - val_mean_absolute_error: 0.0662\nEpoch 25/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0676 - val_loss: 0.0091 - val_mean_absolute_error: 0.0661\nEpoch 26/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0678 - val_loss: 0.0091 - val_mean_absolute_error: 0.0651\n110/110 [==============================] - 0s 3ms/step - loss: 0.0091 - mean_absolute_error: 0.0651\nEpoch 1/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0898 - mean_absolute_error: 0.2464 - val_loss: 0.0458 - val_mean_absolute_error: 0.1822\nEpoch 2/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0298 - mean_absolute_error: 0.1368 - val_loss: 0.0261 - val_mean_absolute_error: 0.1254\nEpoch 3/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0201 - mean_absolute_error: 0.1044 - val_loss: 0.0212 - val_mean_absolute_error: 0.1074\nEpoch 4/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0177 - mean_absolute_error: 0.0954 - val_loss: 0.0196 - val_mean_absolute_error: 0.1001\nEpoch 5/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0168 - mean_absolute_error: 0.0914 - val_loss: 0.0188 - val_mean_absolute_error: 0.0952\nEpoch 6/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0164 - mean_absolute_error: 0.0887 - val_loss: 0.0184 - val_mean_absolute_error: 0.0922\nEpoch 7/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0162 - mean_absolute_error: 0.0868 - val_loss: 0.0183 - val_mean_absolute_error: 0.0902\nEpoch 8/50\n383/383 [==============================] - 3s 8ms/step - loss: 0.0161 - mean_absolute_error: 0.0857 - val_loss: 0.0182 - val_mean_absolute_error: 0.0892\nEpoch 9/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0161 - mean_absolute_error: 0.0850 - val_loss: 0.0182 - val_mean_absolute_error: 0.0886\nEpoch 10/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0846 - val_loss: 0.0182 - val_mean_absolute_error: 0.0882\nEpoch 11/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0161 - mean_absolute_error: 0.0843 - val_loss: 0.0182 - val_mean_absolute_error: 0.0881\nEpoch 12/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0841 - val_loss: 0.0182 - val_mean_absolute_error: 0.0882\nEpoch 13/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0161 - mean_absolute_error: 0.0840 - val_loss: 0.0182 - val_mean_absolute_error: 0.0879\nEpoch 14/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0161 - mean_absolute_error: 0.0839 - val_loss: 0.0182 - val_mean_absolute_error: 0.0879\nEpoch 15/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0161 - mean_absolute_error: 0.0838 - val_loss: 0.0182 - val_mean_absolute_error: 0.0879\n109/109 [==============================] - 0s 3ms/step - loss: 0.0182 - mean_absolute_error: 0.0879\n54/54 [==============================] - 0s 3ms/step - loss: 0.0142 - mean_absolute_error: 0.0753\nEpoch 1/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.2482 - mean_absolute_error: 0.3754 - val_loss: 0.0894 - val_mean_absolute_error: 0.2307\nEpoch 2/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0494 - mean_absolute_error: 0.1644 - val_loss: 0.0418 - val_mean_absolute_error: 0.1526\nEpoch 3/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0359 - mean_absolute_error: 0.1355 - val_loss: 0.0332 - val_mean_absolute_error: 0.1363\nEpoch 4/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0292 - mean_absolute_error: 0.1238 - val_loss: 0.0271 - val_mean_absolute_error: 0.1251\nEpoch 5/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0239 - mean_absolute_error: 0.1140 - val_loss: 0.0222 - val_mean_absolute_error: 0.1156\nEpoch 6/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0194 - mean_absolute_error: 0.1045 - val_loss: 0.0179 - val_mean_absolute_error: 0.1047\nEpoch 7/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0157 - mean_absolute_error: 0.0950 - val_loss: 0.0143 - val_mean_absolute_error: 0.0954\nEpoch 8/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0126 - mean_absolute_error: 0.0857 - val_loss: 0.0112 - val_mean_absolute_error: 0.0842\nEpoch 9/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0101 - mean_absolute_error: 0.0765 - val_loss: 0.0089 - val_mean_absolute_error: 0.0749\nEpoch 10/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0081 - mean_absolute_error: 0.0680 - val_loss: 0.0071 - val_mean_absolute_error: 0.0652\nEpoch 11/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0068 - mean_absolute_error: 0.0602 - val_loss: 0.0060 - val_mean_absolute_error: 0.0581\nEpoch 12/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0059 - mean_absolute_error: 0.0535 - val_loss: 0.0053 - val_mean_absolute_error: 0.0512\nEpoch 13/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0054 - mean_absolute_error: 0.0483 - val_loss: 0.0049 - val_mean_absolute_error: 0.0461\nEpoch 14/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0051 - mean_absolute_error: 0.0446 - val_loss: 0.0048 - val_mean_absolute_error: 0.0430\nEpoch 15/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0050 - mean_absolute_error: 0.0422 - val_loss: 0.0047 - val_mean_absolute_error: 0.0409\nEpoch 16/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0050 - mean_absolute_error: 0.0408 - val_loss: 0.0047 - val_mean_absolute_error: 0.0398\nEpoch 17/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0402 - val_loss: 0.0047 - val_mean_absolute_error: 0.0398\nEpoch 18/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0398 - val_loss: 0.0047 - val_mean_absolute_error: 0.0391\nEpoch 19/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0396 - val_loss: 0.0046 - val_mean_absolute_error: 0.0385\nEpoch 20/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0046 - val_mean_absolute_error: 0.0386\nEpoch 21/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0047 - val_mean_absolute_error: 0.0389\nEpoch 22/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0049 - mean_absolute_error: 0.0395 - val_loss: 0.0047 - val_mean_absolute_error: 0.0402\n110/110 [==============================] - 0s 3ms/step - loss: 0.0047 - mean_absolute_error: 0.0402\n55/55 [==============================] - 0s 3ms/step - loss: 0.0044 - mean_absolute_error: 0.0384\nEpoch 1/50\n384/384 [==============================] - 3s 6ms/step - loss: 0.0198 - mean_absolute_error: 0.0841 - val_loss: 0.0054 - val_mean_absolute_error: 0.0540\nEpoch 2/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0051 - mean_absolute_error: 0.0518 - val_loss: 0.0041 - val_mean_absolute_error: 0.0472\nEpoch 3/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0043 - mean_absolute_error: 0.0478 - val_loss: 0.0035 - val_mean_absolute_error: 0.0434\nEpoch 4/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0038 - mean_absolute_error: 0.0450 - val_loss: 0.0032 - val_mean_absolute_error: 0.0410\nEpoch 5/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0036 - mean_absolute_error: 0.0434 - val_loss: 0.0035 - val_mean_absolute_error: 0.0452\nEpoch 6/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0034 - mean_absolute_error: 0.0423 - val_loss: 0.0027 - val_mean_absolute_error: 0.0380\nEpoch 7/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0032 - mean_absolute_error: 0.0411 - val_loss: 0.0034 - val_mean_absolute_error: 0.0463\nEpoch 8/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0031 - mean_absolute_error: 0.0402 - val_loss: 0.0026 - val_mean_absolute_error: 0.0371\nEpoch 9/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0029 - mean_absolute_error: 0.0390 - val_loss: 0.0026 - val_mean_absolute_error: 0.0376\nEpoch 10/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0029 - mean_absolute_error: 0.0386 - val_loss: 0.0024 - val_mean_absolute_error: 0.0364\nEpoch 11/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0028 - mean_absolute_error: 0.0376 - val_loss: 0.0030 - val_mean_absolute_error: 0.0426\nEpoch 12/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0026 - mean_absolute_error: 0.0363 - val_loss: 0.0021 - val_mean_absolute_error: 0.0331\nEpoch 13/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0026 - mean_absolute_error: 0.0361 - val_loss: 0.0022 - val_mean_absolute_error: 0.0338\nEpoch 14/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0026 - mean_absolute_error: 0.0358 - val_loss: 0.0024 - val_mean_absolute_error: 0.0350\nEpoch 15/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0025 - mean_absolute_error: 0.0349 - val_loss: 0.0023 - val_mean_absolute_error: 0.0339\n110/110 [==============================] - 0s 3ms/step - loss: 0.0023 - mean_absolute_error: 0.0339\nEpoch 1/50\n383/383 [==============================] - 3s 5ms/step - loss: 0.0308 - mean_absolute_error: 0.1119 - val_loss: 0.0166 - val_mean_absolute_error: 0.0857\nEpoch 2/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0140 - mean_absolute_error: 0.0785 - val_loss: 0.0156 - val_mean_absolute_error: 0.0842\nEpoch 3/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0135 - mean_absolute_error: 0.0777 - val_loss: 0.0152 - val_mean_absolute_error: 0.0805\nEpoch 4/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0133 - mean_absolute_error: 0.0769 - val_loss: 0.0151 - val_mean_absolute_error: 0.0809\nEpoch 5/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0131 - mean_absolute_error: 0.0765 - val_loss: 0.0148 - val_mean_absolute_error: 0.0808\nEpoch 6/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0130 - mean_absolute_error: 0.0762 - val_loss: 0.0150 - val_mean_absolute_error: 0.0803\nEpoch 7/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0130 - mean_absolute_error: 0.0761 - val_loss: 0.0147 - val_mean_absolute_error: 0.0795\nEpoch 8/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0129 - mean_absolute_error: 0.0756 - val_loss: 0.0148 - val_mean_absolute_error: 0.0820\nEpoch 9/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0128 - mean_absolute_error: 0.0753 - val_loss: 0.0148 - val_mean_absolute_error: 0.0808\nEpoch 10/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0128 - mean_absolute_error: 0.0752 - val_loss: 0.0146 - val_mean_absolute_error: 0.0773\nEpoch 11/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0128 - mean_absolute_error: 0.0753 - val_loss: 0.0147 - val_mean_absolute_error: 0.0787\nEpoch 12/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0127 - mean_absolute_error: 0.0748 - val_loss: 0.0145 - val_mean_absolute_error: 0.0783\nEpoch 13/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0127 - mean_absolute_error: 0.0747 - val_loss: 0.0147 - val_mean_absolute_error: 0.0769\nEpoch 14/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0127 - mean_absolute_error: 0.0747 - val_loss: 0.0145 - val_mean_absolute_error: 0.0789\nEpoch 15/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0126 - mean_absolute_error: 0.0747 - val_loss: 0.0144 - val_mean_absolute_error: 0.0790\nEpoch 16/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0126 - mean_absolute_error: 0.0744 - val_loss: 0.0144 - val_mean_absolute_error: 0.0792\nEpoch 17/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0125 - mean_absolute_error: 0.0742 - val_loss: 0.0148 - val_mean_absolute_error: 0.0785\nEpoch 18/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0125 - mean_absolute_error: 0.0741 - val_loss: 0.0144 - val_mean_absolute_error: 0.0770\nEpoch 19/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0124 - mean_absolute_error: 0.0739 - val_loss: 0.0144 - val_mean_absolute_error: 0.0779\nEpoch 20/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0124 - mean_absolute_error: 0.0739 - val_loss: 0.0144 - val_mean_absolute_error: 0.0809\nEpoch 21/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0124 - mean_absolute_error: 0.0738 - val_loss: 0.0144 - val_mean_absolute_error: 0.0779\nEpoch 22/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0123 - mean_absolute_error: 0.0735 - val_loss: 0.0144 - val_mean_absolute_error: 0.0801\nEpoch 23/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0122 - mean_absolute_error: 0.0732 - val_loss: 0.0148 - val_mean_absolute_error: 0.0776\nEpoch 24/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0122 - mean_absolute_error: 0.0732 - val_loss: 0.0142 - val_mean_absolute_error: 0.0771\nEpoch 25/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0122 - mean_absolute_error: 0.0730 - val_loss: 0.0142 - val_mean_absolute_error: 0.0777\nEpoch 26/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0121 - mean_absolute_error: 0.0728 - val_loss: 0.0142 - val_mean_absolute_error: 0.0764\nEpoch 27/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0121 - mean_absolute_error: 0.0729 - val_loss: 0.0144 - val_mean_absolute_error: 0.0765\nEpoch 28/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0121 - mean_absolute_error: 0.0726 - val_loss: 0.0142 - val_mean_absolute_error: 0.0780\nEpoch 29/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0120 - mean_absolute_error: 0.0724 - val_loss: 0.0144 - val_mean_absolute_error: 0.0761\nEpoch 30/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0120 - mean_absolute_error: 0.0722 - val_loss: 0.0140 - val_mean_absolute_error: 0.0767\nEpoch 31/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0119 - mean_absolute_error: 0.0719 - val_loss: 0.0141 - val_mean_absolute_error: 0.0764\nEpoch 32/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0119 - mean_absolute_error: 0.0719 - val_loss: 0.0141 - val_mean_absolute_error: 0.0758\nEpoch 33/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0119 - mean_absolute_error: 0.0718 - val_loss: 0.0142 - val_mean_absolute_error: 0.0759\n109/109 [==============================] - 0s 3ms/step - loss: 0.0142 - mean_absolute_error: 0.0759\n54/54 [==============================] - 0s 3ms/step - loss: 0.0093 - mean_absolute_error: 0.0600\nEpoch 1/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0100 - mean_absolute_error: 0.0581 - val_loss: 0.0031 - val_mean_absolute_error: 0.0402\nEpoch 2/50\n384/384 [==============================] - 1s 4ms/step - loss: 0.0029 - mean_absolute_error: 0.0343 - val_loss: 0.0024 - val_mean_absolute_error: 0.0316\nEpoch 3/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0024 - mean_absolute_error: 0.0313 - val_loss: 0.0023 - val_mean_absolute_error: 0.0319\nEpoch 4/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0022 - mean_absolute_error: 0.0292 - val_loss: 0.0017 - val_mean_absolute_error: 0.0273\nEpoch 5/50\n384/384 [==============================] - 1s 4ms/step - loss: 0.0021 - mean_absolute_error: 0.0287 - val_loss: 0.0016 - val_mean_absolute_error: 0.0254\nEpoch 6/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0020 - mean_absolute_error: 0.0274 - val_loss: 0.0017 - val_mean_absolute_error: 0.0271\nEpoch 7/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0020 - mean_absolute_error: 0.0277 - val_loss: 0.0016 - val_mean_absolute_error: 0.0256\nEpoch 8/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0020 - mean_absolute_error: 0.0273 - val_loss: 0.0022 - val_mean_absolute_error: 0.0315\nEpoch 9/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0019 - mean_absolute_error: 0.0268 - val_loss: 0.0014 - val_mean_absolute_error: 0.0242\nEpoch 10/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0019 - mean_absolute_error: 0.0269 - val_loss: 0.0014 - val_mean_absolute_error: 0.0260\nEpoch 11/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0019 - mean_absolute_error: 0.0266 - val_loss: 0.0015 - val_mean_absolute_error: 0.0241\nEpoch 12/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0018 - mean_absolute_error: 0.0261 - val_loss: 0.0013 - val_mean_absolute_error: 0.0234\nEpoch 13/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0018 - mean_absolute_error: 0.0254 - val_loss: 0.0014 - val_mean_absolute_error: 0.0248\nEpoch 14/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0018 - mean_absolute_error: 0.0254 - val_loss: 0.0014 - val_mean_absolute_error: 0.0258\nEpoch 15/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0017 - mean_absolute_error: 0.0250 - val_loss: 0.0013 - val_mean_absolute_error: 0.0241\n110/110 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_absolute_error: 0.0241\n55/55 [==============================] - 0s 2ms/step - loss: 9.2933e-04 - mean_absolute_error: 0.0206\nEpoch 1/50\n384/384 [==============================] - 8s 18ms/step - loss: 0.0318 - mean_absolute_error: 0.1260 - val_loss: 0.0111 - val_mean_absolute_error: 0.0743\nEpoch 2/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0072 - mean_absolute_error: 0.0594 - val_loss: 0.0048 - val_mean_absolute_error: 0.0489\nEpoch 3/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0044 - mean_absolute_error: 0.0463 - val_loss: 0.0035 - val_mean_absolute_error: 0.0425\nEpoch 4/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0037 - mean_absolute_error: 0.0425 - val_loss: 0.0033 - val_mean_absolute_error: 0.0408\nEpoch 5/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0034 - mean_absolute_error: 0.0404 - val_loss: 0.0030 - val_mean_absolute_error: 0.0389\nEpoch 6/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0032 - mean_absolute_error: 0.0390 - val_loss: 0.0027 - val_mean_absolute_error: 0.0364\nEpoch 7/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0030 - mean_absolute_error: 0.0379 - val_loss: 0.0027 - val_mean_absolute_error: 0.0362\nEpoch 8/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0030 - mean_absolute_error: 0.0373 - val_loss: 0.0025 - val_mean_absolute_error: 0.0352\nEpoch 9/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0029 - mean_absolute_error: 0.0366 - val_loss: 0.0025 - val_mean_absolute_error: 0.0347\nEpoch 10/50\n384/384 [==============================] - 9s 22ms/step - loss: 0.0028 - mean_absolute_error: 0.0361 - val_loss: 0.0025 - val_mean_absolute_error: 0.0356\nEpoch 11/50\n384/384 [==============================] - 10s 26ms/step - loss: 0.0027 - mean_absolute_error: 0.0355 - val_loss: 0.0024 - val_mean_absolute_error: 0.0343\nEpoch 12/50\n384/384 [==============================] - 8s 19ms/step - loss: 0.0027 - mean_absolute_error: 0.0351 - val_loss: 0.0023 - val_mean_absolute_error: 0.0333\nEpoch 13/50\n384/384 [==============================] - 7s 17ms/step - loss: 0.0026 - mean_absolute_error: 0.0347 - val_loss: 0.0023 - val_mean_absolute_error: 0.0332\nEpoch 14/50\n384/384 [==============================] - 7s 17ms/step - loss: 0.0026 - mean_absolute_error: 0.0344 - val_loss: 0.0023 - val_mean_absolute_error: 0.0335\nEpoch 15/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0025 - mean_absolute_error: 0.0340 - val_loss: 0.0021 - val_mean_absolute_error: 0.0322\nEpoch 16/50\n384/384 [==============================] - 7s 17ms/step - loss: 0.0025 - mean_absolute_error: 0.0338 - val_loss: 0.0022 - val_mean_absolute_error: 0.0336\nEpoch 17/50\n384/384 [==============================] - 6s 17ms/step - loss: 0.0024 - mean_absolute_error: 0.0335 - val_loss: 0.0023 - val_mean_absolute_error: 0.0331\nEpoch 18/50\n384/384 [==============================] - 7s 18ms/step - loss: 0.0024 - mean_absolute_error: 0.0333 - val_loss: 0.0022 - val_mean_absolute_error: 0.0341\n109/109 [==============================] - 1s 6ms/step - loss: 0.0022 - mean_absolute_error: 0.0341\nEpoch 1/50\n383/383 [==============================] - 8s 18ms/step - loss: 0.0464 - mean_absolute_error: 0.1531 - val_loss: 0.0205 - val_mean_absolute_error: 0.0995\nEpoch 2/50\n383/383 [==============================] - 7s 17ms/step - loss: 0.0166 - mean_absolute_error: 0.0891 - val_loss: 0.0177 - val_mean_absolute_error: 0.0897\nEpoch 3/50\n383/383 [==============================] - 7s 17ms/step - loss: 0.0150 - mean_absolute_error: 0.0823 - val_loss: 0.0166 - val_mean_absolute_error: 0.0856\nEpoch 4/50\n383/383 [==============================] - 7s 17ms/step - loss: 0.0143 - mean_absolute_error: 0.0798 - val_loss: 0.0160 - val_mean_absolute_error: 0.0822\nEpoch 5/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0138 - mean_absolute_error: 0.0780 - val_loss: 0.0155 - val_mean_absolute_error: 0.0808\nEpoch 6/50\n383/383 [==============================] - 7s 17ms/step - loss: 0.0134 - mean_absolute_error: 0.0765 - val_loss: 0.0151 - val_mean_absolute_error: 0.0813\nEpoch 7/50\n383/383 [==============================] - 7s 17ms/step - loss: 0.0132 - mean_absolute_error: 0.0757 - val_loss: 0.0149 - val_mean_absolute_error: 0.0784\nEpoch 8/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0131 - mean_absolute_error: 0.0751 - val_loss: 0.0150 - val_mean_absolute_error: 0.0796\nEpoch 9/50\n383/383 [==============================] - 6s 17ms/step - loss: 0.0131 - mean_absolute_error: 0.0749 - val_loss: 0.0145 - val_mean_absolute_error: 0.0789\nEpoch 10/50\n383/383 [==============================] - 7s 18ms/step - loss: 0.0129 - mean_absolute_error: 0.0744 - val_loss: 0.0147 - val_mean_absolute_error: 0.0790\nEpoch 11/50\n383/383 [==============================] - 8s 20ms/step - loss: 0.0128 - mean_absolute_error: 0.0738 - val_loss: 0.0148 - val_mean_absolute_error: 0.0796\nEpoch 12/50\n383/383 [==============================] - 8s 22ms/step - loss: 0.0127 - mean_absolute_error: 0.0735 - val_loss: 0.0145 - val_mean_absolute_error: 0.0761\n109/109 [==============================] - 1s 9ms/step - loss: 0.0145 - mean_absolute_error: 0.0761\n54/54 [==============================] - 0s 7ms/step - loss: 0.0119 - mean_absolute_error: 0.0646\nEpoch 1/50\n384/384 [==============================] - 16s 37ms/step - loss: 0.0325 - mean_absolute_error: 0.1182 - val_loss: 0.0116 - val_mean_absolute_error: 0.0698\nEpoch 2/50\n384/384 [==============================] - 12s 30ms/step - loss: 0.0071 - mean_absolute_error: 0.0542 - val_loss: 0.0048 - val_mean_absolute_error: 0.0443\nEpoch 3/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0040 - mean_absolute_error: 0.0400 - val_loss: 0.0032 - val_mean_absolute_error: 0.0368\nEpoch 4/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0032 - mean_absolute_error: 0.0355 - val_loss: 0.0027 - val_mean_absolute_error: 0.0336\nEpoch 5/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0029 - mean_absolute_error: 0.0334 - val_loss: 0.0023 - val_mean_absolute_error: 0.0309\nEpoch 6/50\n384/384 [==============================] - 8s 19ms/step - loss: 0.0026 - mean_absolute_error: 0.0317 - val_loss: 0.0022 - val_mean_absolute_error: 0.0297\nEpoch 7/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0024 - mean_absolute_error: 0.0302 - val_loss: 0.0020 - val_mean_absolute_error: 0.0287\nEpoch 8/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0023 - mean_absolute_error: 0.0288 - val_loss: 0.0019 - val_mean_absolute_error: 0.0274\nEpoch 9/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0021 - mean_absolute_error: 0.0277 - val_loss: 0.0017 - val_mean_absolute_error: 0.0265\nEpoch 10/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0020 - mean_absolute_error: 0.0268 - val_loss: 0.0016 - val_mean_absolute_error: 0.0251\nEpoch 11/50\n384/384 [==============================] - 12s 30ms/step - loss: 0.0019 - mean_absolute_error: 0.0264 - val_loss: 0.0017 - val_mean_absolute_error: 0.0259\nEpoch 12/50\n384/384 [==============================] - 10s 25ms/step - loss: 0.0018 - mean_absolute_error: 0.0257 - val_loss: 0.0016 - val_mean_absolute_error: 0.0257\nEpoch 13/50\n384/384 [==============================] - 9s 23ms/step - loss: 0.0018 - mean_absolute_error: 0.0253 - val_loss: 0.0014 - val_mean_absolute_error: 0.0238\nEpoch 14/50\n384/384 [==============================] - 9s 24ms/step - loss: 0.0017 - mean_absolute_error: 0.0248 - val_loss: 0.0014 - val_mean_absolute_error: 0.0239\nEpoch 15/50\n384/384 [==============================] - 11s 27ms/step - loss: 0.0017 - mean_absolute_error: 0.0245 - val_loss: 0.0015 - val_mean_absolute_error: 0.0246\nEpoch 16/50\n384/384 [==============================] - 9s 23ms/step - loss: 0.0017 - mean_absolute_error: 0.0243 - val_loss: 0.0015 - val_mean_absolute_error: 0.0244\nEpoch 17/50\n384/384 [==============================] - 11s 29ms/step - loss: 0.0016 - mean_absolute_error: 0.0240 - val_loss: 0.0013 - val_mean_absolute_error: 0.0228\nEpoch 18/50\n384/384 [==============================] - 12s 31ms/step - loss: 0.0016 - mean_absolute_error: 0.0238 - val_loss: 0.0013 - val_mean_absolute_error: 0.0225\nEpoch 19/50\n384/384 [==============================] - 11s 30ms/step - loss: 0.0016 - mean_absolute_error: 0.0235 - val_loss: 0.0013 - val_mean_absolute_error: 0.0224\nEpoch 20/50\n384/384 [==============================] - 13s 33ms/step - loss: 0.0015 - mean_absolute_error: 0.0233 - val_loss: 0.0013 - val_mean_absolute_error: 0.0227\nEpoch 21/50\n384/384 [==============================] - 9s 24ms/step - loss: 0.0015 - mean_absolute_error: 0.0231 - val_loss: 0.0012 - val_mean_absolute_error: 0.0219\nEpoch 22/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0015 - mean_absolute_error: 0.0230 - val_loss: 0.0012 - val_mean_absolute_error: 0.0219\nEpoch 23/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0229 - val_loss: 0.0014 - val_mean_absolute_error: 0.0233\nEpoch 24/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0227 - val_loss: 0.0012 - val_mean_absolute_error: 0.0218\nEpoch 25/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0226 - val_loss: 0.0012 - val_mean_absolute_error: 0.0228\nEpoch 26/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0014 - mean_absolute_error: 0.0225 - val_loss: 0.0012 - val_mean_absolute_error: 0.0212\nEpoch 27/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0014 - mean_absolute_error: 0.0223 - val_loss: 0.0012 - val_mean_absolute_error: 0.0216\nEpoch 28/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0014 - mean_absolute_error: 0.0223 - val_loss: 0.0011 - val_mean_absolute_error: 0.0210\nEpoch 29/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0014 - mean_absolute_error: 0.0221 - val_loss: 0.0011 - val_mean_absolute_error: 0.0212\nEpoch 30/50\n384/384 [==============================] - 7s 19ms/step - loss: 0.0014 - mean_absolute_error: 0.0220 - val_loss: 0.0011 - val_mean_absolute_error: 0.0213\nEpoch 31/50\n384/384 [==============================] - 9s 24ms/step - loss: 0.0014 - mean_absolute_error: 0.0219 - val_loss: 0.0011 - val_mean_absolute_error: 0.0212\nEpoch 32/50\n384/384 [==============================] - 9s 23ms/step - loss: 0.0014 - mean_absolute_error: 0.0218 - val_loss: 0.0011 - val_mean_absolute_error: 0.0208\nEpoch 33/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0013 - mean_absolute_error: 0.0218 - val_loss: 0.0011 - val_mean_absolute_error: 0.0205\nEpoch 34/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0013 - mean_absolute_error: 0.0216 - val_loss: 0.0011 - val_mean_absolute_error: 0.0208\nEpoch 35/50\n384/384 [==============================] - 9s 23ms/step - loss: 0.0013 - mean_absolute_error: 0.0215 - val_loss: 0.0012 - val_mean_absolute_error: 0.0213\nEpoch 36/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0013 - mean_absolute_error: 0.0215 - val_loss: 0.0010 - val_mean_absolute_error: 0.0201\nEpoch 37/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0013 - mean_absolute_error: 0.0213 - val_loss: 0.0011 - val_mean_absolute_error: 0.0206\nEpoch 38/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0013 - mean_absolute_error: 0.0213 - val_loss: 0.0010 - val_mean_absolute_error: 0.0201\nEpoch 39/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0013 - mean_absolute_error: 0.0211 - val_loss: 9.9319e-04 - val_mean_absolute_error: 0.0198\nEpoch 40/50\n384/384 [==============================] - 8s 21ms/step - loss: 0.0013 - mean_absolute_error: 0.0210 - val_loss: 9.8922e-04 - val_mean_absolute_error: 0.0201\nEpoch 41/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0013 - mean_absolute_error: 0.0210 - val_loss: 0.0011 - val_mean_absolute_error: 0.0210\nEpoch 42/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0012 - mean_absolute_error: 0.0209 - val_loss: 0.0010 - val_mean_absolute_error: 0.0201\nEpoch 43/50\n384/384 [==============================] - 8s 20ms/step - loss: 0.0012 - mean_absolute_error: 0.0208 - val_loss: 0.0011 - val_mean_absolute_error: 0.0207\n109/109 [==============================] - 1s 9ms/step - loss: 0.0011 - mean_absolute_error: 0.0207\n55/55 [==============================] - 1s 8ms/step - loss: 7.9657e-04 - mean_absolute_error: 0.0176\n</pre> In\u00a0[10]: Copied! <pre>KERNEL_WIDTH = 3\nconv_window = DataWindow(input_width=KERNEL_WIDTH, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\n\ncnn_model = Sequential([\n    Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),\n    Dense(units=32, activation='relu'),\n    Dense(units=1)\n])\nhistory = compile_and_fit(cnn_model, conv_window)\nval_performance['CNN'] = cnn_model.evaluate(conv_window.val)\ntest_performance['CNN'] = cnn_model.evaluate(conv_window.test)\n</pre> KERNEL_WIDTH = 3 conv_window = DataWindow(input_width=KERNEL_WIDTH, label_width=1, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])  cnn_model = Sequential([     Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),     Dense(units=32, activation='relu'),     Dense(units=1) ]) history = compile_and_fit(cnn_model, conv_window) val_performance['CNN'] = cnn_model.evaluate(conv_window.val) test_performance['CNN'] = cnn_model.evaluate(conv_window.test) <pre>Epoch 1/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0360 - mean_absolute_error: 0.1166 - val_loss: 0.0072 - val_mean_absolute_error: 0.0614\nEpoch 2/50\n384/384 [==============================] - 1s 4ms/step - loss: 0.0064 - mean_absolute_error: 0.0587 - val_loss: 0.0050 - val_mean_absolute_error: 0.0527\nEpoch 3/50\n384/384 [==============================] - 1s 4ms/step - loss: 0.0051 - mean_absolute_error: 0.0525 - val_loss: 0.0042 - val_mean_absolute_error: 0.0494\nEpoch 4/50\n384/384 [==============================] - 1s 4ms/step - loss: 0.0043 - mean_absolute_error: 0.0482 - val_loss: 0.0041 - val_mean_absolute_error: 0.0500\nEpoch 5/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0039 - mean_absolute_error: 0.0459 - val_loss: 0.0033 - val_mean_absolute_error: 0.0439\nEpoch 6/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0036 - mean_absolute_error: 0.0438 - val_loss: 0.0033 - val_mean_absolute_error: 0.0439\nEpoch 7/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0033 - mean_absolute_error: 0.0421 - val_loss: 0.0036 - val_mean_absolute_error: 0.0461\nEpoch 8/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0032 - mean_absolute_error: 0.0409 - val_loss: 0.0027 - val_mean_absolute_error: 0.0393\nEpoch 9/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0030 - mean_absolute_error: 0.0396 - val_loss: 0.0025 - val_mean_absolute_error: 0.0367\nEpoch 10/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0029 - mean_absolute_error: 0.0386 - val_loss: 0.0026 - val_mean_absolute_error: 0.0375\nEpoch 11/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0029 - mean_absolute_error: 0.0385 - val_loss: 0.0023 - val_mean_absolute_error: 0.0354\nEpoch 12/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0028 - mean_absolute_error: 0.0384 - val_loss: 0.0022 - val_mean_absolute_error: 0.0349\nEpoch 13/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0027 - mean_absolute_error: 0.0374 - val_loss: 0.0023 - val_mean_absolute_error: 0.0358\nEpoch 14/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0027 - mean_absolute_error: 0.0372 - val_loss: 0.0021 - val_mean_absolute_error: 0.0340\nEpoch 15/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0026 - mean_absolute_error: 0.0364 - val_loss: 0.0021 - val_mean_absolute_error: 0.0336\nEpoch 16/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0026 - mean_absolute_error: 0.0359 - val_loss: 0.0023 - val_mean_absolute_error: 0.0357\nEpoch 17/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0026 - mean_absolute_error: 0.0360 - val_loss: 0.0021 - val_mean_absolute_error: 0.0336\nEpoch 18/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0026 - mean_absolute_error: 0.0358 - val_loss: 0.0021 - val_mean_absolute_error: 0.0345\n110/110 [==============================] - 0s 3ms/step - loss: 0.0021 - mean_absolute_error: 0.0345\n55/55 [==============================] - 0s 4ms/step - loss: 0.0017 - mean_absolute_error: 0.0308\n</pre> In\u00a0[11]: Copied! <pre>cnn_lstm_model = Sequential([\n    Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),\n    LSTM(32, return_sequences=True),\n    LSTM(32, return_sequences=True),\n    Dense(units=1)\n])\nhistory = compile_and_fit(cnn_lstm_model, conv_window)\nval_performance['CNN + LSTM'] = cnn_lstm_model.evaluate(conv_window.val)\ntest_performance['CNN + LSTM'] = cnn_lstm_model.evaluate(conv_window.test)\n</pre> cnn_lstm_model = Sequential([     Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),     LSTM(32, return_sequences=True),     LSTM(32, return_sequences=True),     Dense(units=1) ]) history = compile_and_fit(cnn_lstm_model, conv_window) val_performance['CNN + LSTM'] = cnn_lstm_model.evaluate(conv_window.val) test_performance['CNN + LSTM'] = cnn_lstm_model.evaluate(conv_window.test) <pre>Epoch 1/50\n384/384 [==============================] - 6s 8ms/step - loss: 0.0306 - mean_absolute_error: 0.1142 - val_loss: 0.0070 - val_mean_absolute_error: 0.0596\nEpoch 2/50\n384/384 [==============================] - 3s 7ms/step - loss: 0.0063 - mean_absolute_error: 0.0583 - val_loss: 0.0050 - val_mean_absolute_error: 0.0522\nEpoch 3/50\n384/384 [==============================] - 4s 11ms/step - loss: 0.0049 - mean_absolute_error: 0.0515 - val_loss: 0.0039 - val_mean_absolute_error: 0.0466\nEpoch 4/50\n384/384 [==============================] - 4s 10ms/step - loss: 0.0041 - mean_absolute_error: 0.0469 - val_loss: 0.0039 - val_mean_absolute_error: 0.0478\nEpoch 5/50\n384/384 [==============================] - 4s 10ms/step - loss: 0.0038 - mean_absolute_error: 0.0446 - val_loss: 0.0031 - val_mean_absolute_error: 0.0417\nEpoch 6/50\n384/384 [==============================] - 4s 11ms/step - loss: 0.0035 - mean_absolute_error: 0.0427 - val_loss: 0.0033 - val_mean_absolute_error: 0.0429\nEpoch 7/50\n384/384 [==============================] - 4s 10ms/step - loss: 0.0033 - mean_absolute_error: 0.0412 - val_loss: 0.0027 - val_mean_absolute_error: 0.0392\nEpoch 8/50\n384/384 [==============================] - 4s 10ms/step - loss: 0.0031 - mean_absolute_error: 0.0402 - val_loss: 0.0026 - val_mean_absolute_error: 0.0377\nEpoch 9/50\n384/384 [==============================] - 4s 10ms/step - loss: 0.0031 - mean_absolute_error: 0.0398 - val_loss: 0.0026 - val_mean_absolute_error: 0.0374\nEpoch 10/50\n384/384 [==============================] - 4s 10ms/step - loss: 0.0029 - mean_absolute_error: 0.0382 - val_loss: 0.0026 - val_mean_absolute_error: 0.0382\nEpoch 11/50\n384/384 [==============================] - 4s 11ms/step - loss: 0.0028 - mean_absolute_error: 0.0378 - val_loss: 0.0030 - val_mean_absolute_error: 0.0419\n110/110 [==============================] - 1s 5ms/step - loss: 0.0030 - mean_absolute_error: 0.0419\n55/55 [==============================] - 0s 5ms/step - loss: 0.0027 - mean_absolute_error: 0.0408\n</pre> In\u00a0[12]: Copied! <pre>KERNEL_WIDTH = 3\nLABEL_WIDTH = 24\nINPUT_WIDTH = LABEL_WIDTH + KERNEL_WIDTH - 1\nmulti_conv_window = DataWindow(input_width=INPUT_WIDTH, label_width=LABEL_WIDTH, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\n\nms_cnn_model = Sequential([\n    Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),\n    Dense(units=32, activation='relu'),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\nhistory = compile_and_fit(ms_cnn_model, multi_conv_window)\nms_val_performance['CNN'] = ms_cnn_model.evaluate(multi_conv_window.val)\nms_test_performance['CNN'] = ms_cnn_model.evaluate(multi_conv_window.test)\n</pre> KERNEL_WIDTH = 3 LABEL_WIDTH = 24 INPUT_WIDTH = LABEL_WIDTH + KERNEL_WIDTH - 1 multi_conv_window = DataWindow(input_width=INPUT_WIDTH, label_width=LABEL_WIDTH, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])  ms_cnn_model = Sequential([     Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),     Dense(units=32, activation='relu'),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) history = compile_and_fit(ms_cnn_model, multi_conv_window) ms_val_performance['CNN'] = ms_cnn_model.evaluate(multi_conv_window.val) ms_test_performance['CNN'] = ms_cnn_model.evaluate(multi_conv_window.test) <pre>Epoch 1/50\n383/383 [==============================] - 3s 5ms/step - loss: 0.0358 - mean_absolute_error: 0.1232 - val_loss: 0.0164 - val_mean_absolute_error: 0.0839\nEpoch 2/50\n383/383 [==============================] - 2s 4ms/step - loss: 0.0142 - mean_absolute_error: 0.0791 - val_loss: 0.0160 - val_mean_absolute_error: 0.0859\nEpoch 3/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0138 - mean_absolute_error: 0.0784 - val_loss: 0.0158 - val_mean_absolute_error: 0.0823\nEpoch 4/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0135 - mean_absolute_error: 0.0774 - val_loss: 0.0154 - val_mean_absolute_error: 0.0840\nEpoch 5/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0132 - mean_absolute_error: 0.0766 - val_loss: 0.0152 - val_mean_absolute_error: 0.0831\nEpoch 6/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0130 - mean_absolute_error: 0.0759 - val_loss: 0.0147 - val_mean_absolute_error: 0.0796\nEpoch 7/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0130 - mean_absolute_error: 0.0759 - val_loss: 0.0146 - val_mean_absolute_error: 0.0820\nEpoch 8/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0129 - mean_absolute_error: 0.0757 - val_loss: 0.0148 - val_mean_absolute_error: 0.0824\nEpoch 9/50\n383/383 [==============================] - 3s 9ms/step - loss: 0.0127 - mean_absolute_error: 0.0750 - val_loss: 0.0149 - val_mean_absolute_error: 0.0822\nEpoch 10/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0126 - mean_absolute_error: 0.0746 - val_loss: 0.0146 - val_mean_absolute_error: 0.0796\nEpoch 11/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0126 - mean_absolute_error: 0.0745 - val_loss: 0.0145 - val_mean_absolute_error: 0.0796\nEpoch 12/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0125 - mean_absolute_error: 0.0741 - val_loss: 0.0145 - val_mean_absolute_error: 0.0778\nEpoch 13/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0124 - mean_absolute_error: 0.0735 - val_loss: 0.0145 - val_mean_absolute_error: 0.0768\nEpoch 14/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0123 - mean_absolute_error: 0.0732 - val_loss: 0.0146 - val_mean_absolute_error: 0.0806\nEpoch 15/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0123 - mean_absolute_error: 0.0730 - val_loss: 0.0145 - val_mean_absolute_error: 0.0769\n109/109 [==============================] - 1s 5ms/step - loss: 0.0145 - mean_absolute_error: 0.0769\n54/54 [==============================] - 0s 4ms/step - loss: 0.0097 - mean_absolute_error: 0.0625\n</pre> In\u00a0[13]: Copied! <pre>ms_cnn_lstm_model = Sequential([\n    Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),\n    LSTM(32, return_sequences=True),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\nhistory = compile_and_fit(ms_cnn_lstm_model, multi_conv_window)\nms_val_performance['CNN + LSTM'] = ms_cnn_lstm_model.evaluate(multi_conv_window.val)\nms_test_performance['CNN + LSTM'] = ms_cnn_lstm_model.evaluate(multi_conv_window.test)\n</pre> ms_cnn_lstm_model = Sequential([     Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),     LSTM(32, return_sequences=True),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) history = compile_and_fit(ms_cnn_lstm_model, multi_conv_window) ms_val_performance['CNN + LSTM'] = ms_cnn_lstm_model.evaluate(multi_conv_window.val) ms_test_performance['CNN + LSTM'] = ms_cnn_lstm_model.evaluate(multi_conv_window.test) <pre>Epoch 1/50\n383/383 [==============================] - 10s 22ms/step - loss: 0.0419 - mean_absolute_error: 0.1452 - val_loss: 0.0174 - val_mean_absolute_error: 0.0904\nEpoch 2/50\n383/383 [==============================] - 10s 25ms/step - loss: 0.0147 - mean_absolute_error: 0.0823 - val_loss: 0.0161 - val_mean_absolute_error: 0.0859\nEpoch 3/50\n383/383 [==============================] - 10s 27ms/step - loss: 0.0141 - mean_absolute_error: 0.0801 - val_loss: 0.0158 - val_mean_absolute_error: 0.0866\nEpoch 4/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0137 - mean_absolute_error: 0.0787 - val_loss: 0.0155 - val_mean_absolute_error: 0.0855\nEpoch 5/50\n383/383 [==============================] - 8s 22ms/step - loss: 0.0132 - mean_absolute_error: 0.0770 - val_loss: 0.0149 - val_mean_absolute_error: 0.0810\nEpoch 6/50\n383/383 [==============================] - 11s 28ms/step - loss: 0.0127 - mean_absolute_error: 0.0752 - val_loss: 0.0151 - val_mean_absolute_error: 0.0766\nEpoch 7/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0123 - mean_absolute_error: 0.0733 - val_loss: 0.0136 - val_mean_absolute_error: 0.0763\nEpoch 8/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0120 - mean_absolute_error: 0.0722 - val_loss: 0.0134 - val_mean_absolute_error: 0.0750\nEpoch 9/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0120 - mean_absolute_error: 0.0721 - val_loss: 0.0132 - val_mean_absolute_error: 0.0769\nEpoch 10/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0117 - mean_absolute_error: 0.0711 - val_loss: 0.0131 - val_mean_absolute_error: 0.0743\nEpoch 11/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0116 - mean_absolute_error: 0.0705 - val_loss: 0.0129 - val_mean_absolute_error: 0.0742\nEpoch 12/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0114 - mean_absolute_error: 0.0699 - val_loss: 0.0138 - val_mean_absolute_error: 0.0805\nEpoch 13/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0113 - mean_absolute_error: 0.0695 - val_loss: 0.0127 - val_mean_absolute_error: 0.0736\nEpoch 14/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0112 - mean_absolute_error: 0.0690 - val_loss: 0.0129 - val_mean_absolute_error: 0.0742\nEpoch 15/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0111 - mean_absolute_error: 0.0687 - val_loss: 0.0125 - val_mean_absolute_error: 0.0730\nEpoch 16/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0109 - mean_absolute_error: 0.0678 - val_loss: 0.0126 - val_mean_absolute_error: 0.0743\nEpoch 17/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0109 - mean_absolute_error: 0.0678 - val_loss: 0.0131 - val_mean_absolute_error: 0.0719\nEpoch 18/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0108 - mean_absolute_error: 0.0674 - val_loss: 0.0131 - val_mean_absolute_error: 0.0774\n109/109 [==============================] - 1s 9ms/step - loss: 0.0131 - mean_absolute_error: 0.0774\n54/54 [==============================] - 1s 9ms/step - loss: 0.0084 - mean_absolute_error: 0.0599\n</pre> In\u00a0[14]: Copied! <pre>mo_conv_window = DataWindow(input_width=INPUT_WIDTH, label_width=LABEL_WIDTH, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)\n\nmo_cnn_model = Sequential([\n    Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),\n    Dense(units=32, activation='relu'),\n    Dense(units=2)\n])\nhistory = compile_and_fit(mo_cnn_model, mo_conv_window)\nmo_val_performance['CNN'] = mo_cnn_model.evaluate(mo_conv_window.val)\nmo_test_performance['CNN'] = mo_cnn_model.evaluate(mo_conv_window.test)\n</pre> mo_conv_window = DataWindow(input_width=INPUT_WIDTH, label_width=LABEL_WIDTH, shift=1, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=col_names)  mo_cnn_model = Sequential([     Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),     Dense(units=32, activation='relu'),     Dense(units=2) ]) history = compile_and_fit(mo_cnn_model, mo_conv_window) mo_val_performance['CNN'] = mo_cnn_model.evaluate(mo_conv_window.val) mo_test_performance['CNN'] = mo_cnn_model.evaluate(mo_conv_window.test) <pre>Epoch 1/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0101 - mean_absolute_error: 0.0589 - val_loss: 0.0023 - val_mean_absolute_error: 0.0318\nEpoch 2/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0020 - mean_absolute_error: 0.0288 - val_loss: 0.0014 - val_mean_absolute_error: 0.0258\nEpoch 3/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0016 - mean_absolute_error: 0.0252 - val_loss: 0.0012 - val_mean_absolute_error: 0.0236\nEpoch 4/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0015 - mean_absolute_error: 0.0238 - val_loss: 0.0012 - val_mean_absolute_error: 0.0232\nEpoch 5/50\n384/384 [==============================] - 2s 4ms/step - loss: 0.0014 - mean_absolute_error: 0.0230 - val_loss: 0.0011 - val_mean_absolute_error: 0.0219\nEpoch 6/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0013 - mean_absolute_error: 0.0224 - val_loss: 0.0011 - val_mean_absolute_error: 0.0213\nEpoch 7/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0013 - mean_absolute_error: 0.0220 - val_loss: 0.0011 - val_mean_absolute_error: 0.0214\nEpoch 8/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0013 - mean_absolute_error: 0.0217 - val_loss: 0.0010 - val_mean_absolute_error: 0.0206\nEpoch 9/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0013 - mean_absolute_error: 0.0215 - val_loss: 9.9162e-04 - val_mean_absolute_error: 0.0201\nEpoch 10/50\n384/384 [==============================] - 3s 6ms/step - loss: 0.0012 - mean_absolute_error: 0.0213 - val_loss: 0.0010 - val_mean_absolute_error: 0.0204\nEpoch 11/50\n384/384 [==============================] - 2s 6ms/step - loss: 0.0012 - mean_absolute_error: 0.0209 - val_loss: 0.0010 - val_mean_absolute_error: 0.0206\nEpoch 12/50\n384/384 [==============================] - 2s 5ms/step - loss: 0.0012 - mean_absolute_error: 0.0208 - val_loss: 0.0011 - val_mean_absolute_error: 0.0214\n109/109 [==============================] - 0s 3ms/step - loss: 0.0011 - mean_absolute_error: 0.0214\n55/55 [==============================] - 0s 3ms/step - loss: 8.4113e-04 - mean_absolute_error: 0.0187\n</pre> In\u00a0[16]: Copied! <pre>mo_cnn_lstm_model = Sequential([\n    Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),\n    LSTM(32, return_sequences=True),\n    Dense(units=2)\n])\nhistory = compile_and_fit(mo_cnn_lstm_model, mo_conv_window)\nmo_val_performance['CNN + LSTM'] = mo_cnn_lstm_model.evaluate(mo_conv_window.val)\nmo_test_performance['CNN + LSTM'] = mo_cnn_lstm_model.evaluate(mo_conv_window.test)\n</pre> mo_cnn_lstm_model = Sequential([     Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),     LSTM(32, return_sequences=True),     Dense(units=2) ]) history = compile_and_fit(mo_cnn_lstm_model, mo_conv_window) mo_val_performance['CNN + LSTM'] = mo_cnn_lstm_model.evaluate(mo_conv_window.val) mo_test_performance['CNN + LSTM'] = mo_cnn_lstm_model.evaluate(mo_conv_window.test) <pre>Epoch 1/50\n384/384 [==============================] - 6s 13ms/step - loss: 0.0230 - mean_absolute_error: 0.0913 - val_loss: 0.0041 - val_mean_absolute_error: 0.0415\nEpoch 2/50\n384/384 [==============================] - 5s 13ms/step - loss: 0.0032 - mean_absolute_error: 0.0363 - val_loss: 0.0024 - val_mean_absolute_error: 0.0324\nEpoch 3/50\n384/384 [==============================] - 5s 14ms/step - loss: 0.0026 - mean_absolute_error: 0.0322 - val_loss: 0.0020 - val_mean_absolute_error: 0.0292\nEpoch 4/50\n384/384 [==============================] - 5s 14ms/step - loss: 0.0022 - mean_absolute_error: 0.0298 - val_loss: 0.0017 - val_mean_absolute_error: 0.0279\nEpoch 5/50\n384/384 [==============================] - 5s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0278 - val_loss: 0.0017 - val_mean_absolute_error: 0.0279\nEpoch 6/50\n384/384 [==============================] - 5s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0263 - val_loss: 0.0015 - val_mean_absolute_error: 0.0274\nEpoch 7/50\n384/384 [==============================] - 6s 15ms/step - loss: 0.0016 - mean_absolute_error: 0.0255 - val_loss: 0.0013 - val_mean_absolute_error: 0.0238\nEpoch 8/50\n384/384 [==============================] - 5s 14ms/step - loss: 0.0016 - mean_absolute_error: 0.0246 - val_loss: 0.0013 - val_mean_absolute_error: 0.0237\nEpoch 9/50\n384/384 [==============================] - 6s 15ms/step - loss: 0.0015 - mean_absolute_error: 0.0240 - val_loss: 0.0013 - val_mean_absolute_error: 0.0238\nEpoch 10/50\n384/384 [==============================] - 5s 14ms/step - loss: 0.0015 - mean_absolute_error: 0.0236 - val_loss: 0.0012 - val_mean_absolute_error: 0.0238\nEpoch 11/50\n384/384 [==============================] - 6s 14ms/step - loss: 0.0015 - mean_absolute_error: 0.0232 - val_loss: 0.0013 - val_mean_absolute_error: 0.0246\nEpoch 12/50\n384/384 [==============================] - 6s 15ms/step - loss: 0.0014 - mean_absolute_error: 0.0229 - val_loss: 0.0012 - val_mean_absolute_error: 0.0224\nEpoch 13/50\n384/384 [==============================] - 6s 15ms/step - loss: 0.0014 - mean_absolute_error: 0.0227 - val_loss: 0.0011 - val_mean_absolute_error: 0.0221\nEpoch 14/50\n384/384 [==============================] - 6s 15ms/step - loss: 0.0014 - mean_absolute_error: 0.0225 - val_loss: 0.0011 - val_mean_absolute_error: 0.0211\nEpoch 15/50\n384/384 [==============================] - 6s 15ms/step - loss: 0.0014 - mean_absolute_error: 0.0224 - val_loss: 0.0011 - val_mean_absolute_error: 0.0222\nEpoch 16/50\n384/384 [==============================] - 6s 15ms/step - loss: 0.0013 - mean_absolute_error: 0.0221 - val_loss: 0.0011 - val_mean_absolute_error: 0.0224\nEpoch 17/50\n384/384 [==============================] - 6s 16ms/step - loss: 0.0013 - mean_absolute_error: 0.0219 - val_loss: 0.0011 - val_mean_absolute_error: 0.0217\n109/109 [==============================] - 1s 6ms/step - loss: 0.0011 - mean_absolute_error: 0.0217\n55/55 [==============================] - 0s 6ms/step - loss: 8.2616e-04 - mean_absolute_error: 0.0185\n</pre> In\u00a0[23]: Copied! <pre># single step\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Single Step')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(val_performance.keys(), [v[1] for v in val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(test_performance.keys(), [v[1] for v in test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.xticks(rotation=45, ha='right')\nplt.legend()\nplt.tight_layout()\n</pre> # single step\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Single Step') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(val_performance.keys(), [v[1] for v in val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(test_performance.keys(), [v[1] for v in test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.xticks(rotation=45, ha='right') plt.legend() plt.tight_layout() In\u00a0[22]: Copied! <pre># multi step\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Multi Step')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(ms_val_performance.keys(), [v[1] for v in ms_val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(ms_test_performance.keys(), [v[1] for v in ms_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.legend()\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\n</pre> # multi step\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Multi Step') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(ms_val_performance.keys(), [v[1] for v in ms_val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(ms_test_performance.keys(), [v[1] for v in ms_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.legend() plt.xticks(rotation=45, ha='right') plt.tight_layout() In\u00a0[24]: Copied! <pre># multi output\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Multi Output')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(mo_val_performance.keys(), [v[1] for v in mo_val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(mo_test_performance.keys(), [v[1] for v in mo_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.xticks(rotation=45, ha='right')\nplt.legend()\nplt.tight_layout()\n</pre> # multi output\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Multi Output') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(mo_val_performance.keys(), [v[1] for v in mo_val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(mo_test_performance.keys(), [v[1] for v in mo_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.xticks(rotation=45, ha='right') plt.legend() plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter16.html#cnn","title":"\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(CNN)\u3092\u8abf\u3079\u308b\u00b6","text":""},{"location":"chapter16.html#cnn","title":"CNN\u3092\u5b9f\u88c5\u3059\u308b\u00b6","text":""},{"location":"chapter17.html","title":"\u7b2c17\u7ae0 \u4e88\u6e2c\u3092\u4f7f\u3063\u3066\u3055\u3089\u306b\u4e88\u6e2c\u3092\u884c\u3046","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.metrics import MeanAbsoluteError\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Conv1D\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.losses import MeanSquaredError from tensorflow.keras.metrics import MeanAbsoluteError from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.optimizers import Adam from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense, LSTM, Conv1D <pre>2023-11-23 10:28:40.029962: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-11-23 10:28:40.045119: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-23 10:28:40.159970: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2023-11-23 10:28:40.160215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2023-11-23 10:28:40.187395: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-11-23 10:28:40.234423: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-23 10:28:40.235547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-23 10:28:41.093645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n</pre> In\u00a0[3]: Copied! <pre>url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv'\nurl_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv'\nurl_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'\n\ndf_train = pd.read_csv(url_train, index_col=0)\ndf_val = pd.read_csv(url_val, index_col=0)\ndf_test = pd.read_csv(url_test, index_col=0)\n</pre> url_train = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/train.csv' url_val = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/val.csv' url_test = 'https://raw.githubusercontent.com/marcopeix/TimeSeriesForecastingInPython/master/data/test.csv'  df_train = pd.read_csv(url_train, index_col=0) df_val = pd.read_csv(url_val, index_col=0) df_test = pd.read_csv(url_test, index_col=0) In\u00a0[4]: Copied! <pre>class DataWindow:\n    def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):\n        # window size\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n        self.total_window_size = input_width + shift\n        \n        # \u30c7\u30fc\u30bf\n        self.df_train = df_train\n        self.df_val = df_val\n        self.df_test = df_test\n        \n        # \u30e9\u30d9\u30eb\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}\n        \n        # \u30b9\u30e9\u30a4\u30b9\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n        \n        # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e\n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n    \n    def split_to_inputs_labels(self, features):\n        inputs = features[:, self.input_slice, :]\n        labels = features[:, self.labels_slice, :]\n        if self.label_columns is not None:\n            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n        inputs.set_shape([None, self.input_width, None])\n        labels.set_shape([None, self.label_width, None])\n        return inputs, labels\n    \n    def plot(self, plot_col: str, model=None, max_subplots=3):\n        inputs, labels = self.sample_batch\n        plt.figure(figsize=(12, 8))\n        plot_col_index = self.column_indices[plot_col]\n        n_max = min(max_subplots, len(inputs))\n        \n        for n in range(n_max):\n            plt.subplot(n_max, 1, n+1)\n            plt.ylabel(f'{plot_col} [scaled]')\n            plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)\n            \n            if self.label_columns:\n                label_col_index = self.label_columns_indices.get(plot_col, None)\n            else:\n                label_col_index = plot_col_index\n            \n            if label_col_index is None:\n                continue\n            \n            plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)\n            \n            if model is not None:\n                predictions = model(inputs)\n                plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)\n            \n            if n == 0:\n                plt.legend()\n        plt.xlabel('Time (h)')\n    \n    def make_dataset(self, data):\n        data = np.array(data, dtype=np.float32)\n        ds = tf.keras.utils.timeseries_dataset_from_array(\n            data=data,\n            targets=None,\n            sequence_length=self.total_window_size,\n            sequence_stride=1,\n            shuffle=True,\n            batch_size=32,\n        )\n        ds = ds.map(self.split_to_inputs_labels)\n        return ds\n    \n    @property\n    def train(self):\n        return self.make_dataset(self.df_train)\n    \n    @property\n    def val(self):\n        return self.make_dataset(self.df_val)\n    \n    @property\n    def test(self):\n        return self.make_dataset(self.df_test)\n    \n    @property\n    def sample_batch(self):\n        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n        result = getattr(self, '_sample_batch', None)\n        if result is None:\n            result = next(iter(self.train))\n            self._sample_batch = result\n        return result\n</pre> class DataWindow:     def __init__(self, input_width, label_width, shift, df_train, df_val, df_test, label_columns=None):         # window size         self.input_width = input_width         self.label_width = label_width         self.shift = shift         self.total_window_size = input_width + shift                  # \u30c7\u30fc\u30bf         self.df_train = df_train         self.df_val = df_val         self.df_test = df_test                  # \u30e9\u30d9\u30eb         self.label_columns = label_columns         if label_columns is not None:             self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}         self.column_indices = {name: i for i, name in enumerate(self.df_train.columns)}                  # \u30b9\u30e9\u30a4\u30b9         self.input_slice = slice(0, input_width)         self.input_indices = np.arange(self.total_window_size)[self.input_slice]                  # \u30e9\u30d9\u30eb\u958b\u59cb\u4f4d\u7f6e         self.label_start = self.total_window_size - self.label_width         self.labels_slice = slice(self.label_start, None)         self.label_indices = np.arange(self.total_window_size)[self.labels_slice]          def split_to_inputs_labels(self, features):         inputs = features[:, self.input_slice, :]         labels = features[:, self.labels_slice, :]         if self.label_columns is not None:             labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)         inputs.set_shape([None, self.input_width, None])         labels.set_shape([None, self.label_width, None])         return inputs, labels          def plot(self, plot_col: str, model=None, max_subplots=3):         inputs, labels = self.sample_batch         plt.figure(figsize=(12, 8))         plot_col_index = self.column_indices[plot_col]         n_max = min(max_subplots, len(inputs))                  for n in range(n_max):             plt.subplot(n_max, 1, n+1)             plt.ylabel(f'{plot_col} [scaled]')             plt.plot(self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=-10)                          if self.label_columns:                 label_col_index = self.label_columns_indices.get(plot_col, None)             else:                 label_col_index = plot_col_index                          if label_col_index is None:                 continue                          plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='tab:green', s=64)                          if model is not None:                 predictions = model(inputs)                 plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='tab:red', s=64)                          if n == 0:                 plt.legend()         plt.xlabel('Time (h)')          def make_dataset(self, data):         data = np.array(data, dtype=np.float32)         ds = tf.keras.utils.timeseries_dataset_from_array(             data=data,             targets=None,             sequence_length=self.total_window_size,             sequence_stride=1,             shuffle=True,             batch_size=32,         )         ds = ds.map(self.split_to_inputs_labels)         return ds          @property     def train(self):         return self.make_dataset(self.df_train)          @property     def val(self):         return self.make_dataset(self.df_val)          @property     def test(self):         return self.make_dataset(self.df_test)          @property     def sample_batch(self):         \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"         result = getattr(self, '_sample_batch', None)         if result is None:             result = next(iter(self.train))             self._sample_batch = result         return result In\u00a0[5]: Copied! <pre># for training\ndef compile_and_fit(model, window, patience=3, max_epochs=50):\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=patience,\n        mode='min'\n    )\n    model.compile(\n        loss=MeanSquaredError(),\n        optimizer=Adam(),\n        metrics=[MeanAbsoluteError()]\n    )\n    history = model.fit(\n        window.train,\n        epochs=max_epochs,\n        validation_data=window.val,\n        callbacks=[early_stopping]\n    )\n    return history\n</pre> # for training def compile_and_fit(model, window, patience=3, max_epochs=50):     early_stopping = EarlyStopping(         monitor='val_loss',         patience=patience,         mode='min'     )     model.compile(         loss=MeanSquaredError(),         optimizer=Adam(),         metrics=[MeanAbsoluteError()]     )     history = model.fit(         window.train,         epochs=max_epochs,         validation_data=window.val,         callbacks=[early_stopping]     )     return history In\u00a0[6]: Copied! <pre># models\nclass Baseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n        \n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        elif isinstance(self.label_index, list):\n            tensors = []\n            for index in self.label_index:\n                res = inputs[:, :, index]\n                res = res[:, :, tf.newaxis]\n                tensors.append(res)\n            return tf.concat(tensors, axis=-1)\n        else:\n            res = inputs[:, :, self.label_index]\n            return res[:, :, tf.newaxis]\n\nclass MultiStepLastBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        if self.label_index is None:\n            return tf.tile(inputs[:, -1:, :], [1, 24, 1])\n        return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])\n    \nclass RepeatBaseline(Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n    \n    def call(self, inputs):\n        return inputs[:, :, self.label_index:]\n\n# \u7dda\u5f62\u30e2\u30c7\u30eb\nms_linear = Sequential([Dense(units=1, kernel_initializer=tf.initializers.zeros)])\n# DNN\nms_dense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\n# LSTM\nms_lstm_model = Sequential([\n    LSTM(32, return_sequences=True),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\n# CNN\nKERNEL_WIDTH = 3\nms_cnn_model = Sequential([\n    Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),\n    Dense(units=32, activation='relu'),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\nms_cnn_lstm_model = Sequential([\n    Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),\n    LSTM(32, return_sequences=True),\n    Dense(units=1, kernel_initializer=tf.initializers.zeros)\n])\n</pre> # models class Baseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index              def call(self, inputs):         if self.label_index is None:             return inputs         elif isinstance(self.label_index, list):             tensors = []             for index in self.label_index:                 res = inputs[:, :, index]                 res = res[:, :, tf.newaxis]                 tensors.append(res)             return tf.concat(tensors, axis=-1)         else:             res = inputs[:, :, self.label_index]             return res[:, :, tf.newaxis]  class MultiStepLastBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         if self.label_index is None:             return tf.tile(inputs[:, -1:, :], [1, 24, 1])         return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])      class RepeatBaseline(Model):     def __init__(self, label_index=None):         super().__init__()         self.label_index = label_index          def call(self, inputs):         return inputs[:, :, self.label_index:]  # \u7dda\u5f62\u30e2\u30c7\u30eb ms_linear = Sequential([Dense(units=1, kernel_initializer=tf.initializers.zeros)]) # DNN ms_dense = Sequential([     Dense(units=64, activation='relu'),     Dense(units=64, activation='relu'),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) # LSTM ms_lstm_model = Sequential([     LSTM(32, return_sequences=True),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) # CNN KERNEL_WIDTH = 3 ms_cnn_model = Sequential([     Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),     Dense(units=32, activation='relu'),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) ms_cnn_lstm_model = Sequential([     Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'),     LSTM(32, return_sequences=True),     Dense(units=1, kernel_initializer=tf.initializers.zeros) ]) In\u00a0[7]: Copied! <pre># \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7\ncolumn_indices = {name: i for i, name in enumerate(df_train.columns)}\nmulti_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\nLABEL_WIDTH = 24\nINPUT_WIDTH = LABEL_WIDTH + KERNEL_WIDTH - 1\nmulti_conv_window = DataWindow(input_width=INPUT_WIDTH, label_width=LABEL_WIDTH, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume'])\nms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\nms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume'])\nms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])\n\nms_val_performance = {}\nms_test_performance = {}\n\nms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val)\nms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test)\nms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val)\nms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test)\nhistory = compile_and_fit(ms_linear, multi_window)\nms_val_performance['Linear'] = ms_linear.evaluate(multi_window.val)\nms_test_performance['Linear'] = ms_linear.evaluate(multi_window.test)\nhistory = compile_and_fit(ms_dense, multi_window)\nms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val)\nms_test_performance['Dense'] = ms_dense.evaluate(multi_window.test)\nhistory = compile_and_fit(ms_lstm_model, multi_window)\nms_val_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.val)\nms_test_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.test)\nhistory = compile_and_fit(ms_cnn_model, multi_conv_window)\nms_val_performance['CNN'] = ms_cnn_model.evaluate(multi_conv_window.val)\nms_test_performance['CNN'] = ms_cnn_model.evaluate(multi_conv_window.test)\nhistory = compile_and_fit(ms_cnn_lstm_model, multi_conv_window)\nms_val_performance['CNN + LSTM'] = ms_cnn_lstm_model.evaluate(multi_conv_window.val)\nms_test_performance['CNN + LSTM'] = ms_cnn_lstm_model.evaluate(multi_conv_window.test)\n</pre> # \u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7 column_indices = {name: i for i, name in enumerate(df_train.columns)} multi_window = DataWindow(input_width=24, label_width=24, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) LABEL_WIDTH = 24 INPUT_WIDTH = LABEL_WIDTH + KERNEL_WIDTH - 1 multi_conv_window = DataWindow(input_width=INPUT_WIDTH, label_width=LABEL_WIDTH, shift=24, df_train=df_train, df_val=df_val, df_test=df_test, label_columns=['traffic_volume']) ms_baseline_last = MultiStepLastBaseline(label_index=column_indices['traffic_volume']) ms_baseline_last.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()]) ms_baseline_repeat = RepeatBaseline(label_index=column_indices['traffic_volume']) ms_baseline_repeat.compile(loss=MeanSquaredError(), metrics=[MeanAbsoluteError()])  ms_val_performance = {} ms_test_performance = {}  ms_val_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.val) ms_test_performance['Baseline - Last'] = ms_baseline_last.evaluate(multi_window.test) ms_val_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.val) ms_test_performance['Baseline - Repeat'] = ms_baseline_repeat.evaluate(multi_window.test) history = compile_and_fit(ms_linear, multi_window) ms_val_performance['Linear'] = ms_linear.evaluate(multi_window.val) ms_test_performance['Linear'] = ms_linear.evaluate(multi_window.test) history = compile_and_fit(ms_dense, multi_window) ms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val) ms_test_performance['Dense'] = ms_dense.evaluate(multi_window.test) history = compile_and_fit(ms_lstm_model, multi_window) ms_val_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.val) ms_test_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.test) history = compile_and_fit(ms_cnn_model, multi_conv_window) ms_val_performance['CNN'] = ms_cnn_model.evaluate(multi_conv_window.val) ms_test_performance['CNN'] = ms_cnn_model.evaluate(multi_conv_window.test) history = compile_and_fit(ms_cnn_lstm_model, multi_conv_window) ms_val_performance['CNN + LSTM'] = ms_cnn_lstm_model.evaluate(multi_conv_window.val) ms_test_performance['CNN + LSTM'] = ms_cnn_lstm_model.evaluate(multi_conv_window.test) <pre>109/109 [==============================] - 0s 3ms/step - loss: 0.1875 - mean_absolute_error: 0.3522\n54/54 [==============================] - 0s 3ms/step - loss: 0.1814 - mean_absolute_error: 0.3473\n109/109 [==============================] - 0s 2ms/step - loss: 0.2065 - mean_absolute_error: 0.3473\n54/54 [==============================] - 0s 2ms/step - loss: 0.2018 - mean_absolute_error: 0.3413\nEpoch 1/50\n383/383 [==============================] - 1s 3ms/step - loss: 0.0897 - mean_absolute_error: 0.2464 - val_loss: 0.0457 - val_mean_absolute_error: 0.1821\nEpoch 2/50\n383/383 [==============================] - 1s 4ms/step - loss: 0.0297 - mean_absolute_error: 0.1365 - val_loss: 0.0262 - val_mean_absolute_error: 0.1259\nEpoch 3/50\n383/383 [==============================] - 1s 2ms/step - loss: 0.0200 - mean_absolute_error: 0.1043 - val_loss: 0.0213 - val_mean_absolute_error: 0.1083\nEpoch 4/50\n383/383 [==============================] - 1s 2ms/step - loss: 0.0176 - mean_absolute_error: 0.0954 - val_loss: 0.0197 - val_mean_absolute_error: 0.1007\nEpoch 5/50\n383/383 [==============================] - 1s 3ms/step - loss: 0.0168 - mean_absolute_error: 0.0914 - val_loss: 0.0189 - val_mean_absolute_error: 0.0961\nEpoch 6/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0164 - mean_absolute_error: 0.0887 - val_loss: 0.0185 - val_mean_absolute_error: 0.0932\nEpoch 7/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0162 - mean_absolute_error: 0.0868 - val_loss: 0.0183 - val_mean_absolute_error: 0.0910\nEpoch 8/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0857 - val_loss: 0.0182 - val_mean_absolute_error: 0.0898\nEpoch 9/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0850 - val_loss: 0.0182 - val_mean_absolute_error: 0.0889\nEpoch 10/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0846 - val_loss: 0.0182 - val_mean_absolute_error: 0.0889\nEpoch 11/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0161 - mean_absolute_error: 0.0843 - val_loss: 0.0182 - val_mean_absolute_error: 0.0887\nEpoch 12/50\n383/383 [==============================] - 1s 3ms/step - loss: 0.0161 - mean_absolute_error: 0.0841 - val_loss: 0.0182 - val_mean_absolute_error: 0.0886\n109/109 [==============================] - 0s 2ms/step - loss: 0.0182 - mean_absolute_error: 0.0886\n54/54 [==============================] - 0s 3ms/step - loss: 0.0142 - mean_absolute_error: 0.0768\nEpoch 1/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0298 - mean_absolute_error: 0.1096 - val_loss: 0.0172 - val_mean_absolute_error: 0.0849\nEpoch 2/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0148 - mean_absolute_error: 0.0794 - val_loss: 0.0166 - val_mean_absolute_error: 0.0850\nEpoch 3/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0143 - mean_absolute_error: 0.0788 - val_loss: 0.0160 - val_mean_absolute_error: 0.0842\nEpoch 4/50\n383/383 [==============================] - 3s 6ms/step - loss: 0.0137 - mean_absolute_error: 0.0780 - val_loss: 0.0158 - val_mean_absolute_error: 0.0868\nEpoch 5/50\n383/383 [==============================] - 3s 8ms/step - loss: 0.0134 - mean_absolute_error: 0.0774 - val_loss: 0.0154 - val_mean_absolute_error: 0.0821\nEpoch 6/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0133 - mean_absolute_error: 0.0771 - val_loss: 0.0150 - val_mean_absolute_error: 0.0822\nEpoch 7/50\n383/383 [==============================] - 2s 5ms/step - loss: 0.0131 - mean_absolute_error: 0.0766 - val_loss: 0.0148 - val_mean_absolute_error: 0.0806\nEpoch 8/50\n383/383 [==============================] - 2s 6ms/step - loss: 0.0131 - mean_absolute_error: 0.0762 - val_loss: 0.0148 - val_mean_absolute_error: 0.0801\nEpoch 9/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0130 - mean_absolute_error: 0.0761 - val_loss: 0.0160 - val_mean_absolute_error: 0.0834\nEpoch 10/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0129 - mean_absolute_error: 0.0757 - val_loss: 0.0146 - val_mean_absolute_error: 0.0799\nEpoch 11/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0129 - mean_absolute_error: 0.0755 - val_loss: 0.0149 - val_mean_absolute_error: 0.0817\nEpoch 12/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0129 - mean_absolute_error: 0.0756 - val_loss: 0.0145 - val_mean_absolute_error: 0.0801\nEpoch 13/50\n383/383 [==============================] - 3s 9ms/step - loss: 0.0128 - mean_absolute_error: 0.0753 - val_loss: 0.0146 - val_mean_absolute_error: 0.0821\nEpoch 14/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0128 - mean_absolute_error: 0.0752 - val_loss: 0.0145 - val_mean_absolute_error: 0.0790\nEpoch 15/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0127 - mean_absolute_error: 0.0750 - val_loss: 0.0144 - val_mean_absolute_error: 0.0805\nEpoch 16/50\n383/383 [==============================] - 3s 8ms/step - loss: 0.0127 - mean_absolute_error: 0.0749 - val_loss: 0.0143 - val_mean_absolute_error: 0.0787\nEpoch 17/50\n383/383 [==============================] - 3s 8ms/step - loss: 0.0126 - mean_absolute_error: 0.0746 - val_loss: 0.0145 - val_mean_absolute_error: 0.0805\nEpoch 18/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0126 - mean_absolute_error: 0.0745 - val_loss: 0.0143 - val_mean_absolute_error: 0.0781\nEpoch 19/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0126 - mean_absolute_error: 0.0745 - val_loss: 0.0145 - val_mean_absolute_error: 0.0796\n109/109 [==============================] - 1s 4ms/step - loss: 0.0145 - mean_absolute_error: 0.0796\n54/54 [==============================] - 0s 4ms/step - loss: 0.0100 - mean_absolute_error: 0.0657\nEpoch 1/50\n383/383 [==============================] - 11s 24ms/step - loss: 0.0477 - mean_absolute_error: 0.1570 - val_loss: 0.0209 - val_mean_absolute_error: 0.1023\nEpoch 2/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0167 - mean_absolute_error: 0.0899 - val_loss: 0.0175 - val_mean_absolute_error: 0.0906\nEpoch 3/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0150 - mean_absolute_error: 0.0830 - val_loss: 0.0165 - val_mean_absolute_error: 0.0869\nEpoch 4/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0143 - mean_absolute_error: 0.0803 - val_loss: 0.0159 - val_mean_absolute_error: 0.0836\nEpoch 5/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0139 - mean_absolute_error: 0.0789 - val_loss: 0.0159 - val_mean_absolute_error: 0.0852\nEpoch 6/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0137 - mean_absolute_error: 0.0777 - val_loss: 0.0153 - val_mean_absolute_error: 0.0828\nEpoch 7/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0134 - mean_absolute_error: 0.0766 - val_loss: 0.0153 - val_mean_absolute_error: 0.0820\nEpoch 8/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0133 - mean_absolute_error: 0.0764 - val_loss: 0.0151 - val_mean_absolute_error: 0.0826\nEpoch 9/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0131 - mean_absolute_error: 0.0756 - val_loss: 0.0148 - val_mean_absolute_error: 0.0783\nEpoch 10/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0130 - mean_absolute_error: 0.0752 - val_loss: 0.0147 - val_mean_absolute_error: 0.0803\nEpoch 11/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0129 - mean_absolute_error: 0.0747 - val_loss: 0.0156 - val_mean_absolute_error: 0.0827\nEpoch 12/50\n383/383 [==============================] - 9s 23ms/step - loss: 0.0129 - mean_absolute_error: 0.0745 - val_loss: 0.0150 - val_mean_absolute_error: 0.0803\nEpoch 13/50\n383/383 [==============================] - 9s 24ms/step - loss: 0.0127 - mean_absolute_error: 0.0740 - val_loss: 0.0149 - val_mean_absolute_error: 0.0799\n109/109 [==============================] - 1s 11ms/step - loss: 0.0149 - mean_absolute_error: 0.0799\n54/54 [==============================] - 1s 9ms/step - loss: 0.0109 - mean_absolute_error: 0.0663\nEpoch 1/50\n383/383 [==============================] - 4s 8ms/step - loss: 0.0354 - mean_absolute_error: 0.1243 - val_loss: 0.0169 - val_mean_absolute_error: 0.0852\nEpoch 2/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0144 - mean_absolute_error: 0.0788 - val_loss: 0.0164 - val_mean_absolute_error: 0.0867\nEpoch 3/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0140 - mean_absolute_error: 0.0781 - val_loss: 0.0160 - val_mean_absolute_error: 0.0839\nEpoch 4/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0138 - mean_absolute_error: 0.0778 - val_loss: 0.0160 - val_mean_absolute_error: 0.0862\nEpoch 5/50\n383/383 [==============================] - 3s 8ms/step - loss: 0.0137 - mean_absolute_error: 0.0775 - val_loss: 0.0159 - val_mean_absolute_error: 0.0863\nEpoch 6/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0135 - mean_absolute_error: 0.0772 - val_loss: 0.0156 - val_mean_absolute_error: 0.0818\nEpoch 7/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0133 - mean_absolute_error: 0.0766 - val_loss: 0.0154 - val_mean_absolute_error: 0.0844\nEpoch 8/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0132 - mean_absolute_error: 0.0761 - val_loss: 0.0151 - val_mean_absolute_error: 0.0835\nEpoch 9/50\n383/383 [==============================] - 3s 8ms/step - loss: 0.0131 - mean_absolute_error: 0.0758 - val_loss: 0.0150 - val_mean_absolute_error: 0.0823\nEpoch 10/50\n383/383 [==============================] - 3s 8ms/step - loss: 0.0129 - mean_absolute_error: 0.0751 - val_loss: 0.0147 - val_mean_absolute_error: 0.0800\nEpoch 11/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0128 - mean_absolute_error: 0.0746 - val_loss: 0.0148 - val_mean_absolute_error: 0.0794\nEpoch 12/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0127 - mean_absolute_error: 0.0743 - val_loss: 0.0146 - val_mean_absolute_error: 0.0793\nEpoch 13/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0126 - mean_absolute_error: 0.0741 - val_loss: 0.0145 - val_mean_absolute_error: 0.0806\nEpoch 14/50\n383/383 [==============================] - 3s 8ms/step - loss: 0.0126 - mean_absolute_error: 0.0741 - val_loss: 0.0144 - val_mean_absolute_error: 0.0797\nEpoch 15/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0125 - mean_absolute_error: 0.0735 - val_loss: 0.0144 - val_mean_absolute_error: 0.0795\nEpoch 16/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0125 - mean_absolute_error: 0.0737 - val_loss: 0.0146 - val_mean_absolute_error: 0.0824\nEpoch 17/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0125 - mean_absolute_error: 0.0735 - val_loss: 0.0143 - val_mean_absolute_error: 0.0796\nEpoch 18/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0124 - mean_absolute_error: 0.0731 - val_loss: 0.0143 - val_mean_absolute_error: 0.0795\nEpoch 19/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0123 - mean_absolute_error: 0.0729 - val_loss: 0.0146 - val_mean_absolute_error: 0.0774\nEpoch 20/50\n383/383 [==============================] - 3s 7ms/step - loss: 0.0123 - mean_absolute_error: 0.0729 - val_loss: 0.0144 - val_mean_absolute_error: 0.0807\n109/109 [==============================] - 1s 4ms/step - loss: 0.0144 - mean_absolute_error: 0.0807\n54/54 [==============================] - 0s 4ms/step - loss: 0.0098 - mean_absolute_error: 0.0655\nEpoch 1/50\n383/383 [==============================] - 13s 29ms/step - loss: 0.0420 - mean_absolute_error: 0.1441 - val_loss: 0.0171 - val_mean_absolute_error: 0.0897\nEpoch 2/50\n383/383 [==============================] - 11s 27ms/step - loss: 0.0146 - mean_absolute_error: 0.0824 - val_loss: 0.0163 - val_mean_absolute_error: 0.0865\nEpoch 3/50\n383/383 [==============================] - 11s 28ms/step - loss: 0.0141 - mean_absolute_error: 0.0798 - val_loss: 0.0161 - val_mean_absolute_error: 0.0855\nEpoch 4/50\n383/383 [==============================] - 13s 33ms/step - loss: 0.0139 - mean_absolute_error: 0.0787 - val_loss: 0.0158 - val_mean_absolute_error: 0.0849\nEpoch 5/50\n383/383 [==============================] - 15s 38ms/step - loss: 0.0138 - mean_absolute_error: 0.0782 - val_loss: 0.0156 - val_mean_absolute_error: 0.0832\nEpoch 6/50\n383/383 [==============================] - 17s 45ms/step - loss: 0.0136 - mean_absolute_error: 0.0774 - val_loss: 0.0155 - val_mean_absolute_error: 0.0811\nEpoch 7/50\n383/383 [==============================] - 13s 34ms/step - loss: 0.0134 - mean_absolute_error: 0.0767 - val_loss: 0.0156 - val_mean_absolute_error: 0.0859\nEpoch 8/50\n383/383 [==============================] - 15s 40ms/step - loss: 0.0132 - mean_absolute_error: 0.0759 - val_loss: 0.0159 - val_mean_absolute_error: 0.0884\nEpoch 9/50\n383/383 [==============================] - 12s 30ms/step - loss: 0.0129 - mean_absolute_error: 0.0752 - val_loss: 0.0147 - val_mean_absolute_error: 0.0786\nEpoch 10/50\n383/383 [==============================] - 14s 37ms/step - loss: 0.0127 - mean_absolute_error: 0.0747 - val_loss: 0.0151 - val_mean_absolute_error: 0.0859\nEpoch 11/50\n383/383 [==============================] - 12s 30ms/step - loss: 0.0126 - mean_absolute_error: 0.0743 - val_loss: 0.0142 - val_mean_absolute_error: 0.0807\nEpoch 12/50\n383/383 [==============================] - 13s 33ms/step - loss: 0.0125 - mean_absolute_error: 0.0737 - val_loss: 0.0149 - val_mean_absolute_error: 0.0841\nEpoch 13/50\n383/383 [==============================] - 13s 34ms/step - loss: 0.0123 - mean_absolute_error: 0.0731 - val_loss: 0.0138 - val_mean_absolute_error: 0.0778\nEpoch 14/50\n383/383 [==============================] - 14s 36ms/step - loss: 0.0119 - mean_absolute_error: 0.0718 - val_loss: 0.0135 - val_mean_absolute_error: 0.0744\nEpoch 15/50\n383/383 [==============================] - 14s 36ms/step - loss: 0.0118 - mean_absolute_error: 0.0712 - val_loss: 0.0130 - val_mean_absolute_error: 0.0747\nEpoch 16/50\n383/383 [==============================] - 11s 30ms/step - loss: 0.0116 - mean_absolute_error: 0.0704 - val_loss: 0.0129 - val_mean_absolute_error: 0.0752\nEpoch 17/50\n383/383 [==============================] - 13s 33ms/step - loss: 0.0115 - mean_absolute_error: 0.0700 - val_loss: 0.0127 - val_mean_absolute_error: 0.0722\nEpoch 18/50\n383/383 [==============================] - 11s 29ms/step - loss: 0.0113 - mean_absolute_error: 0.0692 - val_loss: 0.0124 - val_mean_absolute_error: 0.0727\nEpoch 19/50\n383/383 [==============================] - 11s 29ms/step - loss: 0.0112 - mean_absolute_error: 0.0686 - val_loss: 0.0123 - val_mean_absolute_error: 0.0728\nEpoch 20/50\n383/383 [==============================] - 10s 27ms/step - loss: 0.0110 - mean_absolute_error: 0.0678 - val_loss: 0.0123 - val_mean_absolute_error: 0.0721\nEpoch 21/50\n383/383 [==============================] - 10s 27ms/step - loss: 0.0109 - mean_absolute_error: 0.0674 - val_loss: 0.0119 - val_mean_absolute_error: 0.0690\nEpoch 22/50\n383/383 [==============================] - 10s 27ms/step - loss: 0.0108 - mean_absolute_error: 0.0668 - val_loss: 0.0119 - val_mean_absolute_error: 0.0696\nEpoch 23/50\n383/383 [==============================] - 11s 28ms/step - loss: 0.0107 - mean_absolute_error: 0.0665 - val_loss: 0.0126 - val_mean_absolute_error: 0.0766\nEpoch 24/50\n383/383 [==============================] - 10s 26ms/step - loss: 0.0106 - mean_absolute_error: 0.0663 - val_loss: 0.0119 - val_mean_absolute_error: 0.0730\nEpoch 25/50\n383/383 [==============================] - 11s 27ms/step - loss: 0.0105 - mean_absolute_error: 0.0657 - val_loss: 0.0120 - val_mean_absolute_error: 0.0693\nEpoch 26/50\n383/383 [==============================] - 11s 29ms/step - loss: 0.0104 - mean_absolute_error: 0.0655 - val_loss: 0.0116 - val_mean_absolute_error: 0.0687\nEpoch 27/50\n383/383 [==============================] - 16s 42ms/step - loss: 0.0103 - mean_absolute_error: 0.0650 - val_loss: 0.0123 - val_mean_absolute_error: 0.0735\nEpoch 28/50\n383/383 [==============================] - 11s 28ms/step - loss: 0.0102 - mean_absolute_error: 0.0644 - val_loss: 0.0117 - val_mean_absolute_error: 0.0703\nEpoch 29/50\n383/383 [==============================] - 10s 27ms/step - loss: 0.0102 - mean_absolute_error: 0.0643 - val_loss: 0.0118 - val_mean_absolute_error: 0.0671\n109/109 [==============================] - 1s 11ms/step - loss: 0.0118 - mean_absolute_error: 0.0671\n54/54 [==============================] - 1s 10ms/step - loss: 0.0081 - mean_absolute_error: 0.0531\n</pre> In\u00a0[8]: Copied! <pre># multi step\u306e\u7d50\u679c\u6bd4\u8f03\nplt.title('Multi Step')\nplt.xlabel('Models')\nplt.ylabel('MAE')\nplt.bar(ms_val_performance.keys(), [v[1] for v in ms_val_performance.values()], width=-0.25, align='edge', label='Validation')\nplt.bar(ms_test_performance.keys(), [v[1] for v in ms_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/')\nplt.legend()\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\n</pre> # multi step\u306e\u7d50\u679c\u6bd4\u8f03 plt.title('Multi Step') plt.xlabel('Models') plt.ylabel('MAE') plt.bar(ms_val_performance.keys(), [v[1] for v in ms_val_performance.values()], width=-0.25, align='edge', label='Validation') plt.bar(ms_test_performance.keys(), [v[1] for v in ms_test_performance.values()], width=0.25, align='edge', label='Test', hatch='/') plt.legend() plt.xticks(rotation=45, ha='right') plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter17.html#arlstm","title":"ARLSTM\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u8abf\u3079\u308b\u00b6","text":""},{"location":"chapter17.html#arlstm","title":"ARLSTM\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u00b6","text":""}]}